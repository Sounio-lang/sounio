//! Metal Shading Language (MSL) Code Generation
//!
//! Native Apple Metal backend for Sounio GPU computing.
//! Supports M1/M2/M3 Apple Silicon and Intel Macs.
//!
//! # Architecture
//!
//! ```text
//! GPU IR ──> MSL Source ──> Metal Compiler ──> GPU Binary
//! ```
//!
//! # Metal vs CUDA Mapping
//!
//! | CUDA Concept | Metal Equivalent |
//! |--------------|------------------|
//! | Thread | Thread |
//! | Warp (32) | Simdgroup (32) |
//! | Block | Threadgroup |
//! | Grid | Grid |
//! | `__shared__` | `threadgroup` |
//! | `__device__` | `device` |
//! | `__constant__` | `constant` |
//! | `atomicAdd` | `atomic_fetch_add_explicit` |
//! | `__syncthreads()` | `threadgroup_barrier(mem_flags::mem_threadgroup)` |
//! | `__shfl_xor_sync` | `simd_shuffle_xor` |
//!
//! # Epistemic Shadow Variables in MSL
//!
//! ```metal
//! struct EpistemicFloat {
//!     float value;
//!     float epsilon;      // Uncertainty bound
//!     bool valid;         // Validity flag
//!     uint64_t provenance; // Data lineage
//! };
//! ```

use std::collections::HashMap;
use std::fmt::Write;

use super::ir::*;

/// Metal codegen configuration
#[derive(Debug, Clone)]
pub struct MetalCodegenConfig {
    /// Target GPU family
    pub gpu_family: MetalGpuFamily,
    /// Enable fast math optimizations
    pub fast_math: bool,
    /// Generate debug info
    pub debug_info: bool,
    /// Enable epistemic tracking
    pub epistemic_enabled: bool,
    /// Maximum threads per threadgroup
    pub max_threads_per_threadgroup: u32,
}

impl Default for MetalCodegenConfig {
    fn default() -> Self {
        Self {
            gpu_family: MetalGpuFamily::Apple8,
            fast_math: true,
            debug_info: false,
            epistemic_enabled: true,
            max_threads_per_threadgroup: 256,
        }
    }
}

/// Metal Shading Language code generator
pub struct MetalCodegen {
    config: MetalCodegenConfig,
    output: String,
    indent: usize,
    value_counter: u32,
    /// Map from GPU IR values to MSL variable names
    value_names: HashMap<ValueId, String>,
}

impl MetalCodegen {
    pub fn new(config: MetalCodegenConfig) -> Self {
        Self {
            config,
            output: String::with_capacity(16384),
            indent: 0,
            value_counter: 0,
            value_names: HashMap::new(),
        }
    }

    /// Generate MSL code from GPU module
    pub fn generate(&mut self, module: &GpuModule) -> String {
        self.output.clear();
        self.emit_header();
        self.emit_epistemic_types();
        self.emit_epistemic_functions();
        self.emit_simdgroup_helpers();

        // Generate constants
        for constant in &module.constants {
            self.emit_constant(constant);
        }

        // Generate device functions
        for func in module.device_functions.values() {
            self.emit_device_function(func);
        }

        // Generate kernels
        for kernel in module.kernels.values() {
            self.emit_kernel(kernel);
        }

        std::mem::take(&mut self.output)
    }

    fn emit(&mut self, s: &str) {
        let indent = "    ".repeat(self.indent);
        writeln!(self.output, "{}{}", indent, s).unwrap();
    }

    fn emit_raw(&mut self, s: &str) {
        writeln!(self.output, "{}", s).unwrap();
    }

    fn fresh_var(&mut self, prefix: &str) -> String {
        let name = format!("{}_{}", prefix, self.value_counter);
        self.value_counter += 1;
        name
    }

    fn emit_header(&mut self) {
        let msl_version = self.config.gpu_family.msl_version();
        self.emit_raw("// Generated by Sounio Compiler - Metal Backend");
        self.emit_raw(&format!("// Target: {:?}", self.config.gpu_family));
        self.emit_raw(&format!("// MSL Version: {}", msl_version));
        self.emit_raw("");
        self.emit_raw("#include <metal_stdlib>");
        self.emit_raw("#include <simdgroup_matrix>");
        self.emit_raw("using namespace metal;");
        self.emit_raw("");

        if self.config.fast_math {
            self.emit_raw("// Fast math enabled");
            self.emit_raw("#pragma clang fp contract(fast)");
            self.emit_raw("");
        }
    }

    fn emit_epistemic_types(&mut self) {
        if !self.config.epistemic_enabled {
            return;
        }

        self.emit_raw("// ============================================");
        self.emit_raw("// Epistemic Types - Shadow Variables for Uncertainty");
        self.emit_raw("// ============================================");
        self.emit_raw("");

        // EpistemicFloat - single precision with uncertainty tracking
        self.emit_raw("struct EpistemicFloat {");
        self.emit_raw("    float value;           // Actual value");
        self.emit_raw("    float epsilon;         // Uncertainty bound (σ)");
        self.emit_raw("    bool valid;            // Validity flag");
        self.emit_raw("    uint64_t provenance;   // Data lineage bitmask");
        self.emit_raw("");
        self.emit_raw("    // Constructors");
        self.emit_raw("    EpistemicFloat() : value(0), epsilon(0), valid(true), provenance(0) {}");
        self.emit_raw(
            "    EpistemicFloat(float v) : value(v), epsilon(0), valid(true), provenance(0) {}",
        );
        self.emit_raw("    EpistemicFloat(float v, float e) : value(v), epsilon(e), valid(true), provenance(0) {}");
        self.emit_raw("    EpistemicFloat(float v, float e, bool val, uint64_t prov)");
        self.emit_raw("        : value(v), epsilon(e), valid(val), provenance(prov) {}");
        self.emit_raw("");
        self.emit_raw("    // Confidence level (1 - epsilon for small epsilon)");
        self.emit_raw("    float confidence() const { return 1.0f - epsilon; }");
        self.emit_raw("};");
        self.emit_raw("");

        // EpistemicFloat2/3/4 for vector operations
        self.emit_raw("struct EpistemicFloat2 {");
        self.emit_raw("    float2 value;");
        self.emit_raw("    float2 epsilon;");
        self.emit_raw("    bool2 valid;");
        self.emit_raw("    uint64_t provenance;");
        self.emit_raw("};");
        self.emit_raw("");

        self.emit_raw("struct EpistemicFloat4 {");
        self.emit_raw("    float4 value;");
        self.emit_raw("    float4 epsilon;");
        self.emit_raw("    bool4 valid;");
        self.emit_raw("    uint64_t provenance;");
        self.emit_raw("};");
        self.emit_raw("");
    }

    fn emit_epistemic_functions(&mut self) {
        if !self.config.epistemic_enabled {
            return;
        }

        self.emit_raw("// ============================================");
        self.emit_raw("// Epistemic Operations - Uncertainty Propagation");
        self.emit_raw("// ============================================");
        self.emit_raw("");

        // Epistemic addition with quadrature propagation
        self.emit_raw("// Epistemic add: ε_c = √(ε_a² + ε_b²)");
        self.emit_raw("inline EpistemicFloat epistemic_add(EpistemicFloat a, EpistemicFloat b) {");
        self.emit_raw("    EpistemicFloat result;");
        self.emit_raw("    result.value = a.value + b.value;");
        self.emit_raw("    result.epsilon = sqrt(a.epsilon * a.epsilon + b.epsilon * b.epsilon);");
        self.emit_raw("    result.valid = a.valid && b.valid;");
        self.emit_raw("    result.provenance = a.provenance ^ b.provenance;");
        self.emit_raw("    return result;");
        self.emit_raw("}");
        self.emit_raw("");

        // Epistemic subtraction
        self.emit_raw("// Epistemic sub: same propagation as add");
        self.emit_raw("inline EpistemicFloat epistemic_sub(EpistemicFloat a, EpistemicFloat b) {");
        self.emit_raw("    EpistemicFloat result;");
        self.emit_raw("    result.value = a.value - b.value;");
        self.emit_raw("    result.epsilon = sqrt(a.epsilon * a.epsilon + b.epsilon * b.epsilon);");
        self.emit_raw("    result.valid = a.valid && b.valid;");
        self.emit_raw("    result.provenance = a.provenance ^ b.provenance;");
        self.emit_raw("    return result;");
        self.emit_raw("}");
        self.emit_raw("");

        // Epistemic multiplication
        self.emit_raw("// Epistemic mul: ε_c ≈ |a|·ε_b + |b|·ε_a");
        self.emit_raw("inline EpistemicFloat epistemic_mul(EpistemicFloat a, EpistemicFloat b) {");
        self.emit_raw("    EpistemicFloat result;");
        self.emit_raw("    result.value = a.value * b.value;");
        self.emit_raw("    result.epsilon = abs(a.value) * b.epsilon + abs(b.value) * a.epsilon;");
        self.emit_raw("    result.valid = a.valid && b.valid;");
        self.emit_raw("    result.provenance = a.provenance ^ b.provenance;");
        self.emit_raw("    return result;");
        self.emit_raw("}");
        self.emit_raw("");

        // Epistemic division
        self.emit_raw("// Epistemic div: ε_c ≈ (|a|·ε_b + |b|·ε_a) / b²");
        self.emit_raw("inline EpistemicFloat epistemic_div(EpistemicFloat a, EpistemicFloat b) {");
        self.emit_raw("    EpistemicFloat result;");
        self.emit_raw("    float b_sq = b.value * b.value;");
        self.emit_raw("    result.value = a.value / b.value;");
        self.emit_raw(
            "    result.epsilon = (abs(a.value) * b.epsilon + abs(b.value) * a.epsilon) / b_sq;",
        );
        self.emit_raw("    result.valid = a.valid && b.valid && (abs(b.value) > 1e-10f);");
        self.emit_raw("    result.provenance = a.provenance ^ b.provenance;");
        self.emit_raw("    return result;");
        self.emit_raw("}");
        self.emit_raw("");

        // Epistemic FMA
        self.emit_raw("// Epistemic fma: a * b + c");
        self.emit_raw("inline EpistemicFloat epistemic_fma(EpistemicFloat a, EpistemicFloat b, EpistemicFloat c) {");
        self.emit_raw("    EpistemicFloat ab = epistemic_mul(a, b);");
        self.emit_raw("    return epistemic_add(ab, c);");
        self.emit_raw("}");
        self.emit_raw("");

        // Epistemic sqrt
        self.emit_raw("// Epistemic sqrt: ε_c = ε_a / (2·√a)");
        self.emit_raw("inline EpistemicFloat epistemic_sqrt(EpistemicFloat a) {");
        self.emit_raw("    EpistemicFloat result;");
        self.emit_raw("    result.value = sqrt(a.value);");
        self.emit_raw("    result.epsilon = a.epsilon / (2.0f * result.value);");
        self.emit_raw("    result.valid = a.valid && (a.value >= 0.0f);");
        self.emit_raw("    result.provenance = a.provenance;");
        self.emit_raw("    return result;");
        self.emit_raw("}");
        self.emit_raw("");

        // Epistemic exp
        self.emit_raw("// Epistemic exp: ε_c = exp(a) · ε_a");
        self.emit_raw("inline EpistemicFloat epistemic_exp(EpistemicFloat a) {");
        self.emit_raw("    EpistemicFloat result;");
        self.emit_raw("    result.value = exp(a.value);");
        self.emit_raw("    result.epsilon = result.value * a.epsilon;");
        self.emit_raw("    result.valid = a.valid;");
        self.emit_raw("    result.provenance = a.provenance;");
        self.emit_raw("    return result;");
        self.emit_raw("}");
        self.emit_raw("");

        // Epistemic log
        self.emit_raw("// Epistemic log: ε_c = ε_a / a");
        self.emit_raw("inline EpistemicFloat epistemic_log(EpistemicFloat a) {");
        self.emit_raw("    EpistemicFloat result;");
        self.emit_raw("    result.value = log(a.value);");
        self.emit_raw("    result.epsilon = a.epsilon / a.value;");
        self.emit_raw("    result.valid = a.valid && (a.value > 0.0f);");
        self.emit_raw("    result.provenance = a.provenance;");
        self.emit_raw("    return result;");
        self.emit_raw("}");
        self.emit_raw("");

        // Confidence gate
        self.emit_raw("// Check if confidence exceeds threshold");
        self.emit_raw("inline bool confidence_gate(EpistemicFloat a, float threshold) {");
        self.emit_raw("    return a.valid && (a.epsilon < (1.0f - threshold));");
        self.emit_raw("}");
        self.emit_raw("");
    }

    fn emit_simdgroup_helpers(&mut self) {
        self.emit_raw("// ============================================");
        self.emit_raw("// Simdgroup Operations (CUDA warp equivalent)");
        self.emit_raw("// ============================================");
        self.emit_raw("");

        // Simdgroup reduce for epistemic values
        if self.config.epistemic_enabled {
            self.emit_raw("// Simdgroup-wide epsilon reduction (max)");
            self.emit_raw("inline float simdgroup_epsilon_max(float epsilon) {");
            self.emit_raw("    return simd_max(epsilon);");
            self.emit_raw("}");
            self.emit_raw("");

            self.emit_raw("// Simdgroup-wide epsilon reduction (min)");
            self.emit_raw("inline float simdgroup_epsilon_min(float epsilon) {");
            self.emit_raw("    return simd_min(epsilon);");
            self.emit_raw("}");
            self.emit_raw("");

            self.emit_raw("// Simdgroup-wide validity vote (count valid)");
            self.emit_raw("inline uint simdgroup_validity_count(bool valid) {");
            self.emit_raw("    return popcount(simd_ballot(valid));");
            self.emit_raw("}");
            self.emit_raw("");

            self.emit_raw("// Simdgroup-wide epistemic reduce (sum values, max epsilon)");
            self.emit_raw("inline EpistemicFloat simdgroup_epistemic_sum(EpistemicFloat a) {");
            self.emit_raw("    EpistemicFloat result;");
            self.emit_raw("    result.value = simd_sum(a.value);");
            self.emit_raw("    result.epsilon = simd_max(a.epsilon);  // Conservative bound");
            self.emit_raw("    result.valid = simd_all(a.valid);");
            self.emit_raw("    result.provenance = 0;  // Aggregated");
            self.emit_raw("    return result;");
            self.emit_raw("}");
            self.emit_raw("");
        }

        // Standard simdgroup helpers
        self.emit_raw("// Simdgroup shuffle XOR (for reductions)");
        self.emit_raw("template<typename T>");
        self.emit_raw("inline T simdgroup_shuffle_xor(T value, ushort mask) {");
        self.emit_raw("    return simd_shuffle_xor(value, mask);");
        self.emit_raw("}");
        self.emit_raw("");

        self.emit_raw("// Simdgroup broadcast from lane 0");
        self.emit_raw("template<typename T>");
        self.emit_raw("inline T simdgroup_broadcast_first(T value) {");
        self.emit_raw("    return simd_broadcast_first(value);");
        self.emit_raw("}");
        self.emit_raw("");
    }

    fn emit_constant(&mut self, constant: &GpuConstant) {
        let ty = self.type_to_msl(&constant.ty);
        let value = self.const_value_to_msl(&constant.value);
        self.emit_raw(&format!("constant {} {} = {};", ty, constant.name, value));
    }

    fn emit_device_function(&mut self, func: &GpuFunction) {
        let return_ty = self.type_to_msl(&func.return_type);
        let params = self.params_to_msl(&func.params);

        let inline = if func.inline { "inline " } else { "" };
        self.emit_raw(&format!(
            "{}{}  {}({}) {{",
            inline, return_ty, func.name, params
        ));

        self.indent += 1;
        for block in &func.blocks {
            self.emit_block(block);
        }
        self.indent -= 1;

        self.emit_raw("}");
        self.emit_raw("");
    }

    fn emit_kernel(&mut self, kernel: &GpuKernel) {
        self.emit_raw(&format!("// Kernel: {}", kernel.name));

        // Emit threadgroup memory declarations
        for shared in &kernel.shared_memory {
            let ty = self.type_to_msl(&shared.elem_type);
            self.emit_raw(&format!(
                "// Shared memory: {} {}[{}]",
                ty, shared.name, shared.size
            ));
        }

        // Build parameter list
        let mut params = Vec::new();

        // Regular parameters
        for (i, param) in kernel.params.iter().enumerate() {
            let ty = self.type_to_msl(&param.ty);
            let space = self.memory_space_to_msl(param.space);
            params.push(format!("{} {} {} [[buffer({})]]", space, ty, param.name, i));
        }

        // Add threadgroup memory parameters
        for (i, shared) in kernel.shared_memory.iter().enumerate() {
            let ty = self.type_to_msl(&shared.elem_type);
            params.push(format!(
                "threadgroup {}* {} [[threadgroup({})]]",
                ty, shared.name, i
            ));
        }

        // Add built-in parameters
        params.push("uint3 thread_position_in_grid [[thread_position_in_grid]]".to_string());
        params.push(
            "uint3 thread_position_in_threadgroup [[thread_position_in_threadgroup]]".to_string(),
        );
        params.push(
            "uint3 threadgroup_position_in_grid [[threadgroup_position_in_grid]]".to_string(),
        );
        params.push("uint3 threads_per_threadgroup [[threads_per_threadgroup]]".to_string());
        params.push("uint simd_lane_id [[thread_index_in_simdgroup]]".to_string());
        params.push("uint simd_group_id [[simdgroup_index_in_threadgroup]]".to_string());

        let params_str = params.join(",\n    ");

        self.emit_raw(&format!("kernel void {}(", kernel.name));
        self.emit_raw(&format!("    {}", params_str));
        self.emit_raw(") {");

        // Emit local variable declarations
        self.indent += 1;

        // Thread indices (CUDA compatibility)
        self.emit("// Thread indices (CUDA-style naming)");
        self.emit("uint threadIdx_x = thread_position_in_threadgroup.x;");
        self.emit("uint threadIdx_y = thread_position_in_threadgroup.y;");
        self.emit("uint threadIdx_z = thread_position_in_threadgroup.z;");
        self.emit("uint blockIdx_x = threadgroup_position_in_grid.x;");
        self.emit("uint blockIdx_y = threadgroup_position_in_grid.y;");
        self.emit("uint blockIdx_z = threadgroup_position_in_grid.z;");
        self.emit("uint blockDim_x = threads_per_threadgroup.x;");
        self.emit("uint blockDim_y = threads_per_threadgroup.y;");
        self.emit("uint blockDim_z = threads_per_threadgroup.z;");
        self.emit("uint globalIdx = thread_position_in_grid.x;");
        self.emit("");

        // Emit blocks
        for block in &kernel.blocks {
            self.emit_block(block);
        }

        self.indent -= 1;
        self.emit_raw("}");
        self.emit_raw("");
    }

    fn emit_block(&mut self, block: &GpuBlock) {
        if !block.label.is_empty() {
            self.emit(&format!("{}: // Block {}", block.label, block.id));
        }

        for (value_id, op) in &block.instructions {
            self.emit_instruction(*value_id, op);
        }

        self.emit_terminator(&block.terminator);
    }

    fn emit_instruction(&mut self, result: ValueId, op: &GpuOp) {
        let result_name = self.get_or_create_var_name(result);

        match op {
            // Constants
            GpuOp::ConstInt(val, ty) => {
                let ty_str = self.type_to_msl(ty);
                self.emit(&format!("{} {} = {};", ty_str, result_name, val));
            }
            GpuOp::ConstFloat(val, ty) => {
                let ty_str = self.type_to_msl(ty);
                self.emit(&format!("{} {} = {}f;", ty_str, result_name, val));
            }
            GpuOp::ConstBool(val) => {
                self.emit(&format!("bool {} = {};", result_name, val));
            }

            // Arithmetic
            GpuOp::Add(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} + {};", result_name, a_name, b_name));
            }
            GpuOp::Sub(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} - {};", result_name, a_name, b_name));
            }
            GpuOp::Mul(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} * {};", result_name, a_name, b_name));
            }
            GpuOp::Div(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} / {};", result_name, a_name, b_name));
            }
            GpuOp::Rem(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} % {};", result_name, a_name, b_name));
            }
            GpuOp::Neg(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("auto {} = -{};", result_name, a_name));
            }

            // Float arithmetic
            GpuOp::FAdd(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("float {} = {} + {};", result_name, a_name, b_name));
            }
            GpuOp::FSub(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("float {} = {} - {};", result_name, a_name, b_name));
            }
            GpuOp::FMul(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("float {} = {} * {};", result_name, a_name, b_name));
            }
            GpuOp::FDiv(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("float {} = {} / {};", result_name, a_name, b_name));
            }
            GpuOp::FNeg(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("float {} = -{};", result_name, a_name));
            }

            // Fast math
            GpuOp::FMulAdd(a, b, c) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                let c_name = self.get_var_name(*c);
                self.emit(&format!(
                    "float {} = fma({}, {}, {});",
                    result_name, a_name, b_name, c_name
                ));
            }
            GpuOp::FastSin(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("float {} = sin({});", result_name, a_name));
            }
            GpuOp::FastCos(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("float {} = cos({});", result_name, a_name));
            }
            GpuOp::FastExp(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("float {} = exp({});", result_name, a_name));
            }
            GpuOp::FastLog(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("float {} = log({});", result_name, a_name));
            }
            GpuOp::FastSqrt(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("float {} = sqrt({});", result_name, a_name));
            }
            GpuOp::FastRsqrt(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("float {} = rsqrt({});", result_name, a_name));
            }

            // Comparisons
            GpuOp::Eq(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} == {};", result_name, a_name, b_name));
            }
            GpuOp::Ne(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} != {};", result_name, a_name, b_name));
            }
            GpuOp::Lt(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} < {};", result_name, a_name, b_name));
            }
            GpuOp::Le(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} <= {};", result_name, a_name, b_name));
            }
            GpuOp::Gt(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} > {};", result_name, a_name, b_name));
            }
            GpuOp::Ge(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} >= {};", result_name, a_name, b_name));
            }

            // Float comparisons
            GpuOp::FEq(a, b)
            | GpuOp::FNe(a, b)
            | GpuOp::FLt(a, b)
            | GpuOp::FLe(a, b)
            | GpuOp::FGt(a, b)
            | GpuOp::FGe(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                let op_str = match op {
                    GpuOp::FEq(_, _) => "==",
                    GpuOp::FNe(_, _) => "!=",
                    GpuOp::FLt(_, _) => "<",
                    GpuOp::FLe(_, _) => "<=",
                    GpuOp::FGt(_, _) => ">",
                    GpuOp::FGe(_, _) => ">=",
                    _ => unreachable!(),
                };
                self.emit(&format!(
                    "bool {} = {} {} {};",
                    result_name, a_name, op_str, b_name
                ));
            }

            // Logical
            GpuOp::And(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} && {};", result_name, a_name, b_name));
            }
            GpuOp::Or(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("bool {} = {} || {};", result_name, a_name, b_name));
            }
            GpuOp::Not(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("bool {} = !{};", result_name, a_name));
            }

            // Bit operations
            GpuOp::BitAnd(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} & {};", result_name, a_name, b_name));
            }
            GpuOp::BitOr(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} | {};", result_name, a_name, b_name));
            }
            GpuOp::BitXor(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} ^ {};", result_name, a_name, b_name));
            }
            GpuOp::BitNot(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("auto {} = ~{};", result_name, a_name));
            }
            GpuOp::Shl(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} << {};", result_name, a_name, b_name));
            }
            GpuOp::Shr(a, b) | GpuOp::LShr(a, b) => {
                let a_name = self.get_var_name(*a);
                let b_name = self.get_var_name(*b);
                self.emit(&format!("auto {} = {} >> {};", result_name, a_name, b_name));
            }
            GpuOp::PopCount(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("uint {} = popcount({});", result_name, a_name));
            }
            GpuOp::Clz(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("uint {} = clz({});", result_name, a_name));
            }
            GpuOp::Ctz(a) => {
                let a_name = self.get_var_name(*a);
                self.emit(&format!("uint {} = ctz({});", result_name, a_name));
            }

            // Conversions
            GpuOp::Trunc(a, ty) | GpuOp::ZExt(a, ty) | GpuOp::SExt(a, ty) => {
                let a_name = self.get_var_name(*a);
                let ty_str = self.type_to_msl(ty);
                self.emit(&format!(
                    "{} {} = static_cast<{}>({});",
                    ty_str, result_name, ty_str, a_name
                ));
            }
            GpuOp::FpTrunc(a, ty) | GpuOp::FpExt(a, ty) => {
                let a_name = self.get_var_name(*a);
                let ty_str = self.type_to_msl(ty);
                self.emit(&format!(
                    "{} {} = ({})({});",
                    ty_str, result_name, ty_str, a_name
                ));
            }
            GpuOp::FpToSi(a, ty) | GpuOp::FpToUi(a, ty) => {
                let a_name = self.get_var_name(*a);
                let ty_str = self.type_to_msl(ty);
                self.emit(&format!(
                    "{} {} = static_cast<{}>({});",
                    ty_str, result_name, ty_str, a_name
                ));
            }
            GpuOp::SiToFp(a, ty) | GpuOp::UiToFp(a, ty) => {
                let a_name = self.get_var_name(*a);
                let ty_str = self.type_to_msl(ty);
                self.emit(&format!(
                    "{} {} = static_cast<{}>({});",
                    ty_str, result_name, ty_str, a_name
                ));
            }
            GpuOp::Bitcast(a, ty) => {
                let a_name = self.get_var_name(*a);
                let ty_str = self.type_to_msl(ty);
                self.emit(&format!(
                    "{} {} = as_type<{}>({});",
                    ty_str, result_name, ty_str, a_name
                ));
            }

            // === Modern ML Type Conversions (BF16/FP8/F4) ===
            GpuOp::F32ToBF16(val) => {
                let v = self.get_var_name(*val);
                // Metal supports bfloat natively on Apple Silicon
                self.emit(&format!("bfloat {} = bfloat({});", result_name, v));
            }
            GpuOp::BF16ToF32(val) => {
                let v = self.get_var_name(*val);
                self.emit(&format!("float {} = float({});", result_name, v));
            }
            GpuOp::F32ToF8E4M3(val) => {
                let v = self.get_var_name(*val);
                // Metal doesn't have native FP8 - software emulation
                self.emit("// FP8 E4M3 conversion (software emulation)");
                self.emit(&format!(
                    "uchar {} = (uchar)(clamp({}, -448.0f, 448.0f) * 0.5625f + 128.0f);",
                    result_name, v
                ));
            }
            GpuOp::F8E4M3ToF32(val) => {
                let v = self.get_var_name(*val);
                self.emit("// FP8 E4M3 to F32 (software emulation)");
                self.emit(&format!(
                    "float {} = (float({}) - 128.0f) * 1.777f;",
                    result_name, v
                ));
            }
            GpuOp::F32ToF8E5M2(val) => {
                let v = self.get_var_name(*val);
                self.emit("// FP8 E5M2 conversion (software emulation)");
                self.emit(&format!(
                    "uchar {} = (uchar)(clamp({}, -57344.0f, 57344.0f) * 0.00017f + 128.0f);",
                    result_name, v
                ));
            }
            GpuOp::F8E5M2ToF32(val) => {
                let v = self.get_var_name(*val);
                self.emit("// FP8 E5M2 to F32 (software emulation)");
                self.emit(&format!(
                    "float {} = (float({}) - 128.0f) * 5882.35f;",
                    result_name, v
                ));
            }
            GpuOp::F32ToF4(val) => {
                let v = self.get_var_name(*val);
                self.emit("// F4 quantization (software emulation)");
                self.emit(&format!(
                    "uchar {} = (uchar)((as_type<uint>({}) >> 28) & 0x0F);",
                    result_name, v
                ));
            }
            GpuOp::F4ToF32(val) => {
                let v = self.get_var_name(*val);
                self.emit("// F4 dequantization (software emulation)");
                self.emit(&format!(
                    "float {} = as_type<float>((uint({} & 0x0F) << 28));",
                    result_name, v
                ));
            }

            // Packed ML Type Operations
            GpuOp::PackF8x2(lo, hi) => {
                let lo_v = self.get_var_name(*lo);
                let hi_v = self.get_var_name(*hi);
                self.emit(&format!(
                    "ushort {} = (ushort({}) << 8) | ushort({});",
                    result_name, hi_v, lo_v
                ));
            }
            GpuOp::UnpackF8x2Low(val) => {
                let v = self.get_var_name(*val);
                self.emit(&format!("uchar {} = uchar({} & 0x00FF);", result_name, v));
            }
            GpuOp::UnpackF8x2High(val) => {
                let v = self.get_var_name(*val);
                self.emit(&format!(
                    "uchar {} = uchar(({} >> 8) & 0x00FF);",
                    result_name, v
                ));
            }
            GpuOp::PackF4x2(lo, hi) => {
                let lo_v = self.get_var_name(*lo);
                let hi_v = self.get_var_name(*hi);
                self.emit(&format!(
                    "uchar {} = (({} & 0x0F) << 4) | ({} & 0x0F);",
                    result_name, hi_v, lo_v
                ));
            }
            GpuOp::UnpackF4x2Low(val) => {
                let v = self.get_var_name(*val);
                self.emit(&format!("uchar {} = {} & 0x0F;", result_name, v));
            }
            GpuOp::UnpackF4x2High(val) => {
                let v = self.get_var_name(*val);
                self.emit(&format!("uchar {} = ({} >> 4) & 0x0F;", result_name, v));
            }

            // Quantization Utilities
            GpuOp::QuantizeF32ToF8(val, _mode) => {
                let v = self.get_var_name(*val);
                // Software emulation using E4M3 format
                self.emit("// Quantize F32 to F8 (E4M3 format, software emulation)");
                self.emit(&format!(
                    "uchar {} = (uchar)(clamp({}, -448.0f, 448.0f) * 0.5625f + 128.0f);",
                    result_name, v
                ));
            }
            GpuOp::DequantizeF8ToF32(val, scale) => {
                let v = self.get_var_name(*val);
                self.emit("// Dequantize F8 to F32 (software emulation)");
                let base_expr = format!("(float({}) - 128.0f) * 1.777f", v);
                if let Some(scale_val) = scale {
                    let s = self.get_var_name(*scale_val);
                    self.emit(&format!("float {} = ({}) * {};", result_name, base_expr, s));
                } else {
                    self.emit(&format!("float {} = {};", result_name, base_expr));
                }
            }

            // === INT8/INT4 Quantization (Phase 11) ===
            GpuOp::QuantizeF32ToInt8 {
                value,
                scale,
                zero_point,
                symmetric,
            } => {
                let v = self.get_var_name(*value);
                let s = self.get_var_name(*scale);
                let zp = self.get_var_name(*zero_point);
                self.emit("// Quantize F32 to INT8");
                if *symmetric {
                    self.emit(&format!(
                        "char {} = (char)clamp(round({} / {}), -128.0f, 127.0f);",
                        result_name, v, s
                    ));
                } else {
                    self.emit(&format!(
                        "char {} = (char)clamp(round({} / {}) + float({}), -128.0f, 127.0f);",
                        result_name, v, s, zp
                    ));
                }
            }
            GpuOp::DequantizeInt8ToF32 {
                value,
                scale,
                zero_point,
            } => {
                let v = self.get_var_name(*value);
                let s = self.get_var_name(*scale);
                let zp = self.get_var_name(*zero_point);
                self.emit("// Dequantize INT8 to F32");
                self.emit(&format!(
                    "float {} = (float({}) - float({})) * {};",
                    result_name, v, zp, s
                ));
            }
            GpuOp::QuantizeF32ToUint8 {
                value,
                scale,
                zero_point,
            } => {
                let v = self.get_var_name(*value);
                let s = self.get_var_name(*scale);
                let zp = self.get_var_name(*zero_point);
                self.emit("// Quantize F32 to UINT8");
                self.emit(&format!(
                    "uchar {} = (uchar)clamp(round({} / {}) + float({}), 0.0f, 255.0f);",
                    result_name, v, s, zp
                ));
            }
            GpuOp::DequantizeUint8ToF32 {
                value,
                scale,
                zero_point,
            } => {
                let v = self.get_var_name(*value);
                let s = self.get_var_name(*scale);
                let zp = self.get_var_name(*zero_point);
                self.emit("// Dequantize UINT8 to F32");
                self.emit(&format!(
                    "float {} = (float({}) - float({})) * {};",
                    result_name, v, zp, s
                ));
            }
            GpuOp::QuantizeF32ToInt4 {
                value_lo,
                value_hi,
                scale,
                zero_point,
            } => {
                let v_lo = self.get_var_name(*value_lo);
                let v_hi = self.get_var_name(*value_hi);
                let s = self.get_var_name(*scale);
                let zp = self.get_var_name(*zero_point);
                self.emit("// Quantize F32 to INT4 (packed)");
                self.emit(&format!(
                    "int lo = (int)clamp(round({} / {}) + float({}), -8.0f, 7.0f);",
                    v_lo, s, zp
                ));
                self.emit(&format!(
                    "int hi = (int)clamp(round({} / {}) + float({}), -8.0f, 7.0f);",
                    v_hi, s, zp
                ));
                self.emit(&format!(
                    "uchar {} = (uchar)((lo & 0x0F) | ((hi & 0x0F) << 4));",
                    result_name
                ));
            }
            GpuOp::DequantizeInt4ToF32Lo {
                packed,
                scale,
                zero_point,
            } => {
                let p = self.get_var_name(*packed);
                let s = self.get_var_name(*scale);
                let zp = self.get_var_name(*zero_point);
                self.emit("// Dequantize INT4 (low nibble) to F32");
                self.emit(&format!("int lo = ({} & 0x0F); if (lo > 7) lo -= 16;", p));
                self.emit(&format!(
                    "float {} = (float(lo) - float({})) * {};",
                    result_name, zp, s
                ));
            }
            GpuOp::DequantizeInt4ToF32Hi {
                packed,
                scale,
                zero_point,
            } => {
                let p = self.get_var_name(*packed);
                let s = self.get_var_name(*scale);
                let zp = self.get_var_name(*zero_point);
                self.emit("// Dequantize INT4 (high nibble) to F32");
                self.emit(&format!(
                    "int hi = (({} >> 4) & 0x0F); if (hi > 7) hi -= 16;",
                    p
                ));
                self.emit(&format!(
                    "float {} = (float(hi) - float({})) * {};",
                    result_name, zp, s
                ));
            }
            GpuOp::Dp4a { a, b, c }
            | GpuOp::Dp4aUnsigned { a, b, c }
            | GpuOp::Dp4aSU { a, b, c } => {
                let a_v = self.get_var_name(*a);
                let b_v = self.get_var_name(*b);
                let c_v = self.get_var_name(*c);
                self.emit("// INT8 dot product (dp4a) - software emulation");
                self.emit(&format!(
                    "int {} = {} + (int)((char)({} >> 0) & 0xFF) * (int)((char)({} >> 0) & 0xFF)",
                    result_name, c_v, a_v, b_v
                ));
                self.emit(&format!(
                    "    + (int)((char)({} >> 8) & 0xFF) * (int)((char)({} >> 8) & 0xFF)",
                    a_v, b_v
                ));
                self.emit(&format!(
                    "    + (int)((char)({} >> 16) & 0xFF) * (int)((char)({} >> 16) & 0xFF)",
                    a_v, b_v
                ));
                self.emit(&format!(
                    "    + (int)((char)({} >> 24) & 0xFF) * (int)((char)({} >> 24) & 0xFF);",
                    a_v, b_v
                ));
            }
            GpuOp::Int8MatMul { c, .. } => {
                let c_v = self.get_var_name(*c);
                self.emit("// INT8 MatMul not directly supported on Metal");
                self.emit("// Use simdgroup_matrix with conversion or MPSGraph");
                self.emit(&format!(
                    "int {} = {}; // INT8 MatMul placeholder",
                    result_name, c_v
                ));
            }
            GpuOp::QuantizePerChannel { .. } => {
                self.emit("// Per-channel quantization - implemented at higher level");
                self.emit(&format!("uchar {} = 0; // Placeholder", result_name));
            }
            GpuOp::DequantizePerChannel { .. } => {
                self.emit("// Per-channel dequantization - implemented at higher level");
                self.emit(&format!("float {} = 0.0f; // Placeholder", result_name));
            }
            GpuOp::ComputeQuantScale {
                min_val,
                max_val,
                num_bits,
                symmetric,
            } => {
                let min_v = self.get_var_name(*min_val);
                let max_v = self.get_var_name(*max_val);
                self.emit("// Compute quantization scale");
                if *symmetric {
                    let qmax = (1 << (num_bits - 1)) - 1;
                    self.emit(&format!(
                        "float {} = max(abs({}), abs({})) / {}.0f;",
                        result_name, min_v, max_v, qmax
                    ));
                } else {
                    let qmax = (1 << *num_bits) - 1;
                    self.emit(&format!(
                        "float {} = ({} - {}) / {}.0f;",
                        result_name, max_v, min_v, qmax
                    ));
                }
            }
            GpuOp::ComputeZeroPoint { min_val, scale, .. } => {
                let min_v = self.get_var_name(*min_val);
                let s = self.get_var_name(*scale);
                self.emit("// Compute zero point");
                self.emit(&format!(
                    "int {} = (int)round(-{} / {});",
                    result_name, min_v, s
                ));
            }
            GpuOp::FindMinMax { .. } => {
                self.emit("// FindMinMax - reduction implemented at higher level");
                self.emit(&format!(
                    "float2 {} = float2(0.0f, 0.0f); // Placeholder",
                    result_name
                ));
            }
            GpuOp::Requantize {
                value,
                in_scale,
                in_zero_point,
                out_scale,
                out_zero_point,
            } => {
                let v = self.get_var_name(*value);
                let in_s = self.get_var_name(*in_scale);
                let in_zp = self.get_var_name(*in_zero_point);
                let out_s = self.get_var_name(*out_scale);
                let out_zp = self.get_var_name(*out_zero_point);
                self.emit("// Requantize from one scale to another");
                self.emit(&format!(
                    "float tmp = (float({}) - float({})) * {};",
                    v, in_zp, in_s
                ));
                self.emit(&format!(
                    "char {} = (char)clamp(round(tmp / {}) + float({}), -128.0f, 127.0f);",
                    result_name, out_s, out_zp
                ));
            }

            // === Blackwell Features (CUDA sm_100+ - Not available on Metal) ===
            // Metal doesn't support TMA, WGMMA, or NVLink features
            GpuOp::TmaLoadAsync { .. } => {
                self.emit("// TMA not available on Metal - use threadgroup memory instead");
                self.emit(&format!("auto {} = 0; // TMA placeholder", result_name));
            }
            GpuOp::TmaStoreAsync { .. } => {
                self.emit("// TMA not available on Metal");
            }
            GpuOp::TmaMulticastLoad { .. } => {
                self.emit("// TMA multicast not available on Metal");
                self.emit(&format!("auto {} = 0; // TMA placeholder", result_name));
            }
            GpuOp::TmaReduceAsync { .. } => {
                self.emit("// TMA reduce not available on Metal");
            }

            // WGMMA operations - use simdgroup_matrix on Metal
            GpuOp::WgmmaFp4 { a, b, c, .. } => {
                let a_v = self.get_var_name(*a);
                let b_v = self.get_var_name(*b);
                let c_v = self.get_var_name(*c);
                self.emit("// WGMMA FP4 not available - using simdgroup_matrix placeholder");
                self.emit(&format!(
                    "float {} = {}; // FP4 WGMMA placeholder",
                    result_name, c_v
                ));
                self.emit(&format!(
                    "// Would use simdgroup_matrix with {} and {}",
                    a_v, b_v
                ));
            }
            GpuOp::WgmmaFp8 { a, b, c, .. } => {
                let a_v = self.get_var_name(*a);
                let b_v = self.get_var_name(*b);
                let c_v = self.get_var_name(*c);
                self.emit("// WGMMA FP8 not available - using simdgroup_matrix placeholder");
                self.emit(&format!(
                    "float {} = {}; // FP8 WGMMA placeholder",
                    result_name, c_v
                ));
                self.emit(&format!(
                    "// Would use simdgroup_matrix with {} and {}",
                    a_v, b_v
                ));
            }
            GpuOp::WgmmaBf16 { a, b, c, .. } => {
                let a_v = self.get_var_name(*a);
                let b_v = self.get_var_name(*b);
                let c_v = self.get_var_name(*c);
                self.emit("// WGMMA BF16 - using simdgroup_matrix");
                self.emit("// simdgroup_matrix<bfloat, 8, 8> ma, mb, mc;");
                self.emit(&format!("// ma.load({});", a_v));
                self.emit(&format!("// mb.load({});", b_v));
                self.emit("// mc = ma * mb;");
                self.emit(&format!(
                    "float {} = {}; // BF16 WGMMA placeholder",
                    result_name, c_v
                ));
            }

            // Transformer Engine - not available on Metal
            GpuOp::TransformerEngineFusedAttention { .. } => {
                self.emit("// Transformer Engine not available on Metal");
                self.emit("// Use MPSGraph attention operations instead");
            }
            GpuOp::TransformerEngineFp8Gemm { .. } => {
                self.emit("// Transformer Engine FP8 GEMM not available on Metal");
                self.emit("// Use simdgroup_matrix with BF16 instead");
            }

            // Decompression Engine - not available on Metal
            GpuOp::DecompressLz4 { .. } => {
                self.emit("// Hardware LZ4 decompression not available on Metal");
                self.emit("// Use software decompression instead");
            }
            GpuOp::DecompressSnappy { .. } => {
                self.emit("// Hardware Snappy decompression not available on Metal");
            }
            GpuOp::DecompressDeflate { .. } => {
                self.emit("// Hardware Deflate decompression not available on Metal");
            }

            // Cluster operations - not available on Metal
            GpuOp::ClusterId => {
                self.emit("// Cluster ID not available on Metal");
                self.emit(&format!("uint {} = 0; // No cluster support", result_name));
            }
            GpuOp::ClusterDim => {
                self.emit("// Cluster dimension not available on Metal");
                self.emit(&format!("uint {} = 1; // No cluster support", result_name));
            }
            GpuOp::BlockIdInCluster => {
                self.emit("// Block ID in cluster not available on Metal");
                self.emit(&format!("uint {} = 0; // No cluster support", result_name));
            }
            GpuOp::ClusterBarrier => {
                self.emit("// Cluster barrier not available on Metal - using threadgroup_barrier");
                self.emit("threadgroup_barrier(mem_flags::mem_threadgroup);");
            }
            GpuOp::ClusterArrive(_) => {
                self.emit("// Cluster arrive not available on Metal");
            }
            GpuOp::ClusterWait(_) => {
                self.emit("// Cluster wait not available on Metal");
            }

            // NVLink operations - not available on Metal
            GpuOp::NvlinkRead { .. } => {
                self.emit("// NVLink not available on Metal");
                self.emit(&format!("auto {} = 0; // NVLink placeholder", result_name));
            }
            GpuOp::NvlinkWrite { .. } => {
                self.emit("// NVLink not available on Metal");
            }
            GpuOp::NvlinkAtomicAdd { .. } => {
                self.emit("// NVLink atomic not available on Metal");
                self.emit(&format!("auto {} = 0; // NVLink placeholder", result_name));
            }

            // Memory
            GpuOp::Load(ptr, _space) => {
                let ptr_name = self.get_var_name(*ptr);
                self.emit(&format!("auto {} = *{};", result_name, ptr_name));
            }
            GpuOp::Store(ptr, val, _space) => {
                let ptr_name = self.get_var_name(*ptr);
                let val_name = self.get_var_name(*val);
                self.emit(&format!("*{} = {};", ptr_name, val_name));
            }

            // Atomics
            GpuOp::AtomicAdd(ptr, val) => {
                let ptr_name = self.get_var_name(*ptr);
                let val_name = self.get_var_name(*val);
                self.emit(&format!(
                    "auto {} = atomic_fetch_add_explicit((device atomic_int*){}, {}, memory_order_relaxed);",
                    result_name, ptr_name, val_name
                ));
            }
            GpuOp::AtomicSub(ptr, val) => {
                let ptr_name = self.get_var_name(*ptr);
                let val_name = self.get_var_name(*val);
                self.emit(&format!(
                    "auto {} = atomic_fetch_sub_explicit((device atomic_int*){}, {}, memory_order_relaxed);",
                    result_name, ptr_name, val_name
                ));
            }
            GpuOp::AtomicMin(ptr, val) => {
                let ptr_name = self.get_var_name(*ptr);
                let val_name = self.get_var_name(*val);
                self.emit(&format!(
                    "auto {} = atomic_fetch_min_explicit((device atomic_int*){}, {}, memory_order_relaxed);",
                    result_name, ptr_name, val_name
                ));
            }
            GpuOp::AtomicMax(ptr, val) => {
                let ptr_name = self.get_var_name(*ptr);
                let val_name = self.get_var_name(*val);
                self.emit(&format!(
                    "auto {} = atomic_fetch_max_explicit((device atomic_int*){}, {}, memory_order_relaxed);",
                    result_name, ptr_name, val_name
                ));
            }
            GpuOp::AtomicExch(ptr, val) => {
                let ptr_name = self.get_var_name(*ptr);
                let val_name = self.get_var_name(*val);
                self.emit(&format!(
                    "auto {} = atomic_exchange_explicit((device atomic_int*){}, {}, memory_order_relaxed);",
                    result_name, ptr_name, val_name
                ));
            }
            GpuOp::AtomicCas(ptr, cmp, val) => {
                let ptr_name = self.get_var_name(*ptr);
                let cmp_name = self.get_var_name(*cmp);
                let val_name = self.get_var_name(*val);
                self.emit(&format!("auto {}_expected = {};", result_name, cmp_name));
                self.emit(&format!(
                    "atomic_compare_exchange_weak_explicit((device atomic_int*){}, &{}_expected, {}, memory_order_relaxed, memory_order_relaxed);",
                    ptr_name, result_name, val_name
                ));
                self.emit(&format!("auto {} = {}_expected;", result_name, result_name));
            }

            // Address computation
            GpuOp::GetElementPtr(base, indices) => {
                let base_name = self.get_var_name(*base);
                let idx_strs: Vec<String> = indices.iter().map(|i| self.get_var_name(*i)).collect();
                if idx_strs.len() == 1 {
                    self.emit(&format!(
                        "auto {} = {} + {};",
                        result_name, base_name, idx_strs[0]
                    ));
                } else {
                    self.emit(&format!(
                        "auto {} = &{}[{}];",
                        result_name,
                        base_name,
                        idx_strs.join("][")
                    ));
                }
            }

            // GPU intrinsics
            GpuOp::ThreadIdX => self.emit(&format!("uint {} = threadIdx_x;", result_name)),
            GpuOp::ThreadIdY => self.emit(&format!("uint {} = threadIdx_y;", result_name)),
            GpuOp::ThreadIdZ => self.emit(&format!("uint {} = threadIdx_z;", result_name)),
            GpuOp::BlockIdX => self.emit(&format!("uint {} = blockIdx_x;", result_name)),
            GpuOp::BlockIdY => self.emit(&format!("uint {} = blockIdx_y;", result_name)),
            GpuOp::BlockIdZ => self.emit(&format!("uint {} = blockIdx_z;", result_name)),
            GpuOp::BlockDimX => self.emit(&format!("uint {} = blockDim_x;", result_name)),
            GpuOp::BlockDimY => self.emit(&format!("uint {} = blockDim_y;", result_name)),
            GpuOp::BlockDimZ => self.emit(&format!("uint {} = blockDim_z;", result_name)),
            GpuOp::GridDimX => self.emit(&format!(
                "uint {} = 0; // gridDim not directly available",
                result_name
            )),
            GpuOp::GridDimY => self.emit(&format!(
                "uint {} = 0; // gridDim not directly available",
                result_name
            )),
            GpuOp::GridDimZ => self.emit(&format!(
                "uint {} = 0; // gridDim not directly available",
                result_name
            )),
            GpuOp::WarpId => self.emit(&format!("uint {} = simd_group_id;", result_name)),
            GpuOp::LaneId => self.emit(&format!("uint {} = simd_lane_id;", result_name)),
            GpuOp::WarpSize => self.emit(&format!("uint {} = 32;", result_name)),

            // Synchronization
            GpuOp::SyncThreads => {
                self.emit("threadgroup_barrier(mem_flags::mem_threadgroup);");
            }
            GpuOp::SyncWarp(_mask) => {
                self.emit("simdgroup_barrier(mem_flags::mem_none);");
            }
            GpuOp::MemoryFence(space) => {
                let flag = match space {
                    MemorySpace::Shared => "mem_flags::mem_threadgroup",
                    MemorySpace::Global => "mem_flags::mem_device",
                    _ => "mem_flags::mem_none",
                };
                self.emit(&format!("threadgroup_barrier({});", flag));
            }

            // Simdgroup (warp) operations
            GpuOp::WarpShuffle(val, lane) => {
                let val_name = self.get_var_name(*val);
                let lane_name = self.get_var_name(*lane);
                self.emit(&format!(
                    "auto {} = simd_shuffle({}, {});",
                    result_name, val_name, lane_name
                ));
            }
            GpuOp::WarpShuffleUp(val, delta) => {
                let val_name = self.get_var_name(*val);
                let delta_name = self.get_var_name(*delta);
                self.emit(&format!(
                    "auto {} = simd_shuffle_up({}, {});",
                    result_name, val_name, delta_name
                ));
            }
            GpuOp::WarpShuffleDown(val, delta) => {
                let val_name = self.get_var_name(*val);
                let delta_name = self.get_var_name(*delta);
                self.emit(&format!(
                    "auto {} = simd_shuffle_down({}, {});",
                    result_name, val_name, delta_name
                ));
            }
            GpuOp::WarpShuffleXor(val, mask) => {
                let val_name = self.get_var_name(*val);
                let mask_name = self.get_var_name(*mask);
                self.emit(&format!(
                    "auto {} = simd_shuffle_xor({}, {});",
                    result_name, val_name, mask_name
                ));
            }
            GpuOp::WarpVote(vote_op, val) => {
                let val_name = self.get_var_name(*val);
                let func = match vote_op {
                    WarpVoteOp::All => "simd_all",
                    WarpVoteOp::Any => "simd_any",
                    WarpVoteOp::Ballot => "simd_ballot",
                    WarpVoteOp::Eq => "simd_all",
                };
                self.emit(&format!("auto {} = {}({});", result_name, func, val_name));
            }
            GpuOp::WarpReduce(reduce_op, val) => {
                let val_name = self.get_var_name(*val);
                let func = match reduce_op {
                    WarpReduceOp::Add => "simd_sum",
                    WarpReduceOp::Min => "simd_min",
                    WarpReduceOp::Max => "simd_max",
                    WarpReduceOp::And => "simd_and",
                    WarpReduceOp::Or => "simd_or",
                    WarpReduceOp::Xor => "simd_xor",
                };
                self.emit(&format!("auto {} = {}({});", result_name, func, val_name));
            }

            // Control flow
            GpuOp::Select(cond, t, f) => {
                let cond_name = self.get_var_name(*cond);
                let t_name = self.get_var_name(*t);
                let f_name = self.get_var_name(*f);
                self.emit(&format!(
                    "auto {} = {} ? {} : {};",
                    result_name, cond_name, t_name, f_name
                ));
            }

            // Function call
            GpuOp::Call(func_name, args) => {
                let args_str: Vec<String> = args.iter().map(|a| self.get_var_name(*a)).collect();
                self.emit(&format!(
                    "auto {} = {}({});",
                    result_name,
                    func_name,
                    args_str.join(", ")
                ));
            }

            // Parameter
            GpuOp::Param(idx) => {
                self.emit(&format!("// param {} -> {}", idx, result_name));
            }

            // Shared memory address
            GpuOp::SharedAddr(name) => {
                self.emit(&format!("auto {} = &{}[0];", result_name, name));
            }

            // Phi nodes (handled by SSA deconstruction)
            GpuOp::Phi(_) => {
                self.emit(&format!("// phi -> {} (handled separately)", result_name));
            }

            // Texture/Surface (not yet implemented)
            GpuOp::TexFetch(_, _)
            | GpuOp::TexFetch2D(_, _, _)
            | GpuOp::SurfRead(_, _)
            | GpuOp::SurfWrite(_, _, _) => {
                self.emit(&format!(
                    "// Texture op -> {} (not yet implemented)",
                    result_name
                ));
            }

            // ================================================================
            // Cooperative Groups -> Metal simdgroup/threadgroup primitives
            // ================================================================

            // Get cooperative group handle at specified scope
            GpuOp::CoopThisGroup(scope) => {
                // In Metal, group handle is conceptual - store scope for context
                let scope_val = match scope {
                    CooperativeScope::Thread => 0,
                    CooperativeScope::Warp => 1,    // simdgroup
                    CooperativeScope::Block => 2,   // threadgroup
                    CooperativeScope::Cluster => 3, // device-level
                    CooperativeScope::Grid => 4,
                    CooperativeScope::Coalesced => 5,
                    CooperativeScope::TiledPartition(n) => 0x100 | *n,
                };
                self.emit(&format!(
                    "uint {} = {}; // CoopThisGroup scope={:?}",
                    result_name, scope_val, scope
                ));
            }

            // Get group size
            GpuOp::CoopGroupSize(group) => {
                let grp = self.get_var_name(*group);
                self.emit(&format!(
                    "uint {} = ({} == 1) ? simd_size : (({} == 2) ? blockDim_x : 1); // CoopGroupSize",
                    result_name, grp, grp
                ));
            }

            // Get thread rank within group
            GpuOp::CoopThreadRank(group) => {
                let grp = self.get_var_name(*group);
                self.emit(&format!(
                    "uint {} = ({} == 1) ? simd_lane_id : (({} == 2) ? threadIdx_x : 0); // CoopThreadRank",
                    result_name, grp, grp
                ));
            }

            // Check if this thread is group leader
            GpuOp::CoopIsLeader(group) => {
                let grp = self.get_var_name(*group);
                self.emit(&format!(
                    "bool {} = ({} == 1) ? (simd_lane_id == 0) : (({} == 2) ? (threadIdx_x == 0) : true); // CoopIsLeader",
                    result_name, grp, grp
                ));
            }

            // Synchronize group
            GpuOp::CoopSync(group) => {
                let grp = self.get_var_name(*group);
                self.emit(&format!("// CoopSync for group {}", grp));
                self.emit(&format!(
                    "if ({} == 1) {{ simdgroup_barrier(mem_flags::mem_threadgroup); }}",
                    grp
                ));
                self.emit(&format!(
                    "if ({} == 2) {{ threadgroup_barrier(mem_flags::mem_threadgroup); }}",
                    grp
                ));
            }

            // Shuffle broadcast
            GpuOp::CoopShfl(_group, val, src_rank) => {
                let v = self.get_var_name(*val);
                let src = self.get_var_name(*src_rank);
                self.emit(&format!(
                    "auto {} = simd_broadcast({}, {}); // CoopShfl",
                    result_name, v, src
                ));
            }

            // Shuffle with index
            GpuOp::CoopShflIdx(_group, val, idx) => {
                let v = self.get_var_name(*val);
                let i = self.get_var_name(*idx);
                self.emit(&format!(
                    "auto {} = simd_shuffle({}, {}); // CoopShflIdx",
                    result_name, v, i
                ));
            }

            // Shuffle up
            GpuOp::CoopShflUp(_group, val, delta) => {
                let v = self.get_var_name(*val);
                let d = self.get_var_name(*delta);
                self.emit(&format!(
                    "auto {} = simd_shuffle_up({}, {}); // CoopShflUp",
                    result_name, v, d
                ));
            }

            // Shuffle down
            GpuOp::CoopShflDown(_group, val, delta) => {
                let v = self.get_var_name(*val);
                let d = self.get_var_name(*delta);
                self.emit(&format!(
                    "auto {} = simd_shuffle_down({}, {}); // CoopShflDown",
                    result_name, v, d
                ));
            }

            // Shuffle XOR (butterfly)
            GpuOp::CoopShflXor(_group, val, mask) => {
                let v = self.get_var_name(*val);
                let m = self.get_var_name(*mask);
                self.emit(&format!(
                    "auto {} = simd_shuffle_xor({}, {}); // CoopShflXor",
                    result_name, v, m
                ));
            }

            // Collective reduce
            GpuOp::CoopReduce(_group, val, op) => {
                let v = self.get_var_name(*val);
                let func = match op {
                    CoopReduceOp::Add => "simd_sum",
                    CoopReduceOp::Min => "simd_min",
                    CoopReduceOp::Max => "simd_max",
                    CoopReduceOp::And => "simd_and",
                    CoopReduceOp::Or => "simd_or",
                    CoopReduceOp::Xor => "simd_xor",
                    CoopReduceOp::Mul => "simd_product",
                };
                self.emit(&format!(
                    "auto {} = {}({}); // CoopReduce {:?}",
                    result_name, func, v, op
                ));
            }

            // Inclusive scan
            GpuOp::CoopInclusiveScan(_group, val, op) => {
                let v = self.get_var_name(*val);
                let func = match op {
                    CoopReduceOp::Add => "simd_prefix_inclusive_sum",
                    CoopReduceOp::Mul => "simd_prefix_inclusive_product",
                    _ => "simd_prefix_inclusive_sum", // Fallback
                };
                self.emit(&format!(
                    "auto {} = {}({}); // CoopInclusiveScan {:?}",
                    result_name, func, v, op
                ));
            }

            // Exclusive scan
            GpuOp::CoopExclusiveScan(_group, val, op) => {
                let v = self.get_var_name(*val);
                let func = match op {
                    CoopReduceOp::Add => "simd_prefix_exclusive_sum",
                    CoopReduceOp::Mul => "simd_prefix_exclusive_product",
                    _ => "simd_prefix_exclusive_sum", // Fallback
                };
                self.emit(&format!(
                    "auto {} = {}({}); // CoopExclusiveScan {:?}",
                    result_name, func, v, op
                ));
            }

            // Collective ballot
            GpuOp::CoopBallot(_group, pred) => {
                let p = self.get_var_name(*pred);
                self.emit(&format!(
                    "simd_vote::vote_t {} = simd_ballot({}); // CoopBallot",
                    result_name, p
                ));
            }

            // All threads predicate true
            GpuOp::CoopAll(_group, pred) => {
                let p = self.get_var_name(*pred);
                self.emit(&format!(
                    "bool {} = simd_all({}); // CoopAll",
                    result_name, p
                ));
            }

            // Any thread predicate true
            GpuOp::CoopAny(_group, pred) => {
                let p = self.get_var_name(*pred);
                self.emit(&format!(
                    "bool {} = simd_any({}); // CoopAny",
                    result_name, p
                ));
            }

            // Partition into tiles
            GpuOp::CoopPartitionTiled(_group, tile_size) => {
                let scope_val = 0x100 | *tile_size;
                self.emit(&format!(
                    "uint {} = {}; // CoopPartitionTiled size={}",
                    result_name, scope_val, tile_size
                ));
            }

            // Binary partition
            GpuOp::CoopPartitionBinary(_group, pred) => {
                let p = self.get_var_name(*pred);
                self.emit(&format!(
                    "simd_vote::vote_t {} = simd_ballot({}); // CoopPartitionBinary",
                    result_name, p
                ));
            }

            // Labeled partition
            GpuOp::CoopPartitionLabeled(_group, label) => {
                let l = self.get_var_name(*label);
                self.emit(&format!(
                    "// CoopPartitionLabeled by {} - using ballot as approximation",
                    l
                ));
                self.emit(&format!(
                    "simd_vote::vote_t {} = simd_ballot({} == simd_broadcast({}, 0));",
                    result_name, l, l
                ));
            }

            // Coalesced threads (active mask)
            GpuOp::CoopCoalescedThreads => {
                self.emit(&format!(
                    "simd_vote::vote_t {} = simd_active_threads_mask(); // CoopCoalescedThreads",
                    result_name
                ));
            }

            // Elect leader
            GpuOp::CoopElect(_group) => {
                self.emit(&format!(
                    "bool {} = simd_is_first(); // CoopElect",
                    result_name
                ));
            }

            // Memory fence
            GpuOp::CoopMemoryFence(group) => {
                let grp = self.get_var_name(*group);
                self.emit(&format!("// CoopMemoryFence for group {}", grp));
                self.emit(&format!(
                    "if ({} == 1) {{ simdgroup_barrier(mem_flags::mem_device); }}",
                    grp
                ));
                self.emit(&format!(
                    "if ({} == 2) {{ threadgroup_barrier(mem_flags::mem_device); }}",
                    grp
                ));
            }

            // Bio operations (quaternion, DNA, GF4, transmission) - call device functions
            GpuOp::QuatMul(_, _)
            | GpuOp::QuatConj(_)
            | GpuOp::QuatNormSq(_)
            | GpuOp::QuatNormalize(_)
            | GpuOp::QuatSlerp(_, _, _)
            | GpuOp::DnaComplement(_)
            | GpuOp::Gf4Add(_, _)
            | GpuOp::Gf4Mul(_, _)
            | GpuOp::TransmissionCompose(_, _)
            | GpuOp::TransmissionDistort(_, _, _, _, _) => {
                self.emit(&format!(
                    "// Bio op -> {} (implement via device function)",
                    result_name
                ));
            }

            // Atomics not handled above
            GpuOp::AtomicAnd(_, _) | GpuOp::AtomicOr(_, _) | GpuOp::AtomicXor(_, _) => {
                self.emit(&format!(
                    "// Bitwise atomic -> {} (not yet implemented)",
                    result_name
                ));
            }

            // Int/Ptr conversions
            GpuOp::PtrToInt(_) | GpuOp::IntToPtr(_, _) => {
                self.emit(&format!(
                    "// Ptr conversion -> {} (not yet implemented)",
                    result_name
                ));
            }

            // Bitwise ops handled but duplicated
            GpuOp::Xor(_, _) => {
                self.emit(&format!("// Xor -> {} (not yet implemented)", result_name));
            }

            // Warp match
            GpuOp::WarpMatch(_) => {
                self.emit(&format!(
                    "// WarpMatch -> {} (not directly supported in Metal)",
                    result_name
                ));
            }

            // === Debug/Profiling Operations ===

            // Printf is not natively supported in Metal compute shaders
            // Use os_log or print buffer pattern instead
            GpuOp::Printf(fmt_id, args) => {
                self.emit(&format!(
                    "// gpu.printf (format_id={}, {} args)",
                    fmt_id,
                    args.len()
                ));
                self.emit("// Metal does not support printf in compute shaders");
                self.emit("// Use debug buffer pattern: store values to buffer, read on host");
                self.emit(&format!(
                    "auto {} = 0; // printf not supported",
                    result_name
                ));
            }

            // Assert - use Metal's assert or trap
            GpuOp::Assert(cond, msg_id) => {
                let cond_name = self.get_var_name(*cond);
                if let Some(msg) = msg_id {
                    self.emit(&format!("// gpu.assert (msg_id={})", msg));
                }
                self.emit(&format!("if (!{}) {{ /* assertion failed */ }}", cond_name));
            }

            // Trap - Metal doesn't have a direct trap instruction
            GpuOp::Trap => {
                self.emit("// gpu.trap - Metal doesn't support trap");
                self.emit("// Use infinite loop or return as alternative");
            }

            // Breakpoint - not supported in Metal
            GpuOp::Brkpt => {
                self.emit("// gpu.brkpt - breakpoints not supported in Metal compute");
            }

            // Clock counter - Metal doesn't expose raw clock
            GpuOp::Clock => {
                self.emit("// gpu.clock - not directly available in Metal");
                self.emit(&format!(
                    "uint64_t {} = 0; // placeholder for clock",
                    result_name
                ));
            }

            // Global timer - not available in Metal
            GpuOp::GlobalTimer => {
                self.emit("// gpu.globaltimer - not available in Metal");
                self.emit(&format!(
                    "uint64_t {} = 0; // placeholder for globaltimer",
                    result_name
                ));
            }

            // Performance event - not directly available
            GpuOp::PmEvent(event_id) => {
                self.emit(&format!(
                    "// pmevent {} - use Metal GPU counters via API",
                    event_id
                ));
            }

            // ========================================
            // Tile Programming Operations (CUDA 13)
            // Metal does not have native tile support like CUDA's cooperative groups + WMMA.
            // These emit placeholders directing users to use threadgroup memory manually.
            // ========================================
            GpuOp::TileCreate {
                tile_m,
                tile_n,
                element_type,
                layout,
                ..
            } => {
                let layout_str = match layout {
                    TileLayout::RowMajor => "row_major",
                    TileLayout::ColMajor => "col_major",
                    TileLayout::Swizzled { .. } => "swizzled",
                };
                self.emit(&format!(
                    "// TILE_CREATE: {}x{} {:?} ({})",
                    tile_m, tile_n, element_type, layout_str
                ));
                self.emit("// Metal: Use threadgroup memory manually instead");
                self.emit(&format!(
                    "threadgroup float tile_smem[{} * {}];",
                    tile_m, tile_n
                ));
                self.emit(&format!(
                    "{} {} = tile_smem;",
                    "threadgroup float*", result_name
                ));
            }

            GpuOp::TileLoad {
                tile: _,
                src_ptr: _,
                stride: _,
                barrier: _,
            } => {
                self.emit("// TILE_LOAD: Metal does not support TMA-style bulk transfers");
                self.emit("// Use manual coalesced loads into threadgroup memory");
                self.emit("// Example: tile_smem[thread_idx] = src_ptr[thread_idx];");
                self.emit("threadgroup_barrier(mem_flags::mem_threadgroup);");
            }

            GpuOp::TileStore {
                tile: _,
                dst_ptr: _,
                stride: _,
                barrier: _,
            } => {
                self.emit("// TILE_STORE: Metal does not support TMA-style bulk transfers");
                self.emit("// Use manual coalesced stores from threadgroup memory");
                self.emit("// Example: dst_ptr[thread_idx] = tile_smem[thread_idx];");
                self.emit("threadgroup_barrier(mem_flags::mem_threadgroup);");
            }

            GpuOp::TileMma {
                c: _,
                a: _,
                b: _,
                tile_m,
                tile_n,
                tile_k,
            } => {
                self.emit(&format!("// TILE_MMA: {}x{}x{}", tile_m, tile_n, tile_k));
                #[allow(clippy::collapsible_else_if)]
                if self.config.gpu_family >= MetalGpuFamily::Apple7 {
                    self.emit("// Metal M1+: Use simdgroup_matrix for Tensor Core-like operations");
                    self.emit("// simdgroup_float8x8 a_frag, b_frag, c_frag;");
                    self.emit("// simdgroup_multiply_accumulate(c_frag, a_frag, b_frag, c_frag);");
                } else {
                    self.emit("// Metal pre-M1: simdgroup_matrix not available, use manual SIMD");
                }
            }

            GpuOp::TileSync(_tile) => {
                self.emit("threadgroup_barrier(mem_flags::mem_threadgroup);");
            }

            GpuOp::TileGetElement {
                tile: _,
                row: _,
                col: _,
            } => {
                self.emit("// TILE_GET_ELEMENT: Access threadgroup memory element");
                self.emit(&format!(
                    "auto {} = tile_smem[row * tile_n + col];",
                    result_name
                ));
            }

            GpuOp::TileSetElement {
                tile: _,
                row: _,
                col: _,
                value: _,
            } => {
                self.emit("// TILE_SET_ELEMENT: Set threadgroup memory element");
                self.emit("// tile_smem[row * tile_n + col] = value;");
            }

            GpuOp::TileFill { tile: _, value: _ } => {
                self.emit("// TILE_FILL: Fill all elements with scalar");
                self.emit(
                    "// for (uint i = thread_idx; i < tile_m * tile_n; i += threads_per_group)",
                );
                self.emit("//     tile_smem[i] = value;");
                self.emit("threadgroup_barrier(mem_flags::mem_threadgroup);");
            }

            GpuOp::TileReduce { tile: _, reduce_op } => {
                let op_name = match reduce_op {
                    CoopReduceOp::Add => "add",
                    CoopReduceOp::Mul => "mul",
                    CoopReduceOp::Min => "min",
                    CoopReduceOp::Max => "max",
                    CoopReduceOp::And => "and",
                    CoopReduceOp::Or => "or",
                    CoopReduceOp::Xor => "xor",
                };
                self.emit(&format!("// TILE_REDUCE: {} reduction", op_name));
                self.emit("// Use simd_sum/simd_min/simd_max + tree reduction");
                self.emit(&format!(
                    "float {} = 0.0f; // placeholder for {} reduction",
                    result_name, op_name
                ));
            }

            GpuOp::TileTranspose(_tile) => {
                self.emit("// TILE_TRANSPOSE: Transpose tile in-place");
                self.emit("// Requires diagonal swap through threadgroup memory");
                self.emit("threadgroup_barrier(mem_flags::mem_threadgroup);");
            }

            GpuOp::TileM(_tile) => {
                self.emit(&format!(
                    "uint {} = 0; // TILE_M: should be constant-folded",
                    result_name
                ));
            }

            GpuOp::TileN(_tile) => {
                self.emit(&format!(
                    "uint {} = 0; // TILE_N: should be constant-folded",
                    result_name
                ));
            }
        }
    }

    fn emit_terminator(&mut self, term: &GpuTerminator) {
        match term {
            GpuTerminator::Br(target) => {
                self.emit(&format!("goto BB{};", target.0));
            }
            GpuTerminator::CondBr(cond, t, f) => {
                let cond_name = self.get_var_name(*cond);
                self.emit(&format!(
                    "if ({}) goto BB{}; else goto BB{};",
                    cond_name, t.0, f.0
                ));
            }
            GpuTerminator::Return(val) => {
                let val_name = self.get_var_name(*val);
                self.emit(&format!("return {};", val_name));
            }
            GpuTerminator::ReturnVoid => {
                self.emit("return;");
            }
            GpuTerminator::Unreachable => {
                self.emit("// unreachable");
            }
        }
    }

    fn get_or_create_var_name(&mut self, id: ValueId) -> String {
        if let Some(name) = self.value_names.get(&id) {
            name.clone()
        } else {
            let name = format!("v{}", id.0);
            self.value_names.insert(id, name.clone());
            name
        }
    }

    fn get_var_name(&self, id: ValueId) -> String {
        self.value_names
            .get(&id)
            .cloned()
            .unwrap_or_else(|| format!("v{}", id.0))
    }

    fn type_to_msl(&self, ty: &GpuType) -> &'static str {
        match ty {
            GpuType::Void => "void",
            GpuType::Bool => "bool",
            GpuType::I8 => "char",
            GpuType::I16 => "short",
            GpuType::I32 => "int",
            GpuType::I64 => "long",
            GpuType::U8 => "uchar",
            GpuType::U16 => "ushort",
            GpuType::U32 => "uint",
            GpuType::U64 => "ulong",
            GpuType::F16 => "half",
            GpuType::F32 => "float",
            GpuType::F64 => "float", // Metal doesn't have native double on all devices
            // Modern ML types - Metal has bfloat support on Apple Silicon M1+
            GpuType::BF16 => "bfloat",  // BFloat16 supported natively
            GpuType::F8E4M3 => "uchar", // FP8 emulated as byte (no native support)
            GpuType::F8E5M2 => "uchar", // FP8 emulated as byte
            GpuType::F4 => "uchar",     // FP4 packed as byte
            GpuType::Vec2(_) => "float2",
            GpuType::Vec3(_) => "float3",
            GpuType::Vec4(_) => "float4",
            GpuType::Ptr(_, _) => "device void*",
            GpuType::Array(_, _) => "void*",
            GpuType::Struct(name, _) => {
                // Return leaked str to get 'static lifetime
                // This is a workaround for the signature
                Box::leak(name.clone().into_boxed_str())
            }
        }
    }

    fn memory_space_to_msl(&self, space: MemorySpace) -> &'static str {
        match space {
            MemorySpace::Global => "device",
            MemorySpace::Shared => "threadgroup",
            MemorySpace::Local => "thread",
            MemorySpace::Constant => "constant",
            MemorySpace::Texture => "texture2d<float>",
            MemorySpace::Generic => "device",
        }
    }

    fn params_to_msl(&self, params: &[GpuParam]) -> String {
        params
            .iter()
            .map(|p| {
                let ty = self.type_to_msl(&p.ty);
                let space = self.memory_space_to_msl(p.space);
                format!("{} {} {}", space, ty, p.name)
            })
            .collect::<Vec<_>>()
            .join(", ")
    }

    fn const_value_to_msl(&self, val: &GpuConstValue) -> String {
        match val {
            GpuConstValue::Int(i) => i.to_string(),
            GpuConstValue::Float(f) => format!("{}f", f),
            GpuConstValue::Bool(b) => b.to_string(),
            GpuConstValue::Array(vals) => {
                let items: Vec<String> = vals.iter().map(|v| self.const_value_to_msl(v)).collect();
                format!("{{{}}}", items.join(", "))
            }
            GpuConstValue::Struct(vals) => {
                let items: Vec<String> = vals.iter().map(|v| self.const_value_to_msl(v)).collect();
                format!("{{{}}}", items.join(", "))
            }
        }
    }
}

/// Compile HLIR to MSL
pub fn compile_to_msl(hlir: &crate::hlir::HlirModule, gpu_family: MetalGpuFamily) -> String {
    use super::hlir_to_gpu::{LoweringConfig, lower_with_config};

    let config = LoweringConfig {
        target: GpuTarget::Metal { gpu_family },
        epistemic_enabled: true,
        ..Default::default()
    };

    let gpu_module = lower_with_config(hlir, &config);

    let mut codegen = MetalCodegen::new(MetalCodegenConfig {
        gpu_family,
        epistemic_enabled: true,
        ..Default::default()
    });

    codegen.generate(&gpu_module)
}

/// Compile HLIR to MSL with epistemic tracking
pub fn compile_to_msl_epistemic(
    hlir: &crate::hlir::HlirModule,
    gpu_family: MetalGpuFamily,
    epistemic: bool,
) -> String {
    use super::hlir_to_gpu::{LoweringConfig, lower_with_config};

    let config = LoweringConfig {
        target: GpuTarget::Metal { gpu_family },
        epistemic_enabled: epistemic,
        ..Default::default()
    };

    let gpu_module = lower_with_config(hlir, &config);

    let mut codegen = MetalCodegen::new(MetalCodegenConfig {
        gpu_family,
        epistemic_enabled: epistemic,
        ..Default::default()
    });

    codegen.generate(&gpu_module)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_metal_gpu_family() {
        assert_eq!(MetalGpuFamily::Apple8.simd_width(), 32);
        assert_eq!(MetalGpuFamily::Apple9.msl_version(), "3.1");
        assert!(MetalGpuFamily::Apple7.supports_simdgroup_matrix());
    }

    #[test]
    fn test_metal_codegen_empty() {
        let module = GpuModule::new(
            "test",
            GpuTarget::Metal {
                gpu_family: MetalGpuFamily::Apple8,
            },
        );

        let mut codegen = MetalCodegen::new(MetalCodegenConfig::default());
        let msl = codegen.generate(&module);

        assert!(msl.contains("#include <metal_stdlib>"));
        assert!(msl.contains("EpistemicFloat"));
        assert!(msl.contains("epistemic_add"));
    }

    #[test]
    fn test_epistemic_types_generated() {
        let module = GpuModule::new(
            "test",
            GpuTarget::Metal {
                gpu_family: MetalGpuFamily::Apple8,
            },
        );

        let mut codegen = MetalCodegen::new(MetalCodegenConfig {
            epistemic_enabled: true,
            ..Default::default()
        });
        let msl = codegen.generate(&module);

        assert!(msl.contains("struct EpistemicFloat"));
        assert!(msl.contains("float epsilon"));
        assert!(msl.contains("uint64_t provenance"));
        assert!(msl.contains("epistemic_mul"));
        assert!(msl.contains("epistemic_div"));
        assert!(msl.contains("simdgroup_epsilon_max"));
    }
}
