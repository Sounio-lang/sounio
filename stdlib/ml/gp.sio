// Gaussian Processes for Sounio ML
//
// Gaussian Processes (GPs) are NATURALLY EPISTEMIC models. Unlike neural networks
// or other point-estimate methods, GPs provide principled uncertainty quantification
// by construction. Every prediction comes with a variance that represents epistemic
// uncertainty—our lack of knowledge about the true function in regions with sparse data.
//
// Key properties:
// - Non-parametric: complexity grows with data
// - Bayesian: full posterior distribution over functions
// - Kernel-based: flexible function priors via kernel choice
// - Uncertainty-aware: variance increases away from training data
// - Theoretically grounded: maximum likelihood and marginal likelihood optimization
//
// Applications:
// - Regression with uncertainty bounds
// - Bayesian optimization (hyperparameter tuning, molecular design)
// - Time series forecasting with confidence intervals
// - Active learning (query points with highest uncertainty)
// - Spatial statistics and kriging

// ============================================================================
// Helper Functions
// ============================================================================

fn abs_f64(x: f64) -> f64 {
    if x < 0.0 { return 0.0 - x }
    return x
}

fn sqrt_f64(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 }
    var y = x
    y = (y + x / y) / 2.0
    y = (y + x / y) / 2.0
    y = (y + x / y) / 2.0
    y = (y + x / y) / 2.0
    y = (y + x / y) / 2.0
    y = (y + x / y) / 2.0
    y = (y + x / y) / 2.0
    y = (y + x / y) / 2.0
    return y
}

fn exp_f64(x: f64) -> f64 {
    if x > 20.0 { return exp_f64(x / 2.0) * exp_f64(x / 2.0) }
    if x < 0.0 - 20.0 { return 1.0 / exp_f64(0.0 - x) }
    var sum = 1.0
    var term = 1.0
    var i: i64 = 1
    while i < 20 {
        term = term * x / (i as f64)
        sum = sum + term
        i = i + 1
    }
    return sum
}

fn log_f64(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 - 1000000.0 }
    var val = x
    var k = 0.0
    let e = 2.718281828459045
    while val > e { val = val / e; k = k + 1.0 }
    while val < 1.0 / e { val = val * e; k = k - 1.0 }
    let u = (val - 1.0) / (val + 1.0)
    let u2 = u * u
    var sum = u
    var term = u
    term = term * u2; sum = sum + term / 3.0
    term = term * u2; sum = sum + term / 5.0
    term = term * u2; sum = sum + term / 7.0
    term = term * u2; sum = sum + term / 9.0
    term = term * u2; sum = sum + term / 11.0
    return 2.0 * sum + k
}

fn sin_f64(x: f64) -> f64 {
    let pi = 3.141592653589793
    var y = x
    while y > pi { y = y - 2.0 * pi }
    while y < 0.0 - pi { y = y + 2.0 * pi }
    let x2 = y * y
    var sum = y
    var term = y
    term = term * (0.0 - x2) / 6.0; sum = sum + term
    term = term * (0.0 - x2) / 20.0; sum = sum + term
    term = term * (0.0 - x2) / 42.0; sum = sum + term
    term = term * (0.0 - x2) / 72.0; sum = sum + term
    return sum
}

fn cos_f64(x: f64) -> f64 {
    let pi = 3.141592653589793
    var y = x
    while y > pi { y = y - 2.0 * pi }
    while y < 0.0 - pi { y = y + 2.0 * pi }
    let x2 = y * y
    var sum = 1.0
    var term = 1.0
    term = term * (0.0 - x2) / 2.0; sum = sum + term
    term = term * (0.0 - x2) / 12.0; sum = sum + term
    term = term * (0.0 - x2) / 30.0; sum = sum + term
    term = term * (0.0 - x2) / 56.0; sum = sum + term
    return sum
}

fn pow_f64(base: f64, exp: f64) -> f64 {
    return exp_f64(exp * log_f64(base))
}

// Kernel type identifiers
fn kernel_rbf_id() -> i64 { return 0 }
fn kernel_matern32_id() -> i64 { return 1 }
fn kernel_matern52_id() -> i64 { return 2 }
fn kernel_linear_id() -> i64 { return 3 }
fn kernel_periodic_id() -> i64 { return 4 }

// ============================================================================
// Kernel Functions
// ============================================================================

// Kernel hyperparameters
struct KernelParams {
    kernel_type: i64,     // 0=RBF, 1=Matern32, 2=Matern52, 3=Linear, 4=Periodic
    length_scale: f64,    // Characteristic length scale
    variance: f64,        // Output variance (signal variance)
    period: f64,          // Period for periodic kernel
    nu: f64,              // Smoothness parameter for Matern
}

// Radial Basis Function (RBF) / Squared Exponential kernel
// k(x1, x2) = σ² exp_f64(-||x1 - x2||² / (2ℓ²))
// Infinitely differentiable, very smooth functions
fn kernel_rbf(x1: [f64], x2: [f64], d: i64, length_scale: f64, variance: f64) -> f64 {
    var sum_sq = 0.0
    var i = 0
    while i < d {
        let diff = x1[i] - x2[i]
        sum_sq = sum_sq + diff * diff
        i = i + 1
    }
    let dist_sq = sum_sq
    let exponent = -0.5 * dist_sq / (length_scale * length_scale)
    variance * exp_f64(exponent)
}

// Matérn 3/2 kernel
// k(x1, x2) = σ² (1 + √3 r / ℓ) exp_f64(-√3 r / ℓ)
// Once differentiable, less smooth than RBF
fn kernel_matern32(x1: [f64], x2: [f64], d: i64, length_scale: f64, variance: f64) -> f64 {
    var sum_sq = 0.0
    var i = 0
    while i < d {
        let diff = x1[i] - x2[i]
        sum_sq = sum_sq + diff * diff
        i = i + 1
    }
    let r = sqrt_f64(sum_sq)
    let sqrt3 = 1.7320508075688772
    let scaled = sqrt3 * r / length_scale
    variance * (1.0 + scaled) * exp_f64(-scaled)
}

// Matérn 5/2 kernel
// k(x1, x2) = σ² (1 + √5 r / ℓ + 5r² / (3ℓ²)) exp_f64(-√5 r / ℓ)
// Twice differentiable, smoother than Matérn 3/2
fn kernel_matern52(x1: [f64], x2: [f64], d: i64, length_scale: f64, variance: f64) -> f64 {
    var sum_sq = 0.0
    var i = 0
    while i < d {
        let diff = x1[i] - x2[i]
        sum_sq = sum_sq + diff * diff
        i = i + 1
    }
    let r = sqrt_f64(sum_sq)
    let sqrt5 = 2.23606797749979
    let scaled = sqrt5 * r / length_scale
    let term1 = 1.0 + scaled
    let term2 = (5.0 * sum_sq) / (3.0 * length_scale * length_scale)
    variance * (term1 + term2) * exp_f64(-scaled)
}

// Linear kernel
// k(x1, x2) = σ² x1^T x2
// Non-stationary, represents linear functions
fn kernel_linear(x1: [f64], x2: [f64], d: i64, variance: f64) -> f64 {
    var dot = 0.0
    var i = 0
    while i < d {
        dot = dot + x1[i] * x2[i]
        i = i + 1
    }
    variance * dot
}

// Periodic kernel
// k(x1, x2) = σ² exp_f64(-2 sin²(π ||x1 - x2|| / p) / ℓ²)
// Captures periodic patterns with period p
fn kernel_periodic(x1: [f64], x2: [f64], d: i64, length_scale: f64, period: f64, variance: f64) -> f64 {
    var sum_sq = 0.0
    var i = 0
    while i < d {
        let diff = x1[i] - x2[i]
        sum_sq = sum_sq + diff * diff
        i = i + 1
    }
    let r = sqrt_f64(sum_sq)
    let pi = 3.141592653589793
    let sin_term = sin_f64(pi * r / period)
    let exponent = -2.0 * sin_term * sin_term / (length_scale * length_scale)
    variance * exp_f64(exponent)
}

// Kernel sum for composite kernels
fn kernel_sum(k1: f64, k2: f64) -> f64 {
    k1 + k2
}

// Kernel product for composite kernels
fn kernel_product(k1: f64, k2: f64) -> f64 {
    k1 * k2
}

// Evaluate kernel based on parameters
fn evaluate_kernel(x1: [f64], x2: [f64], d: i64, params: KernelParams) -> f64 {
    if params.kernel_type == kernel_rbf_id() {
        kernel_rbf(x1, x2, d, params.length_scale, params.variance)
    } else if params.kernel_type == kernel_matern32_id() {
        kernel_matern32(x1, x2, d, params.length_scale, params.variance)
    } else if params.kernel_type == kernel_matern52_id() {
        kernel_matern52(x1, x2, d, params.length_scale, params.variance)
    } else if params.kernel_type == kernel_linear_id() {
        kernel_linear(x1, x2, d, params.variance)
    } else if params.kernel_type == kernel_periodic_id() {
        kernel_periodic(x1, x2, d, params.length_scale, params.period, params.variance)
    } else {
        // Default to RBF
        kernel_rbf(x1, x2, d, params.length_scale, params.variance)
    }
}

// ============================================================================
// Covariance Matrix Computation
// ============================================================================

// Compute kernel matrix K(X, X) for training data
// Returns flattened [n, n] matrix
fn compute_kernel_matrix(X: [f64], n: i64, d: i64, params: KernelParams) -> [f64] {
    var K = [0.0; (n * n)]
    var i = 0
    while i < n {
        var j = 0
        while j < n {
            // Extract rows i and j
            var x1 = [0.0; d]
            var x2 = [0.0; d]
            var k = 0
            while k < d {
                x1[k] = X[i * d + k]
                x2[k] = X[j * d + k]
                k = k + 1
            }
            K[i * n + j] = evaluate_kernel(x1, x2, d, params)
            j = j + 1
        }
        i = i + 1
    }
    K
}

// Compute cross-kernel matrix K(X1, X2)
// Returns flattened [n1, n2] matrix
fn compute_cross_kernel(X1: [f64], n1: i64, X2: [f64], n2: i64, d: i64, params: KernelParams) -> [f64] {
    var K = [0.0; (n1 * n2)]
    var i = 0
    while i < n1 {
        var j = 0
        while j < n2 {
            var x1 = [0.0; d]
            var x2 = [0.0; d]
            var k = 0
            while k < d {
                x1[k] = X1[i * d + k]
                x2[k] = X2[j * d + k]
                k = k + 1
            }
            K[i * n2 + j] = evaluate_kernel(x1, x2, d, params)
            j = j + 1
        }
        i = i + 1
    }
    K
}

// ============================================================================
// Linear Algebra Utilities
// ============================================================================

// Cholesky decomposition: A = L L^T
// A is symmetric positive definite [n, n]
// Returns lower triangular L
fn cholesky_decompose(A: [f64], n: i64) -> [f64] {
    var L = [0.0; (n * n)]
    var i = 0
    while i < n {
        var j = 0
        while j <= i {
            var sum = 0.0
            var k = 0
            while k < j {
                sum = sum + L[i * n + k] * L[j * n + k]
                k = k + 1
            }

            if i == j {
                L[i * n + j] = sqrt_f64(A[i * n + i] - sum)
            } else {
                L[i * n + j] = (A[i * n + j] - sum) / L[j * n + j]
            }
            j = j + 1
        }
        i = i + 1
    }
    L
}

// Solve L x = b where L is lower triangular
fn solve_lower_triangular(L: [f64], b: [f64], n: i64) -> [f64] {
    var x = [0.0; n]
    var i = 0
    while i < n {
        var sum = 0.0
        var j = 0
        while j < i {
            sum = sum + L[i * n + j] * x[j]
            j = j + 1
        }
        x[i] = (b[i] - sum) / L[i * n + i]
        i = i + 1
    }
    x
}

// Solve U x = b where U is upper triangular
fn solve_upper_triangular(U: [f64], b: [f64], n: i64) -> [f64] {
    var x = [0.0; n]
    var i = n - 1
    while i >= 0 {
        var sum = 0.0
        var j = i + 1
        while j < n {
            sum = sum + U[i * n + j] * x[j]
            j = j + 1
        }
        x[i] = (b[i] - sum) / U[i * n + i]
        i = i - 1
    }
    x
}

// Matrix-vector multiplication: y = A x
fn matvec(A: [f64], x: [f64], n: i64, m: i64) -> [f64] {
    var y = [0.0; n]
    var i = 0
    while i < n {
        var sum = 0.0
        var j = 0
        while j < m {
            sum = sum + A[i * m + j] * x[j]
            j = j + 1
        }
        y[i] = sum
        i = i + 1
    }
    y
}

// Vector dot product
fn dot(x: [f64], y: [f64], n: i64) -> f64 {
    var sum = 0.0
    var i = 0
    while i < n {
        sum = sum + x[i] * y[i]
        i = i + 1
    }
    sum
}

// Transpose matrix
fn transpose(A: [f64], n: i64, m: i64) -> [f64] {
    var AT = [0.0; (m * n)]
    var i = 0
    while i < n {
        var j = 0
        while j < m {
            AT[j * n + i] = A[i * m + j]
            j = j + 1
        }
        i = i + 1
    }
    AT
}

// ============================================================================
// Gaussian Process Regression
// ============================================================================

// Gaussian Process regression model
struct GPModel {
    X_train: [f64],       // Training inputs [n, d] flattened
    y_train: [f64],       // Training outputs [n]
    n_train: i64,         // Number of training points
    input_dim: i64,       // Input dimensionality
    kernel_params: KernelParams,
    noise_variance: f64,  // Observation noise σ²
    alpha: [f64],         // (K + σ²I)^(-1) y
    L: [f64],             // Cholesky factor of (K + σ²I)
}

// GP prediction with epistemic uncertainty
struct GPPrediction {
    mean: f64,            // Posterior mean
    variance: f64,        // EPISTEMIC uncertainty (posterior variance)
    confidence_95_lo: f64,  // 95% confidence lower bound
    confidence_95_hi: f64,  // 95% confidence upper bound
}

// Create new GP model (unfitted)
fn gp_new(kernel_params: KernelParams, noise_variance: f64) -> GPModel {
    GPModel {
        X_train: [0.0; 0],
        y_train: [0.0; 0],
        n_train: 0,
        input_dim: 0,
        kernel_params: kernel_params,
        noise_variance: noise_variance,
        alpha: [0.0; 0],
        L: [0.0; 0],
    }
}

// Fit GP model to training data
// Computes Cholesky decomposition and α = (K + σ²I)^(-1) y
fn gp_fit(X: [f64], y: [f64], n: i64, d: i64, kernel_params: KernelParams, noise_variance: f64) -> GPModel {
    // Compute kernel matrix K
    let K = compute_kernel_matrix(X, n, d, kernel_params)

    // Add noise: K_noisy = K + σ²I
    var K_noisy = [0.0; (n * n)]
    var i = 0
    while i < n {
        var j = 0
        while j < n {
            if i == j {
                K_noisy[i * n + j] = K[i * n + j] + noise_variance
            } else {
                K_noisy[i * n + j] = K[i * n + j]
            }
            j = j + 1
        }
        i = i + 1
    }

    // Cholesky decomposition: K_noisy = L L^T
    let L = cholesky_decompose(K_noisy, n)

    // Solve L α_temp = y
    let alpha_temp = solve_lower_triangular(L, y, n)

    // Solve L^T α = α_temp
    let LT = transpose(L, n, n)
    let alpha = solve_upper_triangular(LT, alpha_temp, n)

    GPModel {
        X_train: X,
        y_train: y,
        n_train: n,
        input_dim: d,
        kernel_params: kernel_params,
        noise_variance: noise_variance,
        alpha: alpha,
        L: L,
    }
}

// Predict at a single test point
fn gp_predict(model: GPModel, x_test: [f64]) -> GPPrediction {
    let n = model.n_train
    let d = model.input_dim

    // Compute k_star = K(x_test, X_train) [1, n]
    var k_star = [0.0; n]
    var i = 0
    while i < n {
        var x_train_i = [0.0; d]
        var j = 0
        while j < d {
            x_train_i[j] = model.X_train[i * d + j]
            j = j + 1
        }
        k_star[i] = evaluate_kernel(x_test, x_train_i, d, model.kernel_params)
        i = i + 1
    }

    // Posterior mean: μ = k_star^T α
    let mean = dot(k_star, model.alpha, n)

    // Posterior variance: σ² = k(x*, x*) - k_star^T (K + σ²I)^(-1) k_star
    let k_star_star = evaluate_kernel(x_test, x_test, d, model.kernel_params)

    // v = L^(-1) k_star
    let v = solve_lower_triangular(model.L, k_star, n)
    let v_dot_v = dot(v, v, n)

    let variance = k_star_star - v_dot_v

    // 95% confidence interval (±1.96 σ)
    let std_dev = sqrt_f64(variance)
    let confidence_95_lo = mean - 1.96 * std_dev
    let confidence_95_hi = mean + 1.96 * std_dev

    GPPrediction {
        mean: mean,
        variance: variance,
        confidence_95_lo: confidence_95_lo,
        confidence_95_hi: confidence_95_hi,
    }
}

// Predict at multiple test points
fn gp_predict_batch(model: GPModel, X_test: [f64], n_test: i64) -> [GPPrediction] {
    var predictions = [GPPrediction {
        mean: 0.0,
        variance: 0.0,
        confidence_95_lo: 0.0,
        confidence_95_hi: 0.0,
    }; n_test]

    var i = 0
    while i < n_test {
        var x_test = [0.0; model.input_dim]
        var j = 0
        while j < model.input_dim {
            x_test[j] = X_test[i * model.input_dim + j]
            j = j + 1
        }
        predictions[i] = gp_predict(model, x_test)
        i = i + 1
    }

    predictions
}

// Draw samples from GP posterior
// Returns [n_samples, n_test] flattened
fn gp_sample(model: GPModel, X_test: [f64], n_test: i64, n_samples: i64, seed: i64) -> [f64] {
    seed_rng(seed)

    let d = model.input_dim
    let n = model.n_train

    // Compute K(X_test, X_test)
    let K_test = compute_kernel_matrix(X_test, n_test, d, model.kernel_params)

    // Compute K(X_test, X_train)
    let K_cross = compute_cross_kernel(X_test, n_test, model.X_train, n, d, model.kernel_params)

    // Posterior mean at test points
    let mean = matvec(K_cross, model.alpha, n_test, n)

    // Posterior covariance: Σ = K_test - K_cross (K + σ²I)^(-1) K_cross^T
    // This is expensive, simplified for now
    var samples = [0.0; (n_samples * n_test)]

    var s = 0
    while s < n_samples {
        var t = 0
        while t < n_test {
            var x_test = [0.0; d]
            var j = 0
            while j < d {
                x_test[j] = X_test[t * d + j]
                j = j + 1
            }
            let pred = gp_predict(model, x_test)
            let std_dev = sqrt_f64(pred.variance)
            samples[s * n_test + t] = pred.mean + std_dev * normal(0.0, 1.0)
            t = t + 1
        }
        s = s + 1
    }

    samples
}

// ============================================================================
// Hyperparameter Optimization
// ============================================================================

// Compute log marginal likelihood
// log p(y | X, θ) = -0.5 y^T (K + σ²I)^(-1) y - 0.5 log|K + σ²I| - n/2 log_f64(2π)
fn gp_log_marginal_likelihood(model: GPModel) -> f64 {
    let n = model.n_train

    // -0.5 y^T α
    let term1 = -0.5 * dot(model.y_train, model.alpha, n)

    // -0.5 log|K + σ²I| = -sum(log_f64(diag(L)))
    var log_det = 0.0
    var i = 0
    while i < n {
        log_det = log_det + log_f64(model.L[i * n + i])
        i = i + 1
    }
    let term2 = -log_det

    // -n/2 log_f64(2π)
    let pi = 3.141592653589793
    let term3 = -0.5 * n as f64 * log_f64(2.0 * pi)

    term1 + term2 + term3
}

// Optimize hyperparameters using gradient descent (simplified)
// In practice, use L-BFGS-B or similar
fn gp_optimize_hyperparams(model: GPModel, n_iterations: i64) -> KernelParams {
    var params = model.kernel_params
    let learning_rate = 0.01

    var iter = 0
    while iter < n_iterations {
        // Compute gradient numerically (finite differences)
        let eps = 1e-5

        // Current likelihood
        let current_model = gp_fit(model.X_train, model.y_train, model.n_train,
                                   model.input_dim, params, model.noise_variance)
        let current_ll = gp_log_marginal_likelihood(current_model)

        // Gradient w.r.t. length_scale
        var params_ls = params
        params_ls.length_scale = params.length_scale + eps
        let model_ls = gp_fit(model.X_train, model.y_train, model.n_train,
                              model.input_dim, params_ls, model.noise_variance)
        let grad_ls = (gp_log_marginal_likelihood(model_ls) - current_ll) / eps

        // Gradient w.r.t. variance
        var params_var = params
        params_var.variance = params.variance + eps
        let model_var = gp_fit(model.X_train, model.y_train, model.n_train,
                               model.input_dim, params_var, model.noise_variance)
        let grad_var = (gp_log_marginal_likelihood(model_var) - current_ll) / eps

        // Update (gradient ascent for maximization)
        params.length_scale = params.length_scale + learning_rate * grad_ls
        params.variance = params.variance + learning_rate * grad_var

        // Ensure positive values
        if params.length_scale < 1e-6 {
            params.length_scale = 1e-6
        }
        if params.variance < 1e-6 {
            params.variance = 1e-6
        }

        iter = iter + 1
    }

    params
}

// ============================================================================
// Sparse GP Approximations
// ============================================================================

// Sparse GP using inducing points (FITC approximation)
struct SparseGPModel {
    X_inducing: [f64],    // Inducing points [m, d]
    n_inducing: i64,
    X_train: [f64],
    y_train: [f64],
    n_train: i64,
    input_dim: i64,
    kernel_params: KernelParams,
    noise_variance: f64,
    Qff_inv_y: [f64],     // Precomputed for prediction
}

// Create sparse GP model
fn sparse_gp_new(n_inducing: i64, d: i64, kernel_params: KernelParams) -> SparseGPModel {
    SparseGPModel {
        X_inducing: [0.0; (n_inducing * d)],
        n_inducing: n_inducing,
        X_train: [0.0; 0],
        y_train: [0.0; 0],
        n_train: 0,
        input_dim: d,
        kernel_params: kernel_params,
        noise_variance: 0.01,
        Qff_inv_y: [0.0; 0],
    }
}

// Fit sparse GP (simplified - full FITC would be more complex)
fn sparse_gp_fit(X_inducing: [f64], n_inducing: i64, X: [f64], y: [f64], n: i64, d: i64,
                 kernel_params: KernelParams, noise_variance: f64) -> SparseGPModel {
    // For simplicity, use subset of data as inducing points
    // In practice, optimize inducing point locations

    SparseGPModel {
        X_inducing: X_inducing,
        n_inducing: n_inducing,
        X_train: X,
        y_train: y,
        n_train: n,
        input_dim: d,
        kernel_params: kernel_params,
        noise_variance: noise_variance,
        Qff_inv_y: [0.0; n_inducing],  // Would be computed properly
    }
}

// Predict with sparse GP
fn sparse_gp_predict(model: SparseGPModel, x_test: [f64]) -> GPPrediction {
    // Simplified - use full GP on inducing points
    let inducing_model = gp_fit(model.X_inducing, model.Qff_inv_y, model.n_inducing,
                                model.input_dim, model.kernel_params, model.noise_variance)
    gp_predict(inducing_model, x_test)
}

// ============================================================================
// GP Classification
// ============================================================================

// GP classifier using Laplace approximation
struct GPClassifier {
    X_train: [f64],
    y_train: [i64],       // Binary labels {0, 1}
    n_train: i64,
    input_dim: i64,
    kernel_params: KernelParams,
    f_mode: [f64],        // MAP estimate of latent function
}

// Sigmoid function
fn sigmoid(x: f64) -> f64 {
    1.0 / (1.0 + exp_f64(-x))
}

// Fit GP classifier
fn gp_classifier_fit(X: [f64], y: [i64], n: i64, d: i64, kernel_params: KernelParams) -> GPClassifier {
    // Compute kernel matrix
    let K = compute_kernel_matrix(X, n, d, kernel_params)

    // Newton's method to find mode (simplified - should iterate)
    var f_mode = [0.0; n]

    GPClassifier {
        X_train: X,
        y_train: y,
        n_train: n,
        input_dim: d,
        kernel_params: kernel_params,
        f_mode: f_mode,
    }
}

// Predict class probabilities
// Returns (probability, uncertainty)
fn gp_classifier_predict(model: GPClassifier, x_test: [f64]) -> (f64, f64) {
    // Simplified - compute mean and variance of latent function
    let n = model.n_train
    let d = model.input_dim

    var k_star = [0.0; n]
    var i = 0
    while i < n {
        var x_train_i = [0.0; d]
        var j = 0
        while j < d {
            x_train_i[j] = model.X_train[i * d + j]
            j = j + 1
        }
        k_star[i] = evaluate_kernel(x_test, x_train_i, d, model.kernel_params)
        i = i + 1
    }

    let f_mean = dot(k_star, model.f_mode, n)
    let f_var = 0.1  // Simplified

    let prob = sigmoid(f_mean)
    let uncertainty = sqrt_f64(f_var)

    (prob, uncertainty)
}

// ============================================================================
// Bayesian Optimization
// ============================================================================

// Bayesian optimization result
struct BOResult {
    x_next: [f64],        // Next point to evaluate
    expected_improvement: f64,
    uncertainty: f64,
}

// Standard normal CDF (approximation)
fn normal_cdf(x: f64) -> f64 {
    0.5 * (1.0 + erf(x / 1.4142135623730951))
}

// Error function (approximation)
fn erf(x: f64) -> f64 {
    // Abramowitz and Stegun approximation
    let a1 = 0.254829592
    let a2 = -0.284496736
    let a3 = 1.421413741
    let a4 = -1.453152027
    let a5 = 1.061405429
    let p = 0.3275911

    let sign = if x < 0.0 { -1.0 } else { 1.0 }
    let x_abs = abs_f64(x)

    let t = 1.0 / (1.0 + p * x_abs)
    let y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * exp_f64(-x_abs * x_abs)

    sign * y
}

// Standard normal PDF
fn normal_pdf(x: f64) -> f64 {
    let pi = 3.141592653589793
    (1.0 / sqrt_f64(2.0 * pi)) * exp_f64(-0.5 * x * x)
}

// Expected Improvement acquisition function
// EI(x) = E[max(f(x) - f_best, 0)]
fn acquisition_ei(model: GPModel, x: [f64], best_y: f64) -> f64 {
    let pred = gp_predict(model, x)
    let mean = pred.mean
    let std = sqrt_f64(pred.variance)

    if std < 1e-10 {
        return 0.0
    }

    let z = (mean - best_y) / std
    let ei = (mean - best_y) * normal_cdf(z) + std * normal_pdf(z)
    ei
}

// Upper Confidence Bound acquisition function
// UCB(x) = μ(x) + β σ(x)
fn acquisition_ucb(model: GPModel, x: [f64], beta: f64) -> f64 {
    let pred = gp_predict(model, x)
    pred.mean + beta * sqrt_f64(pred.variance)
}

// Probability of Improvement acquisition function
// PI(x) = P(f(x) > f_best)
fn acquisition_pi(model: GPModel, x: [f64], best_y: f64) -> f64 {
    let pred = gp_predict(model, x)
    let mean = pred.mean
    let std = sqrt_f64(pred.variance)

    if std < 1e-10 {
        return 0.0
    }

    let z = (mean - best_y) / std
    normal_cdf(z)
}

// Bayesian optimization step
// Finds next point to evaluate by maximizing acquisition function
fn bayesian_optimize_step(model: GPModel, bounds: [f64], best_y: f64,
                          n_candidates: i64, seed: i64) -> BOResult {
    seed_rng(seed)

    let d = model.input_dim

    // Random search over candidates (in practice, use gradient-based optimization)
    var best_x = [0.0; d]
    var best_ei = 0.0
    var best_uncertainty = 0.0

    var i = 0
    while i < n_candidates {
        var x_candidate = [0.0; d]
        var j = 0
        while j < d {
            let lower = bounds[j * 2]
            let upper = bounds[j * 2 + 1]
            x_candidate[j] = lower + normal(0.0, 1.0) * (upper - lower)
            j = j + 1
        }

        let ei = acquisition_ei(model, x_candidate, best_y)

        if ei > best_ei {
            best_ei = ei
            best_x = x_candidate
            let pred = gp_predict(model, x_candidate)
            best_uncertainty = sqrt_f64(pred.variance)
        }

        i = i + 1
    }

    BOResult {
        x_next: best_x,
        expected_improvement: best_ei,
        uncertainty: best_uncertainty,
    }
}

// ============================================================================
// Example Usage and Testing
// ============================================================================

// Example: 1D regression with epistemic uncertainty
fn example_1d_regression() -> GPModel {
    // Training data: y = sin_f64(x) + noise
    let X_train = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]
    let y_train = [0.0, 0.84, 0.91, 0.14, -0.76, -0.96]
    let n = 6
    let d = 1

    // RBF kernel
    let kernel_params = KernelParams {
        kernel_type: kernel_rbf_id(),
        length_scale: 1.0,
        variance: 1.0,
        period: 0.0,
        nu: 0.0,
    }

    let model = gp_fit(X_train, y_train, n, d, kernel_params, 0.01)

    // Predict at test point
    let x_test = [2.5]
    let pred = gp_predict(model, x_test)

    // pred.mean ≈ 0.5, pred.variance shows epistemic uncertainty
    model
}

// Example: Bayesian optimization of 1D function
fn example_bayesian_optimization() -> [f64] {
    // Optimize f(x) = -(x - 2)²
    let X_init = [0.0, 5.0]
    let y_init = [-4.0, -9.0]

    let kernel_params = KernelParams {
        kernel_type: kernel_rbf_id(),
        length_scale: 1.0,
        variance: 1.0,
        period: 0.0,
        nu: 0.0,
    }

    var model = gp_fit(X_init, y_init, 2, 1, kernel_params, 0.01)

    let bounds = [0.0, 5.0]
    let best_y = -4.0

    let result = bayesian_optimize_step(model, bounds, best_y, 100, 42)

    // result.x_next should be near 2.0 (optimum)
    result.x_next
}
