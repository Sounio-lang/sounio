// dual.d - Forward-Mode Automatic Differentiation using Dual Numbers
//
// Dual numbers extend real numbers with an infinitesimal ε where ε² = 0.
// A dual number is: a + bε, where a is the value and b is the derivative.
//
// For any analytic function f:
//   f(a + bε) = f(a) + b*f'(a)*ε
//
// This enables automatic computation of derivatives by propagating through
// arithmetic operations. Perfect for functions with few inputs.
//
// Reference: "Automatic Differentiation in Machine Learning: a Survey"
//            Baydin et al., JMLR 2018

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

fn abs_f64(x: f64) -> f64 {
    if x < 0.0 { return 0.0 - x }
    return x
}

fn sqrt_f64(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 }
    let mut y = x
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    y = 0.5 * (y + x / y)
    return y
}

// Exponential via Taylor series (15 terms)
fn exp_f64(x: f64) -> f64 {
    // Handle large values by range reduction: exp(x) = exp(x/n)^n
    if x > 20.0 { return exp_f64(x / 2.0) * exp_f64(x / 2.0) }
    if x < 0.0 - 20.0 { return 1.0 / exp_f64(0.0 - x) }

    let mut sum = 1.0
    let mut term = 1.0
    term = term * x / 1.0
    sum = sum + term
    term = term * x / 2.0
    sum = sum + term
    term = term * x / 3.0
    sum = sum + term
    term = term * x / 4.0
    sum = sum + term
    term = term * x / 5.0
    sum = sum + term
    term = term * x / 6.0
    sum = sum + term
    term = term * x / 7.0
    sum = sum + term
    term = term * x / 8.0
    sum = sum + term
    term = term * x / 9.0
    sum = sum + term
    term = term * x / 10.0
    sum = sum + term
    term = term * x / 11.0
    sum = sum + term
    term = term * x / 12.0
    sum = sum + term
    term = term * x / 13.0
    sum = sum + term
    term = term * x / 14.0
    sum = sum + term
    term = term * x / 15.0
    sum = sum + term
    return sum
}

// Natural log via Halley's method with range reduction
fn ln_f64(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 - 1000000.0 }  // -inf approximation

    // Range reduction: ln(x) = ln(x/e^k) + k for appropriate k
    let mut val = x
    let mut k = 0.0
    let e = 2.718281828459045

    // Reduce to [1/e, e] range
    while val > e {
        val = val / e
        k = k + 1.0
    }
    while val < 1.0 / e {
        val = val * e
        k = k - 1.0
    }

    // Now use series: ln(1+u) = u - u²/2 + u³/3 - u⁴/4 + ...
    // where u = (val - 1) / (val + 1), so ln(val) = 2 * artanh(u)
    // artanh(u) = u + u³/3 + u⁵/5 + u⁷/7 + ...
    let u = (val - 1.0) / (val + 1.0)
    let u2 = u * u
    let mut sum = u
    let mut term = u
    term = term * u2
    sum = sum + term / 3.0
    term = term * u2
    sum = sum + term / 5.0
    term = term * u2
    sum = sum + term / 7.0
    term = term * u2
    sum = sum + term / 9.0
    term = term * u2
    sum = sum + term / 11.0
    term = term * u2
    sum = sum + term / 13.0
    term = term * u2
    sum = sum + term / 15.0
    term = term * u2
    sum = sum + term / 17.0
    term = term * u2
    sum = sum + term / 19.0
    term = term * u2
    sum = sum + term / 21.0

    return 2.0 * sum + k
}

// Power function: x^n using exp/ln
fn pow_f64(x: f64, n: f64) -> f64 {
    if x <= 0.0 { return 0.0 }
    return exp_f64(n * ln_f64(x))
}

// Sine via Taylor series
fn sin_f64(x: f64) -> f64 {
    // Reduce to [-π, π]
    let pi = 3.141592653589793
    let mut y = x
    while y > pi { y = y - 2.0 * pi }
    while y < 0.0 - pi { y = y + 2.0 * pi }

    // Taylor: sin(x) = x - x³/3! + x⁵/5! - x⁷/7! + ...
    let x2 = y * y
    let mut sum = y
    let mut term = y
    term = term * (0.0 - x2) / (2.0 * 3.0)
    sum = sum + term
    term = term * (0.0 - x2) / (4.0 * 5.0)
    sum = sum + term
    term = term * (0.0 - x2) / (6.0 * 7.0)
    sum = sum + term
    term = term * (0.0 - x2) / (8.0 * 9.0)
    sum = sum + term
    term = term * (0.0 - x2) / (10.0 * 11.0)
    sum = sum + term
    term = term * (0.0 - x2) / (12.0 * 13.0)
    sum = sum + term
    return sum
}

// Cosine via Taylor series
fn cos_f64(x: f64) -> f64 {
    let pi = 3.141592653589793
    let mut y = x
    while y > pi { y = y - 2.0 * pi }
    while y < 0.0 - pi { y = y + 2.0 * pi }

    // Taylor: cos(x) = 1 - x²/2! + x⁴/4! - x⁶/6! + ...
    let x2 = y * y
    let mut sum = 1.0
    let mut term = 1.0
    term = term * (0.0 - x2) / (1.0 * 2.0)
    sum = sum + term
    term = term * (0.0 - x2) / (3.0 * 4.0)
    sum = sum + term
    term = term * (0.0 - x2) / (5.0 * 6.0)
    sum = sum + term
    term = term * (0.0 - x2) / (7.0 * 8.0)
    sum = sum + term
    term = term * (0.0 - x2) / (9.0 * 10.0)
    sum = sum + term
    term = term * (0.0 - x2) / (11.0 * 12.0)
    sum = sum + term
    return sum
}

fn tanh_f64(x: f64) -> f64 {
    let e2x = exp_f64(2.0 * x)
    return (e2x - 1.0) / (e2x + 1.0)
}

fn signum_f64(x: f64) -> f64 {
    if x > 0.0 { return 1.0 }
    if x < 0.0 { return 0.0 - 1.0 }
    return 0.0
}

// ============================================================================
// DUAL NUMBER TYPE
// ============================================================================
// Dual number: val + dot*ε where ε² = 0
// val is the primal (function value)
// dot is the tangent (derivative)

struct Dual {
    val: f64,  // Primal value
    dot: f64   // Tangent (derivative)
}

// Create a constant (derivative = 0)
fn dual_const(val: f64) -> Dual {
    return Dual { val: val, dot: 0.0 }
}

// Create a variable (derivative = 1)
fn dual_var(val: f64) -> Dual {
    return Dual { val: val, dot: 1.0 }
}

// Create with explicit derivative
fn dual_new(val: f64, dot: f64) -> Dual {
    return Dual { val: val, dot: dot }
}

// ============================================================================
// ARITHMETIC OPERATIONS
// ============================================================================

fn dual_add(a: Dual, b: Dual) -> Dual {
    return Dual { val: a.val + b.val, dot: a.dot + b.dot }
}

fn dual_sub(a: Dual, b: Dual) -> Dual {
    return Dual { val: a.val - b.val, dot: a.dot - b.dot }
}

fn dual_mul(a: Dual, b: Dual) -> Dual {
    // Product rule: (f*g)' = f'*g + f*g'
    return Dual {
        val: a.val * b.val,
        dot: a.dot * b.val + a.val * b.dot
    }
}

fn dual_div(a: Dual, b: Dual) -> Dual {
    // Quotient rule: (f/g)' = (f'*g - f*g') / g²
    let g2 = b.val * b.val
    return Dual {
        val: a.val / b.val,
        dot: (a.dot * b.val - a.val * b.dot) / g2
    }
}

fn dual_neg(a: Dual) -> Dual {
    return Dual { val: 0.0 - a.val, dot: 0.0 - a.dot }
}

// Scale by constant
fn dual_scale(a: Dual, s: f64) -> Dual {
    return Dual { val: a.val * s, dot: a.dot * s }
}

// Add constant
fn dual_add_const(a: Dual, c: f64) -> Dual {
    return Dual { val: a.val + c, dot: a.dot }
}

// ============================================================================
// MATHEMATICAL FUNCTIONS
// ============================================================================

fn dual_sqrt(a: Dual) -> Dual {
    // d/dx sqrt(x) = 1/(2*sqrt(x))
    let v = sqrt_f64(a.val)
    return Dual { val: v, dot: a.dot / (2.0 * v) }
}

fn dual_exp(a: Dual) -> Dual {
    // d/dx exp(x) = exp(x)
    let e = exp_f64(a.val)
    return Dual { val: e, dot: a.dot * e }
}

fn dual_ln(a: Dual) -> Dual {
    // d/dx ln(x) = 1/x
    return Dual { val: ln_f64(a.val), dot: a.dot / a.val }
}

fn dual_pow(a: Dual, n: f64) -> Dual {
    // d/dx x^n = n * x^(n-1)
    let v = pow_f64(a.val, n)
    return Dual { val: v, dot: a.dot * n * pow_f64(a.val, n - 1.0) }
}

fn dual_sin(a: Dual) -> Dual {
    // d/dx sin(x) = cos(x)
    return Dual { val: sin_f64(a.val), dot: a.dot * cos_f64(a.val) }
}

fn dual_cos(a: Dual) -> Dual {
    // d/dx cos(x) = -sin(x)
    return Dual { val: cos_f64(a.val), dot: 0.0 - a.dot * sin_f64(a.val) }
}

fn dual_tan(a: Dual) -> Dual {
    // d/dx tan(x) = sec²(x) = 1/cos²(x)
    let c = cos_f64(a.val)
    return Dual { val: sin_f64(a.val) / c, dot: a.dot / (c * c) }
}

fn dual_tanh(a: Dual) -> Dual {
    // d/dx tanh(x) = 1 - tanh²(x)
    let t = tanh_f64(a.val)
    return Dual { val: t, dot: a.dot * (1.0 - t * t) }
}

fn dual_sigmoid(a: Dual) -> Dual {
    // sigmoid(x) = 1 / (1 + exp(-x))
    // d/dx sigmoid(x) = sigmoid(x) * (1 - sigmoid(x))
    let s = 1.0 / (1.0 + exp_f64(0.0 - a.val))
    return Dual { val: s, dot: a.dot * s * (1.0 - s) }
}

fn dual_relu(a: Dual) -> Dual {
    // ReLU(x) = max(0, x)
    // d/dx ReLU(x) = 1 if x > 0, else 0
    if a.val > 0.0 {
        return Dual { val: a.val, dot: a.dot }
    }
    return Dual { val: 0.0, dot: 0.0 }
}

fn dual_abs(a: Dual) -> Dual {
    // d/dx |x| = sign(x)
    return Dual { val: abs_f64(a.val), dot: a.dot * signum_f64(a.val) }
}

fn dual_max(a: Dual, b: Dual) -> Dual {
    if a.val >= b.val {
        return a
    }
    return b
}

fn dual_min(a: Dual, b: Dual) -> Dual {
    if a.val <= b.val {
        return a
    }
    return b
}

// ============================================================================
// DUAL VECTOR (for multi-variable functions)
// ============================================================================

struct DualVec3 {
    x: Dual,
    y: Dual,
    z: Dual
}

fn dualvec3_new(x: Dual, y: Dual, z: Dual) -> DualVec3 {
    return DualVec3 { x: x, y: y, z: z }
}

fn dualvec3_const(x: f64, y: f64, z: f64) -> DualVec3 {
    return DualVec3 {
        x: dual_const(x),
        y: dual_const(y),
        z: dual_const(z)
    }
}

// Create with i-th component as the variable (for gradient computation)
fn dualvec3_var_i(x: f64, y: f64, z: f64, i: i64) -> DualVec3 {
    if i == 0 {
        return DualVec3 { x: dual_var(x), y: dual_const(y), z: dual_const(z) }
    }
    if i == 1 {
        return DualVec3 { x: dual_const(x), y: dual_var(y), z: dual_const(z) }
    }
    return DualVec3 { x: dual_const(x), y: dual_const(y), z: dual_var(z) }
}

fn dualvec3_add(a: DualVec3, b: DualVec3) -> DualVec3 {
    return DualVec3 {
        x: dual_add(a.x, b.x),
        y: dual_add(a.y, b.y),
        z: dual_add(a.z, b.z)
    }
}

fn dualvec3_scale(v: DualVec3, s: Dual) -> DualVec3 {
    return DualVec3 {
        x: dual_mul(v.x, s),
        y: dual_mul(v.y, s),
        z: dual_mul(v.z, s)
    }
}

fn dualvec3_dot(a: DualVec3, b: DualVec3) -> Dual {
    return dual_add(dual_add(dual_mul(a.x, b.x), dual_mul(a.y, b.y)), dual_mul(a.z, b.z))
}

fn dualvec3_norm(v: DualVec3) -> Dual {
    return dual_sqrt(dualvec3_dot(v, v))
}

// ============================================================================
// TESTS
// ============================================================================

fn main() -> i32 {
    println("=== Sounio Forward-Mode Automatic Differentiation ===")
    println("")

    // Test 1: Simple derivative d/dx(x²) at x=3
    println("Test 1: d/dx(x^2) at x=3")
    let x1 = dual_var(3.0)
    let y1 = dual_mul(x1, x1)  // x²
    println("  f(x) = x^2")
    println("  f(3) = ")
    println(y1.val)
    println("  f'(3) = ")
    println(y1.dot)
    println("  Expected: f(3)=9, f'(3)=6")
    println("")

    // Test 2: Chain rule d/dx(exp(x²)) at x=1
    println("Test 2: d/dx(exp(x^2)) at x=1")
    let x2 = dual_var(1.0)
    let y2 = dual_exp(dual_mul(x2, x2))  // exp(x²)
    println("  f(x) = exp(x^2)")
    println("  f(1) = ")
    println(y2.val)
    println("  f'(1) = ")
    println(y2.dot)
    let expected_val = exp_f64(1.0)
    let expected_dot = 2.0 * exp_f64(1.0)  // 2x * exp(x²)
    println("  Expected: f(1)=e≈2.718, f'(1)=2e≈5.436")
    println("")

    // Test 3: Quotient rule d/dx(x/(1+x²)) at x=2
    println("Test 3: d/dx(x/(1+x^2)) at x=2")
    let x3 = dual_var(2.0)
    let one = dual_const(1.0)
    let y3 = dual_div(x3, dual_add(one, dual_mul(x3, x3)))  // x/(1+x²)
    println("  f(x) = x/(1+x^2)")
    println("  f(2) = ")
    println(y3.val)
    println("  f'(2) = ")
    println(y3.dot)
    // f(2) = 2/5 = 0.4
    // f'(x) = (1+x² - x*2x) / (1+x²)² = (1-x²) / (1+x²)²
    // f'(2) = (1-4) / 25 = -3/25 = -0.12
    println("  Expected: f(2)=0.4, f'(2)=-0.12")
    println("")

    // Test 4: sin(x) derivative at x=π/4
    println("Test 4: d/dx(sin(x)) at x=pi/4")
    let pi = 3.141592653589793
    let x4 = dual_var(pi / 4.0)
    let y4 = dual_sin(x4)
    println("  f(x) = sin(x)")
    println("  f(pi/4) = ")
    println(y4.val)
    println("  f'(pi/4) = ")
    println(y4.dot)
    println("  Expected: sin(pi/4)≈0.707, cos(pi/4)≈0.707")
    println("")

    // Test 5: Neural network activation chain
    println("Test 5: d/dx(sigmoid(tanh(x))) at x=0.5")
    let x5 = dual_var(0.5)
    let y5 = dual_sigmoid(dual_tanh(x5))
    println("  f(x) = sigmoid(tanh(x))")
    println("  f(0.5) = ")
    println(y5.val)
    println("  f'(0.5) = ")
    println(y5.dot)
    println("")

    // Test 6: Gradient of f(x,y,z) = x² + xy + z³ at (1,2,3)
    println("Test 6: Gradient of f(x,y,z) = x^2 + x*y + z^3 at (1,2,3)")
    // df/dx = 2x + y = 2 + 2 = 4
    let v_x = dualvec3_var_i(1.0, 2.0, 3.0, 0)
    let f_x = dual_add(dual_add(dual_mul(v_x.x, v_x.x), dual_mul(v_x.x, v_x.y)), dual_pow(v_x.z, 3.0))
    println("  df/dx = ")
    println(f_x.dot)
    // df/dy = x = 1
    let v_y = dualvec3_var_i(1.0, 2.0, 3.0, 1)
    let f_y = dual_add(dual_add(dual_mul(v_y.x, v_y.x), dual_mul(v_y.x, v_y.y)), dual_pow(v_y.z, 3.0))
    println("  df/dy = ")
    println(f_y.dot)
    // df/dz = 3z² = 27
    let v_z = dualvec3_var_i(1.0, 2.0, 3.0, 2)
    let f_z = dual_add(dual_add(dual_mul(v_z.x, v_z.x), dual_mul(v_z.x, v_z.y)), dual_pow(v_z.z, 3.0))
    println("  df/dz = ")
    println(f_z.dot)
    println("  Expected: grad = (4, 1, 27)")
    println("")

    // Validation
    let err1 = abs_f64(y1.val - 9.0) + abs_f64(y1.dot - 6.0)
    let err2 = abs_f64(y2.val - expected_val) + abs_f64(y2.dot - expected_dot)
    let err3 = abs_f64(y3.val - 0.4) + abs_f64(y3.dot + 0.12)
    let err6 = abs_f64(f_x.dot - 4.0) + abs_f64(f_y.dot - 1.0) + abs_f64(f_z.dot - 27.0)

    if err1 < 0.001 && err2 < 0.01 && err3 < 0.001 && err6 < 0.1 {
        println("TEST PASSED: All derivatives computed correctly")
        return 0
    } else {
        println("TEST FAILED: Derivative errors detected")
        println("  err1 = ")
        println(err1)
        println("  err2 = ")
        println(err2)
        println("  err3 = ")
        println(err3)
        println("  err6 = ")
        println(err6)
        return 1
    }
}
