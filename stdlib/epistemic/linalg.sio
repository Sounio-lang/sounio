//! stdlib/epistemic/linalg.d
//!
//! Epistemic Linear Algebra — Sounio v0.79
//!
//! Every matrix operation propagates uncertainty.
//! Every decomposition reports numerical stability.
//! Every eigenvalue carries confidence bounds.
//!
//! # Philosophy
//!
//! Traditional linear algebra treats matrices as exact. In reality:
//! - Measured data has noise
//! - Floating-point introduces error
//! - Ill-conditioned systems amplify uncertainty
//! - Eigenvalues of noisy matrices are uncertain
//!
//! Sounio linalg makes this EXPLICIT.
//!
//! # Quick Example
//!
//! ```sounio
//! use epistemic::linalg::{EMatrix, svd, eig}
//!
//! let A = EMatrix::from_measurements([
//!     [1.0 ± 0.1, 2.0 ± 0.2],
//!     [3.0 ± 0.1, 4.0 ± 0.2],
//! ])
//!
//! let result = svd(A)
//! println("Singular values: {:?}", result.singular_values)
//! println("Condition number: {} ± {}", result.condition.value, result.condition.std())
//! ```

module epistemic::linalg

use core::prelude::*
use core::option::{Option, Some, None}
use core::result::{Result, Ok, Err}
use epistemic::knowledge::{Knowledge, BetaConfidence, Source, Provenance, ProvenanceStep}
use epistemic::propagate::{sum_correlated, product_correlated}
use units::{dimensionless}

// ============================================================================
// STABILITY RISK LEVELS
// ============================================================================

/// Numerical stability risk classification
pub enum StabilityRisk {
    /// κ < 10³ — safe for most computations
    Low,
    /// 10³ ≤ κ < 10⁶ — use extended precision if possible
    Medium,
    /// 10⁶ ≤ κ < 10¹² — results may lose significant digits
    High,
    /// κ ≥ 10¹² — matrix is effectively singular
    Critical,
}

impl StabilityRisk {
    /// Convert condition number to risk level
    pub fn from_condition(kappa: f64) -> Self {
        if kappa < 1e3 {
            StabilityRisk::Low
        } else if kappa < 1e6 {
            StabilityRisk::Medium
        } else if kappa < 1e12 {
            StabilityRisk::High
        } else {
            StabilityRisk::Critical
        }
    }

    /// Confidence multiplier based on risk
    pub fn confidence_factor(self) -> f64 {
        match self {
            StabilityRisk::Low => 1.0,
            StabilityRisk::Medium => 0.9,
            StabilityRisk::High => 0.7,
            StabilityRisk::Critical => 0.3,
        }
    }
}

/// Comprehensive stability report
pub struct StabilityReport {
    pub risk: StabilityRisk,
    pub condition_number: f64,
    pub ulp_error_bound: f64,
    pub recommended_precision: Precision,
    pub warnings: Vec<string>,
}

pub enum Precision {
    F32,
    F64,
    F128,
    Arbitrary,
}

// ============================================================================
// EPISTEMIC VECTOR
// ============================================================================

/// Vector with per-element uncertainty
pub struct EVector<const N: usize> {
    data: [f64; N],
    variances: [f64; N],
    provenance: Provenance,
}

impl<const N: usize> EVector<N> {
    /// Create from exact values (zero variance)
    pub fn exact(values: [f64; N]) -> Self with Alloc {
        EVector {
            data: values,
            variances: [0.0; N],
            provenance: Provenance::new("exact_vector"),
        }
    }

    /// Create from measurements with uniform variance
    pub fn measured(values: [f64; N], std_dev: f64, source: string) -> Self with Alloc {
        let var = std_dev * std_dev
        EVector {
            data: values,
            variances: [var; N],
            provenance: Provenance::measurement(source),
        }
    }

    /// Create from Knowledge elements
    pub fn from_knowledge(elements: [Knowledge<f64>; N]) -> Self with Alloc {
        let mut data = [0.0; N]
        let mut variances = [0.0; N]
        let mut prov = Provenance::new("vector_assembly")

        for i in 0..N {
            data[i] = elements[i].value
            variances[i] = elements[i].variance()
            prov = prov.merge(elements[i].provenance)
        }

        EVector { data, variances, provenance: prov }
    }

    /// Get element as Knowledge
    pub fn get(self, i: usize) -> Knowledge<f64> {
        Knowledge::from_gaussian(
            self.data[i],
            self.variances[i],
            self.provenance.clone(),
        )
    }

    /// Euclidean norm with uncertainty propagation
    pub fn norm(self) -> Knowledge<f64> with Alloc {
        let mut sum_sq = 0.0
        let mut var_term = 0.0

        for i in 0..N {
            sum_sq += self.data[i] * self.data[i]
            var_term += self.data[i] * self.data[i] * self.variances[i]
        }

        let norm_val = sum_sq.sqrt()
        let norm_var = if norm_val > 1e-15 {
            var_term / sum_sq
        } else {
            f64::INFINITY
        }

        Knowledge::from_gaussian(
            norm_val,
            norm_var,
            self.provenance.step("norm", "Euclidean norm"),
        )
    }

    /// Dot product with covariance tracking
    pub fn dot(self, other: EVector<N>) -> Knowledge<f64> with Alloc {
        let mut result = 0.0
        let mut variance = 0.0

        for i in 0..N {
            result += self.data[i] * other.data[i]
            variance += other.data[i] * other.data[i] * self.variances[i]
                      + self.data[i] * self.data[i] * other.variances[i]
        }

        Knowledge::from_gaussian(
            result,
            variance,
            self.provenance.merge(other.provenance).step("dot", "dot product"),
        )
    }

    /// Element-wise addition
    pub fn add(self, other: EVector<N>) -> Self with Alloc {
        let mut data = [0.0; N]
        let mut variances = [0.0; N]

        for i in 0..N {
            data[i] = self.data[i] + other.data[i]
            variances[i] = self.variances[i] + other.variances[i]
        }

        EVector {
            data,
            variances,
            provenance: self.provenance.merge(other.provenance).step("add", "vector addition"),
        }
    }

    /// Element-wise subtraction
    pub fn sub(self, other: EVector<N>) -> Self with Alloc {
        let mut data = [0.0; N]
        let mut variances = [0.0; N]

        for i in 0..N {
            data[i] = self.data[i] - other.data[i]
            variances[i] = self.variances[i] + other.variances[i]
        }

        EVector {
            data,
            variances,
            provenance: self.provenance.merge(other.provenance).step("sub", "vector subtraction"),
        }
    }

    /// Scalar multiplication
    pub fn scale(self, scalar: Knowledge<f64>) -> Self with Alloc {
        let mut data = [0.0; N]
        let mut variances = [0.0; N]
        let s = scalar.value
        let s_var = scalar.variance()

        for i in 0..N {
            data[i] = s * self.data[i]
            variances[i] = self.data[i] * self.data[i] * s_var
                         + s * s * self.variances[i]
        }

        EVector {
            data,
            variances,
            provenance: self.provenance.merge(scalar.provenance).step("scale", "scalar multiply"),
        }
    }

    /// Total variance (trace of covariance)
    pub fn total_variance(self) -> f64 {
        let mut sum = 0.0
        for i in 0..N {
            sum += self.variances[i]
        }
        sum
    }

    /// Mean variance per element
    pub fn mean_variance(self) -> f64 {
        self.total_variance() / (N as f64)
    }

    /// Maximum element variance
    pub fn max_variance(self) -> f64 {
        let mut max_val = 0.0
        for i in 0..N {
            if self.variances[i] > max_val {
                max_val = self.variances[i]
            }
        }
        max_val
    }

    /// Standard deviation vector
    pub fn std_devs(self) -> [f64; N] {
        let mut result = [0.0; N]
        for i in 0..N {
            result[i] = sqrt_f64(self.variances[i])
        }
        result
    }
}

// ============================================================================
// EPISTEMIC MATRIX
// ============================================================================

/// Matrix with per-element uncertainty and stability tracking
pub struct EMatrix<const R: usize, const C: usize> {
    data: [f64; R * C],
    variances: [f64; R * C],
    condition_cache: Option<f64>,
    provenance: Provenance,
}

impl<const R: usize, const C: usize> EMatrix<R, C> {
    /// Create from exact values
    pub fn exact(values: [[f64; C]; R]) -> Self with Alloc {
        let mut data = [0.0; R * C]
        for i in 0..R {
            for j in 0..C {
                data[i * C + j] = values[i][j]
            }
        }

        EMatrix {
            data,
            variances: [0.0; R * C],
            condition_cache: None,
            provenance: Provenance::new("exact_matrix"),
        }
    }

    /// Create from measurements with uniform variance
    pub fn measured(values: [[f64; C]; R], std_dev: f64, source: string) -> Self with Alloc {
        let var = std_dev * std_dev
        let mut data = [0.0; R * C]

        for i in 0..R {
            for j in 0..C {
                data[i * C + j] = values[i][j]
            }
        }

        EMatrix {
            data,
            variances: [var; R * C],
            condition_cache: None,
            provenance: Provenance::measurement(source),
        }
    }

    /// Create from measurements with per-element variance
    pub fn from_measurements(values: [[f64; C]; R], variances: [[f64; C]; R], source: string) -> Self with Alloc {
        let mut data = [0.0; R * C]
        let mut vars = [0.0; R * C]

        for i in 0..R {
            for j in 0..C {
                data[i * C + j] = values[i][j]
                vars[i * C + j] = variances[i][j]
            }
        }

        EMatrix {
            data,
            variances: vars,
            condition_cache: None,
            provenance: Provenance::measurement(source),
        }
    }

    /// Zero matrix
    pub fn zeros() -> Self with Alloc {
        EMatrix {
            data: [0.0; R * C],
            variances: [0.0; R * C],
            condition_cache: Some(f64::INFINITY),
            provenance: Provenance::new("zeros"),
        }
    }

    /// Identity matrix
    pub fn identity() -> Self with Alloc where R == C {
        let mut data = [0.0; R * C]
        for i in 0..R {
            data[i * C + i] = 1.0
        }

        EMatrix {
            data,
            variances: [0.0; R * C],
            condition_cache: Some(1.0),
            provenance: Provenance::new("identity"),
        }
    }

    /// Diagonal matrix
    pub fn diagonal(diag: [f64; R]) -> Self with Alloc where R == C {
        let mut data = [0.0; R * C]
        for i in 0..R {
            data[i * C + i] = diag[i]
        }

        EMatrix {
            data,
            variances: [0.0; R * C],
            condition_cache: None,
            provenance: Provenance::new("diagonal"),
        }
    }

    /// Get element
    pub fn get(self, i: usize, j: usize) -> Knowledge<f64> {
        let idx = i * C + j
        Knowledge::from_gaussian(
            self.data[idx],
            self.variances[idx],
            self.provenance.clone(),
        )
    }

    /// Set element (returns new matrix)
    pub fn set(self, i: usize, j: usize, value: Knowledge<f64>) -> Self with Alloc {
        let mut new_data = self.data
        let mut new_vars = self.variances
        let idx = i * C + j
        new_data[idx] = value.value
        new_vars[idx] = value.variance()

        EMatrix {
            data: new_data,
            variances: new_vars,
            condition_cache: None,
            provenance: self.provenance.merge(value.provenance).step("set", "element update"),
        }
    }

    /// Get row as vector
    pub fn row(self, i: usize) -> EVector<C> with Alloc {
        let mut data = [0.0; C]
        let mut variances = [0.0; C]

        for j in 0..C {
            let idx = i * C + j
            data[j] = self.data[idx]
            variances[j] = self.variances[idx]
        }

        EVector {
            data,
            variances,
            provenance: self.provenance.step("row", format!("row {}", i)),
        }
    }

    /// Get column as vector
    pub fn col(self, j: usize) -> EVector<R> with Alloc {
        let mut data = [0.0; R]
        let mut variances = [0.0; R]

        for i in 0..R {
            let idx = i * C + j
            data[i] = self.data[idx]
            variances[i] = self.variances[idx]
        }

        EVector {
            data,
            variances,
            provenance: self.provenance.step("col", format!("col {}", j)),
        }
    }

    /// Frobenius norm with uncertainty
    pub fn frobenius_norm(self) -> Knowledge<f64> with Alloc {
        let mut sum_sq = 0.0
        let mut var_term = 0.0

        for k in 0..(R * C) {
            sum_sq += self.data[k] * self.data[k]
            var_term += self.data[k] * self.data[k] * self.variances[k]
        }

        let norm = sqrt_f64(sum_sq)
        let variance = if norm > 1e-15 { var_term / sum_sq } else { f64::INFINITY }

        Knowledge::from_gaussian(
            norm,
            variance,
            self.provenance.step("frobenius", "Frobenius norm"),
        )
    }

    /// Trace (sum of diagonal elements)
    pub fn trace(self) -> Knowledge<f64> with Alloc where R == C {
        let mut sum = 0.0
        let mut variance = 0.0

        for i in 0..R {
            let idx = i * C + i
            sum += self.data[idx]
            variance += self.variances[idx]
        }

        Knowledge::from_gaussian(
            sum,
            variance,
            self.provenance.step("trace", "matrix trace"),
        )
    }

    /// Transpose
    pub fn transpose(self) -> EMatrix<C, R> with Alloc {
        let mut data = [0.0; C * R]
        let mut variances = [0.0; C * R]

        for i in 0..R {
            for j in 0..C {
                let src = i * C + j
                let dst = j * R + i
                data[dst] = self.data[src]
                variances[dst] = self.variances[src]
            }
        }

        EMatrix {
            data,
            variances,
            condition_cache: self.condition_cache,
            provenance: self.provenance.step("transpose", "matrix transpose"),
        }
    }

    /// Matrix addition
    pub fn add(self, other: EMatrix<R, C>) -> Self with Alloc {
        let mut data = [0.0; R * C]
        let mut variances = [0.0; R * C]

        for k in 0..(R * C) {
            data[k] = self.data[k] + other.data[k]
            variances[k] = self.variances[k] + other.variances[k]
        }

        EMatrix {
            data,
            variances,
            condition_cache: None,
            provenance: self.provenance.merge(other.provenance).step("add", "matrix addition"),
        }
    }

    /// Matrix subtraction
    pub fn sub(self, other: EMatrix<R, C>) -> Self with Alloc {
        let mut data = [0.0; R * C]
        let mut variances = [0.0; R * C]

        for k in 0..(R * C) {
            data[k] = self.data[k] - other.data[k]
            variances[k] = self.variances[k] + other.variances[k]
        }

        EMatrix {
            data,
            variances,
            condition_cache: None,
            provenance: self.provenance.merge(other.provenance).step("sub", "matrix subtraction"),
        }
    }

    /// Scalar multiplication
    pub fn scale(self, scalar: Knowledge<f64>) -> Self with Alloc {
        let mut data = [0.0; R * C]
        let mut variances = [0.0; R * C]
        let s = scalar.value
        let s_var = scalar.variance()

        for k in 0..(R * C) {
            data[k] = s * self.data[k]
            variances[k] = self.data[k] * self.data[k] * s_var
                         + s * s * self.variances[k]
        }

        EMatrix {
            data,
            variances,
            condition_cache: None,
            provenance: self.provenance.merge(scalar.provenance).step("scale", "scalar multiply"),
        }
    }

    /// Matrix multiplication A × B
    pub fn matmul<const K: usize>(self, other: EMatrix<C, K>) -> EMatrix<R, K> with Alloc {
        let mut data = [0.0; R * K]
        let mut variances = [0.0; R * K]

        for i in 0..R {
            for j in 0..K {
                let mut sum = 0.0
                let mut var = 0.0

                for l in 0..C {
                    let a_idx = i * C + l
                    let b_idx = l * K + j
                    let a = self.data[a_idx]
                    let b = other.data[b_idx]
                    let va = self.variances[a_idx]
                    let vb = other.variances[b_idx]

                    sum += a * b
                    // Variance propagation: Var(AB) = B²Var(A) + A²Var(B) + Var(A)Var(B)
                    var += b * b * va + a * a * vb + va * vb
                }

                let idx = i * K + j
                data[idx] = sum
                variances[idx] = var
            }
        }

        EMatrix {
            data,
            variances,
            condition_cache: None,
            provenance: self.provenance.merge(other.provenance).step("matmul", "matrix multiply"),
        }
    }

    /// Matrix-vector multiplication A × v
    pub fn matvec(self, v: EVector<C>) -> EVector<R> with Alloc {
        let mut data = [0.0; R]
        let mut variances = [0.0; R]

        for i in 0..R {
            let mut sum = 0.0
            let mut var = 0.0

            for j in 0..C {
                let a_idx = i * C + j
                let a = self.data[a_idx]
                let va = self.variances[a_idx]
                let b = v.data[j]
                let vb = v.variances[j]

                sum += a * b
                var += b * b * va + a * a * vb + va * vb
            }

            data[i] = sum
            variances[i] = var
        }

        EVector {
            data,
            variances,
            provenance: self.provenance.merge(v.provenance).step("matvec", "matrix-vector multiply"),
        }
    }

    /// Total element variance
    pub fn total_variance(self) -> f64 {
        let mut sum = 0.0
        for k in 0..(R * C) {
            sum += self.variances[k]
        }
        sum
    }

    /// Maximum element variance
    pub fn max_variance(self) -> f64 {
        let mut max_val = 0.0
        for k in 0..(R * C) {
            if self.variances[k] > max_val {
                max_val = self.variances[k]
            }
        }
        max_val
    }

    /// Mean element variance
    pub fn mean_variance(self) -> f64 {
        self.total_variance() / ((R * C) as f64)
    }
}

// ============================================================================
// SVD DECOMPOSITION
// ============================================================================

/// SVD result with full uncertainty quantification
pub struct SVDResult<const M: usize, const N: usize> {
    pub U: EMatrix<M, {min_const(M, N)}>,
    pub singular_values: EVector<{min_const(M, N)}>,
    pub Vt: EMatrix<{min_const(M, N)}, N>,
    pub rank: Knowledge<usize>,
    pub condition: Knowledge<f64>,
    pub stability: StabilityReport,
}

/// Compute SVD with uncertainty propagation
pub fn svd<const M: usize, const N: usize>(
    A: EMatrix<M, N>
) -> SVDResult<M, N> with Alloc, Compute {

    let k = min_const(M, N)
    let (u_data, s_data, vt_data) = svd_core(A.data, M, N)

    // Propagate uncertainty to singular values using perturbation theory
    // For symmetric perturbations: δσ_i ≈ |u_i^T δA v_i|
    let mut s_variances = [0.0; k]

    for i in 0..k {
        let mut var_sum = 0.0
        for p in 0..M {
            for q in 0..N {
                let u_pi = u_data[p * k + i]
                let v_qi = vt_data[i * N + q]
                let a_var = A.variances[p * N + q]
                // Weissinger's formula: Var(σ_i) ≈ Σ_pq u_pi² v_qi² Var(A_pq)
                var_sum += u_pi * u_pi * v_qi * v_qi * a_var
            }
        }
        s_variances[i] = var_sum
    }

    let singular_values = EVector {
        data: s_data,
        variances: s_variances,
        provenance: A.provenance.step("svd_sigma", "singular values"),
    }

    // U and V uncertainty is harder to quantify precisely
    // Use conservative estimate based on input variance
    let avg_var = A.total_variance() / ((M * N) as f64)

    let U = EMatrix {
        data: u_data,
        variances: [avg_var * 0.1; M * k],
        condition_cache: Some(1.0),
        provenance: A.provenance.step("svd_U", "left singular vectors"),
    }

    let Vt = EMatrix {
        data: vt_data,
        variances: [avg_var * 0.1; k * N],
        condition_cache: Some(1.0),
        provenance: A.provenance.step("svd_Vt", "right singular vectors"),
    }

    // Condition number κ = σ_1 / σ_n
    let sigma_1 = s_data[0]
    let sigma_n = s_data[k - 1]
    let kappa = if sigma_n > 1e-15 { sigma_1 / sigma_n } else { f64::INFINITY }

    // Propagate variance to condition number
    // Var(σ_1/σ_n) ≈ Var(σ_1)/σ_n² + σ_1²·Var(σ_n)/σ_n⁴
    let kappa_var = if sigma_n > 1e-15 {
        let s1_var = s_variances[0]
        let sn_var = s_variances[k - 1]
        (s1_var / (sigma_n * sigma_n)) + (sigma_1 * sigma_1 * sn_var / pow_f64(sigma_n, 4.0))
    } else {
        f64::INFINITY
    }

    let condition = Knowledge::from_gaussian(
        kappa,
        kappa_var,
        A.provenance.step("condition", "condition number κ(A)"),
    )

    // Numerical rank estimation
    let tol = 1e-10 * sigma_1 * (max(M, N) as f64)
    let mut rank_val = 0usize
    for i in 0..k {
        if s_data[i] > tol {
            rank_val += 1
        }
    }

    // Confidence in rank based on gap between smallest kept and largest discarded
    let rank_confidence = if k > 0 && rank_val < k && rank_val > 0 {
        let gap = s_data[rank_val - 1] - tol
        let noise = sqrt_f64(s_variances[rank_val - 1])
        if noise > 0.0 { min_f64(gap / noise, 1.0) } else { 1.0 }
    } else {
        0.95
    }

    let rank = Knowledge::new(
        rank_val,
        BetaConfidence::strong(rank_confidence),
        A.provenance.step("rank", "numerical rank"),
    )

    let stability = StabilityReport {
        risk: StabilityRisk::from_condition(kappa),
        condition_number: kappa,
        ulp_error_bound: kappa * f64::EPSILON,
        recommended_precision: if kappa > 1e8 { Precision::F128 } else { Precision::F64 },
        warnings: build_svd_warnings(kappa, rank_val, k),
    }

    SVDResult { U, singular_values, Vt, rank, condition, stability }
}

fn build_svd_warnings(kappa: f64, rank: usize, max_rank: usize) -> Vec<string> with Alloc {
    let mut warnings = Vec::new()
    if kappa > 1e12 {
        warnings.push("Matrix is effectively singular (κ > 10¹²)")
    } else if kappa > 1e6 {
        warnings.push("High condition number — results may lose precision")
    }
    if rank < max_rank {
        warnings.push(format!("Rank deficient: {} < {} (numerical)", rank, max_rank))
    }
    warnings
}

// ============================================================================
// EIGENDECOMPOSITION
// ============================================================================

/// Eigendecomposition result with uncertainty
pub struct EigResult<const N: usize> {
    pub eigenvalues: EVector<N>,
    pub eigenvalues_imag: Option<EVector<N>>,
    pub eigenvectors: EMatrix<N, N>,
    pub gap_ratios: [f64; N],
    pub is_defective: bool,
    pub stability: StabilityReport,
}

/// Symmetric eigendecomposition with uncertainty propagation
pub fn eig_symmetric<const N: usize>(
    A: EMatrix<N, N>
) -> EigResult<N> with Alloc, Compute {
    let (eigenvalues, eigenvectors) = eig_symmetric_core(A.data, N)

    // For symmetric matrices, eigenvalue perturbation is well-behaved:
    // |δλ_i| ≤ ||δA||_F
    let total_a_var = A.total_variance()
    let spectral_var_bound = total_a_var

    // More refined: use Weyl's inequality and first-order perturbation
    let mut e_variances = [0.0; N]
    for i in 0..N {
        // δλ_i ≈ v_i^T δA v_i for symmetric A
        // Var(λ_i) ≈ Σ_pq v_ip² v_iq² Var(A_pq)
        let mut var_sum = 0.0
        for p in 0..N {
            for q in 0..N {
                let v_ip = eigenvectors[p * N + i]
                let v_iq = eigenvectors[q * N + i]
                let a_var = A.variances[p * N + q]
                var_sum += v_ip * v_ip * v_iq * v_iq * a_var
            }
        }
        e_variances[i] = var_sum
    }

    // Compute gap ratios (eigenvalue separation / perturbation)
    let mut gap_ratios = [0.0; N]
    for i in 0..N {
        let lambda_i = eigenvalues[i]
        let mut min_gap = f64::INFINITY
        for j in 0..N {
            if i != j {
                let gap = abs_f64(lambda_i - eigenvalues[j])
                if gap < min_gap {
                    min_gap = gap
                }
            }
        }
        let pert = sqrt_f64(e_variances[i])
        gap_ratios[i] = if pert > 0.0 { min_gap / pert } else { 100.0 }
    }

    let eigenvalues_vec = EVector {
        data: eigenvalues,
        variances: e_variances,
        provenance: A.provenance.step("eig_values", "eigenvalues"),
    }

    // Eigenvector uncertainty depends on eigenvalue gaps
    let avg_var = A.total_variance() / ((N * N) as f64)
    let eigenvectors_mat = EMatrix {
        data: eigenvectors,
        variances: [avg_var * 0.1; N * N],
        condition_cache: Some(1.0),
        provenance: A.provenance.step("eig_vectors", "eigenvectors"),
    }

    // Condition number for eigenvalue problem
    let mut lambda_max = 0.0
    let mut lambda_min = f64::INFINITY
    for i in 0..N {
        let abs_lambda = abs_f64(eigenvalues[i])
        if abs_lambda > lambda_max {
            lambda_max = abs_lambda
        }
        if abs_lambda > 1e-15 && abs_lambda < lambda_min {
            lambda_min = abs_lambda
        }
    }
    let kappa = if lambda_min > 0.0 { lambda_max / lambda_min } else { f64::INFINITY }

    let stability = StabilityReport {
        risk: StabilityRisk::from_condition(kappa),
        condition_number: kappa,
        ulp_error_bound: kappa * f64::EPSILON * (N as f64),
        recommended_precision: if kappa > 1e8 { Precision::F128 } else { Precision::F64 },
        warnings: Vec::new(),
    }

    EigResult {
        eigenvalues: eigenvalues_vec,
        eigenvalues_imag: None,
        eigenvectors: eigenvectors_mat,
        gap_ratios,
        is_defective: false,
        stability,
    }
}

// ============================================================================
// LINEAR SOLVERS
// ============================================================================

/// Solve Ax = b with full uncertainty quantification
pub fn solve<const N: usize>(
    A: EMatrix<N, N>,
    b: EVector<N>,
) -> Result<EVector<N>, string> with Alloc, Compute {
    let lu_result = lu(A)?

    // Forward substitution: Ly = Pb
    let mut y = [0.0; N]
    let mut y_var = [0.0; N]

    for i in 0..N {
        let pi = lu_result.permutation[i]
        let mut sum = b.data[pi]
        let mut var = b.variances[pi]

        for j in 0..i {
            let l_ij = lu_result.L.data[i * N + j]
            let l_var = lu_result.L.variances[i * N + j]
            sum -= l_ij * y[j]
            var += l_ij * l_ij * y_var[j] + y[j] * y[j] * l_var
        }

        y[i] = sum
        y_var[i] = var
    }

    // Backward substitution: Ux = y
    let mut x = [0.0; N]
    let mut x_var = [0.0; N]

    for ii in 0..N {
        let i = N - 1 - ii  // Reverse iteration
        let mut sum = y[i]
        let mut var = y_var[i]

        for j in (i + 1)..N {
            let u_ij = lu_result.U.data[i * N + j]
            let u_var = lu_result.U.variances[i * N + j]
            sum -= u_ij * x[j]
            var += u_ij * u_ij * x_var[j] + x[j] * x[j] * u_var
        }

        let u_ii = lu_result.U.data[i * N + i]
        if abs_f64(u_ii) < 1e-15 {
            return Err("Singular matrix in solve")
        }

        x[i] = sum / u_ii
        // Var(a/b) ≈ Var(a)/b² + a²·Var(b)/b⁴
        let u_ii_var = lu_result.U.variances[i * N + i]
        x_var[i] = var / (u_ii * u_ii)
                 + sum * sum * u_ii_var / pow_f64(u_ii, 4.0)
    }

    Ok(EVector {
        data: x,
        variances: x_var,
        provenance: A.provenance.merge(b.provenance).step("solve", "linear solve Ax=b"),
    })
}

/// Solve least squares min ||Ax - b||² with uncertainty
pub fn lstsq<const M: usize, const N: usize>(
    A: EMatrix<M, N>,
    b: EVector<M>,
) -> Result<EVector<N>, string> with Alloc, Compute where M >= N {
    let svd_result = svd(A)

    // x = V Σ⁺ Uᵀ b
    // First compute Uᵀ b
    let mut utb = [0.0; N]
    let mut utb_var = [0.0; N]

    for i in 0..N {
        let mut sum = 0.0
        let mut var = 0.0
        for j in 0..M {
            let u_ji = svd_result.U.data[j * N + i]
            let u_var = svd_result.U.variances[j * N + i]
            sum += u_ji * b.data[j]
            var += u_ji * u_ji * b.variances[j] + b.data[j] * b.data[j] * u_var
        }
        utb[i] = sum
        utb_var[i] = var
    }

    // Apply Σ⁺ (pseudoinverse of singular values)
    let tol = 1e-10 * svd_result.singular_values.data[0]
    let mut sinv_utb = [0.0; N]
    let mut sinv_utb_var = [0.0; N]

    for i in 0..N {
        let sigma = svd_result.singular_values.data[i]
        let sigma_var = svd_result.singular_values.variances[i]

        if sigma > tol {
            sinv_utb[i] = utb[i] / sigma
            sinv_utb_var[i] = utb_var[i] / (sigma * sigma)
                            + utb[i] * utb[i] * sigma_var / pow_f64(sigma, 4.0)
        } else {
            sinv_utb[i] = 0.0
            sinv_utb_var[i] = 0.0
        }
    }

    // Finally multiply by V
    let mut x = [0.0; N]
    let mut x_var = [0.0; N]

    for i in 0..N {
        let mut sum = 0.0
        let mut var = 0.0
        for j in 0..N {
            let v_ij = svd_result.Vt.data[j * N + i]  // V = Vt.T, so V[i,j] = Vt[j,i]
            let v_var = svd_result.Vt.variances[j * N + i]
            sum += v_ij * sinv_utb[j]
            var += v_ij * v_ij * sinv_utb_var[j] + sinv_utb[j] * sinv_utb[j] * v_var
        }
        x[i] = sum
        x_var[i] = var
    }

    Ok(EVector {
        data: x,
        variances: x_var,
        provenance: A.provenance.merge(b.provenance).step("lstsq", "least squares"),
    })
}

// ============================================================================
// LU DECOMPOSITION
// ============================================================================

/// LU decomposition result
pub struct LUResult<const N: usize> {
    pub L: EMatrix<N, N>,
    pub U: EMatrix<N, N>,
    pub permutation: [usize; N],
    pub pivot_growth: f64,
    pub stability: StabilityReport,
}

/// LU decomposition with partial pivoting
pub fn lu<const N: usize>(A: EMatrix<N, N>) -> Result<LUResult<N>, string> with Alloc, Compute {
    let (l_data, u_data, perm, pivot_growth, singular) = lu_core(A.data, N)

    if singular {
        return Err("Matrix is singular — LU decomposition failed")
    }

    // Variance amplification through LU
    let avg_var = A.total_variance() / ((N * N) as f64)
    let var_amplification = pivot_growth * pivot_growth

    let L = EMatrix {
        data: l_data,
        variances: [avg_var * var_amplification; N * N],
        condition_cache: Some(1.0),
        provenance: A.provenance.step("lu_L", "lower triangular L"),
    }

    let U = EMatrix {
        data: u_data,
        variances: [avg_var * var_amplification; N * N],
        condition_cache: None,
        provenance: A.provenance.step("lu_U", "upper triangular U"),
    }

    let stability = StabilityReport {
        risk: if pivot_growth > 1e8 { StabilityRisk::High } else { StabilityRisk::Low },
        condition_number: pivot_growth,
        ulp_error_bound: pivot_growth * f64::EPSILON * (N as f64),
        recommended_precision: Precision::F64,
        warnings: if pivot_growth > 1e6 { vec!["High pivot growth — possible instability"] } else { Vec::new() },
    }

    Ok(LUResult { L, U, permutation: perm, pivot_growth, stability })
}

// ============================================================================
// CHOLESKY DECOMPOSITION
// ============================================================================

/// Cholesky decomposition result
pub struct CholeskyResult<const N: usize> {
    pub L: EMatrix<N, N>,
    pub stability: StabilityReport,
}

/// Cholesky decomposition for positive definite matrices
pub fn cholesky<const N: usize>(A: EMatrix<N, N>) -> Result<CholeskyResult<N>, string> with Alloc, Compute {
    let (l_data, success) = cholesky_core(A.data, N)

    if !success {
        return Err("Matrix is not positive definite — Cholesky failed")
    }

    // Variance propagation through Cholesky
    let avg_var = A.total_variance() / ((N * N) as f64)

    let L = EMatrix {
        data: l_data,
        variances: [avg_var; N * N],
        condition_cache: None,
        provenance: A.provenance.step("cholesky_L", "Cholesky factor"),
    }

    let stability = StabilityReport {
        risk: StabilityRisk::Low,
        condition_number: 1.0,
        ulp_error_bound: f64::EPSILON * (N as f64),
        recommended_precision: Precision::F64,
        warnings: Vec::new(),
    }

    Ok(CholeskyResult { L, stability })
}

// ============================================================================
// QR DECOMPOSITION
// ============================================================================

/// QR decomposition result
pub struct QRResult<const M: usize, const N: usize> {
    pub Q: EMatrix<M, M>,
    pub R: EMatrix<M, N>,
    pub stability: StabilityReport,
}

/// QR decomposition with Householder reflections
pub fn qr<const M: usize, const N: usize>(A: EMatrix<M, N>) -> QRResult<M, N> with Alloc, Compute {
    let (q_data, r_data) = qr_core(A.data, M, N)

    let avg_var = A.total_variance() / ((M * N) as f64)

    let Q = EMatrix {
        data: q_data,
        variances: [avg_var * 0.1; M * M],
        condition_cache: Some(1.0),
        provenance: A.provenance.step("qr_Q", "orthogonal Q"),
    }

    let R = EMatrix {
        data: r_data,
        variances: [avg_var; M * N],
        condition_cache: None,
        provenance: A.provenance.step("qr_R", "upper triangular R"),
    }

    let stability = StabilityReport {
        risk: StabilityRisk::Low,
        condition_number: 1.0,
        ulp_error_bound: f64::EPSILON * (M as f64),
        recommended_precision: Precision::F64,
        warnings: Vec::new(),
    }

    QRResult { Q, R, stability }
}

// ============================================================================
// GPU KERNELS
// ============================================================================

/// GPU matrix multiplication with uncertainty propagation
kernel fn matmul_gpu<const M: usize, const K: usize, const N: usize>(
    A: &[f64; M * K],
    A_var: &[f64; M * K],
    B: &[f64; K * N],
    B_var: &[f64; K * N],
    C: &mut [f64; M * N],
    C_var: &mut [f64; M * N],
) with GPU {
    let row = gpu.block_id.y * gpu.block_dim.y + gpu.thread_id.y
    let col = gpu.block_id.x * gpu.block_dim.x + gpu.thread_id.x

    if row < M && col < N {
        let mut sum = 0.0
        let mut var = 0.0

        for k in 0..K {
            let a = A[row * K + k]
            let b = B[k * N + col]
            let va = A_var[row * K + k]
            let vb = B_var[k * N + col]

            sum += a * b
            var += b * b * va + a * a * vb
        }

        C[row * N + col] = sum
        C_var[row * N + col] = var
    }
}

/// GPU vector dot product with variance
kernel fn dot_gpu<const N: usize>(
    a: &[f64; N],
    a_var: &[f64; N],
    b: &[f64; N],
    b_var: &[f64; N],
    result: &mut f64,
    result_var: &mut f64,
) with GPU {
    // Shared memory for reduction
    shared let partial_sum: [f64; 256]
    shared let partial_var: [f64; 256]

    let tid = gpu.thread_id.x
    let i = gpu.block_id.x * gpu.block_dim.x + tid

    if i < N {
        partial_sum[tid] = a[i] * b[i]
        partial_var[tid] = b[i] * b[i] * a_var[i] + a[i] * a[i] * b_var[i]
    } else {
        partial_sum[tid] = 0.0
        partial_var[tid] = 0.0
    }

    gpu.sync_threads()

    // Parallel reduction
    let mut s = gpu.block_dim.x / 2
    while s > 0 {
        if tid < s {
            partial_sum[tid] += partial_sum[tid + s]
            partial_var[tid] += partial_var[tid + s]
        }
        gpu.sync_threads()
        s = s / 2
    }

    if tid == 0 {
        gpu.atomic_add(result, partial_sum[0])
        gpu.atomic_add(result_var, partial_var[0])
    }
}

// ============================================================================
// UTILITY FUNCTIONS
// ============================================================================

const fn min_const(a: usize, b: usize) -> usize {
    if a < b { a } else { b }
}

fn min(a: usize, b: usize) -> usize {
    if a < b { a } else { b }
}

fn max(a: usize, b: usize) -> usize {
    if a > b { a } else { b }
}

fn min_f64(a: f64, b: f64) -> f64 {
    if a < b { a } else { b }
}

fn max_f64(a: f64, b: f64) -> f64 {
    if a > b { a } else { b }
}

fn abs_f64(x: f64) -> f64 {
    if x < 0.0 { 0.0 - x } else { x }
}

fn sqrt_f64(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 }
    // Newton-Raphson iteration
    let mut y = x
    for _ in 0..10 {
        y = 0.5 * (y + x / y)
    }
    y
}

fn pow_f64(base: f64, exp: f64) -> f64 {
    // Simple integer power for common cases
    if exp == 2.0 { return base * base }
    if exp == 3.0 { return base * base * base }
    if exp == 4.0 { let x2 = base * base; return x2 * x2 }
    // Fall back to exp(exp * ln(base)) for general case
    exp_f64(exp * ln_f64(base))
}

fn exp_f64(x: f64) -> f64 {
    // Taylor series approximation
    let mut sum = 1.0
    let mut term = 1.0
    for i in 1..20 {
        term *= x / (i as f64)
        sum += term
        if abs_f64(term) < 1e-15 { break }
    }
    sum
}

fn ln_f64(x: f64) -> f64 {
    if x <= 0.0 { return 0.0 - f64::INFINITY }
    // Newton-Raphson for ln
    let mut y = x - 1.0
    for _ in 0..20 {
        let ey = exp_f64(y)
        y = y + 2.0 * (x - ey) / (x + ey)
    }
    y
}

// ============================================================================
// EXTERNAL CORE ROUTINES
// ============================================================================

extern "D" {
    fn svd_core(data: [f64], m: usize, n: usize) -> ([f64], [f64], [f64])
    fn eig_symmetric_core(data: [f64], n: usize) -> ([f64], [f64])
    fn lu_core(data: [f64], n: usize) -> ([f64], [f64], [usize], f64, bool)
    fn cholesky_core(data: [f64], n: usize) -> ([f64], bool)
    fn qr_core(data: [f64], m: usize, n: usize) -> ([f64], [f64])
}

// ============================================================================
// TESTS
// ============================================================================

#[test]
fn test_vector_dot_variance() with Alloc {
    let v1 = EVector::measured([1.0, 2.0, 3.0], 0.1, "sensor_a")
    let v2 = EVector::measured([4.0, 5.0, 6.0], 0.1, "sensor_b")

    let dot = v1.dot(v2)

    // Expected: 1*4 + 2*5 + 3*6 = 32
    assert_eq!(dot.value, 32.0)
    // Variance should be positive due to input uncertainty
    assert!(dot.variance() > 0.0)
}

#[test]
fn test_vector_norm() with Alloc {
    let v = EVector::exact([3.0, 4.0])
    let norm = v.norm()

    // ||[3, 4]|| = 5
    assert_eq!(norm.value, 5.0)
    // Exact input means zero variance
    assert_eq!(norm.variance(), 0.0)
}

#[test]
fn test_matrix_matmul() with Alloc {
    let A = EMatrix::exact([
        [1.0, 2.0],
        [3.0, 4.0],
    ])
    let B = EMatrix::exact([
        [5.0, 6.0],
        [7.0, 8.0],
    ])

    let C = A.matmul(B)

    // C[0,0] = 1*5 + 2*7 = 19
    // C[0,1] = 1*6 + 2*8 = 22
    // C[1,0] = 3*5 + 4*7 = 43
    // C[1,1] = 3*6 + 4*8 = 50
    assert_eq!(C.get(0, 0).value, 19.0)
    assert_eq!(C.get(0, 1).value, 22.0)
    assert_eq!(C.get(1, 0).value, 43.0)
    assert_eq!(C.get(1, 1).value, 50.0)
}

#[test]
fn test_matrix_transpose() with Alloc {
    let A = EMatrix::exact([
        [1.0, 2.0, 3.0],
        [4.0, 5.0, 6.0],
    ])

    let At = A.transpose()

    assert_eq!(At.get(0, 0).value, 1.0)
    assert_eq!(At.get(0, 1).value, 4.0)
    assert_eq!(At.get(1, 0).value, 2.0)
    assert_eq!(At.get(2, 1).value, 6.0)
}

#[test]
fn test_svd_condition_number() with Alloc, Compute {
    let well_conditioned = EMatrix::exact([
        [4.0, 2.0],
        [2.0, 3.0],
    ])

    let result = svd(well_conditioned)

    // Well-conditioned matrix should have low condition number
    assert!(result.condition.value < 10.0)
    assert_eq!(result.stability.risk, StabilityRisk::Low)
}

#[test]
fn test_identity_properties() with Alloc {
    let I: EMatrix<3, 3> = EMatrix::identity()

    // Trace should be 3
    let tr = I.trace()
    assert_eq!(tr.value, 3.0)

    // Frobenius norm should be sqrt(3)
    let norm = I.frobenius_norm()
    assert!((norm.value - sqrt_f64(3.0)).abs() < 1e-10)
}

#[test]
fn test_variance_propagation_matmul() with Alloc {
    let A = EMatrix::measured([
        [1.0, 0.0],
        [0.0, 1.0],
    ], 0.1, "sensor")

    let B = EMatrix::exact([
        [2.0, 0.0],
        [0.0, 2.0],
    ])

    let C = A.matmul(B)

    // Result should have variance from A propagated
    assert!(C.total_variance() > 0.0)
}

#[test]
fn test_solve_simple() with Alloc, Compute {
    // Solve: [2, 1; 1, 3] x = [5; 7]
    // Solution: x = [1; 2]
    let A = EMatrix::exact([
        [2.0, 1.0],
        [1.0, 3.0],
    ])
    let b = EVector::exact([5.0, 7.0])

    let result = solve(A, b)
    assert!(result.is_ok())

    let x = result.unwrap()
    assert!((x.data[0] - 1.0).abs() < 1e-10)
    assert!((x.data[1] - 2.0).abs() < 1e-10)
}
