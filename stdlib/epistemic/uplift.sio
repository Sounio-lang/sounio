/// epistemic::uplift — Uplift Modeling with Epistemic Uncertainty
///
/// Heterogeneous Treatment Effect (HTE) estimation with full uncertainty quantification:
/// - **Meta-Learners**: S-Learner, T-Learner, X-Learner, R-Learner with epistemic variance
/// - **CATE Estimation**: Conditional Average Treatment Effects with credible intervals
/// - **Uplift Trees**: Decision trees for treatment effect heterogeneity
/// - **Evaluation**: Qini curves, uplift calibration, AUUC with uncertainty
///
/// # Philosophy
///
/// Traditional uplift: "This patient has 0.3 probability of responding to treatment"
/// Sounio epistemic: "This patient has CATE = 0.3 ± 0.08 (95% CI), based on
///                       500 similar patients, with 0.02 residual model uncertainty"
///
/// Every treatment effect estimate includes:
/// - Point estimate of individual/conditional effect
/// - Epistemic uncertainty (how much do we know?)
/// - Aleatoric uncertainty (inherent variability)
/// - Sample size contribution to confidence
///
/// # Quick Start
///
/// ```sounio
/// use std::epistemic::uplift::{TLearner, fit_t_learner, predict_cate}
///
/// // Fit T-Learner for CATE estimation
/// let model = fit_t_learner(X_train, treatment, outcome)
///
/// // Predict CATE for new individual
/// let cate = predict_cate(model, x_new)
/// // cate.value = 0.3, cate.variance = 0.006, cate.confidence = high
///
/// // Get targeting recommendations
/// let targeting = recommend_targeting(model, X_population, budget_fraction)
/// ```

use std::epistemic::knowledge::{Knowledge, Confidence, Provenance}
use std::epistemic::linalg::{EVector, EMatrix, evec_new, evec_zeros, emat_zeros}
use std::epistemic::stats::{Beta, beta_new, beta_mean, beta_variance}

// ============================================================================
// Core Types for Uplift Modeling
// ============================================================================

/// Individual with features for CATE estimation
struct Individual<const D: usize> {
    features: EVector<D>,      // Covariates with measurement uncertainty
    treatment: Option<bool>,   // Treatment assignment (if observed)
    outcome: Option<f64>,      // Observed outcome (if available)
}

/// Treatment effect estimate for an individual
struct IndividualTreatmentEffect {
    cate: Knowledge<f64>,       // Conditional ATE
    ite_lower: f64,             // ITE credible interval lower
    ite_upper: f64,             // ITE credible interval upper
    treatment_prob: f64,        // Propensity score
    response_prob_treated: f64, // P(Y=1|T=1,X)
    response_prob_control: f64, // P(Y=1|T=0,X)
    uncertainty_type: UncertaintyType,
}

/// Type of uncertainty in treatment effect
enum UncertaintyType {
    Epistemic,     // Reducible with more data
    Aleatoric,     // Irreducible randomness
    Mixed,         // Both components
}

/// CATE model predictions
struct CATEPrediction<const D: usize> {
    individual: Individual<D>,
    effect: IndividualTreatmentEffect,
    similar_individuals: i32,   // Number of similar observations
    model_confidence: f64,      // Model's confidence in prediction
}

// ============================================================================
// Meta-Learner Types
// ============================================================================

/// S-Learner: Single model with treatment as feature
struct SLearner<const D: usize> {
    base_model: RegressionModel<D + 1>,  // Augmented feature space
    treatment_idx: usize,                 // Index of treatment indicator
    calibration: CalibrationData,
}

/// T-Learner: Separate models for treatment/control
struct TLearner<const D: usize> {
    model_treated: RegressionModel<D>,
    model_control: RegressionModel<D>,
    propensity_model: Option<ClassificationModel<D>>,
    n_treated: i32,
    n_control: i32,
}

/// X-Learner: Cross-fitting for improved CATE
struct XLearner<const D: usize> {
    model_treated: RegressionModel<D>,
    model_control: RegressionModel<D>,
    cate_model_treated: RegressionModel<D>,  // CATE from treatment group
    cate_model_control: RegressionModel<D>,  // CATE from control group
    propensity_model: ClassificationModel<D>,
}

/// R-Learner: Robinson's residual-based learner
struct RLearner<const D: usize> {
    outcome_model: RegressionModel<D>,
    propensity_model: ClassificationModel<D>,
    cate_model: RegressionModel<D>,
    residual_variance: f64,
}

/// DR-Learner: Doubly-robust learner
struct DRLearner<const D: usize> {
    outcome_model_treated: RegressionModel<D>,
    outcome_model_control: RegressionModel<D>,
    propensity_model: ClassificationModel<D>,
    cate_model: RegressionModel<D>,
}

/// Generic trait for CATE estimators
trait CATEEstimator<const D: usize> {
    fn predict_cate(self: &Self, x: &EVector<D>) -> IndividualTreatmentEffect
    fn predict_batch(self: &Self, X: &[EVector<D>]) -> [IndividualTreatmentEffect]
    fn feature_importance(self: &Self) -> EVector<D>
}

// ============================================================================
// Base Model Types (Simplified for Demonstration)
// ============================================================================

/// Regression model (abstract)
struct RegressionModel<const D: usize> {
    coefficients: EVector<D>,
    intercept: Knowledge<f64>,
    residual_variance: f64,
    r_squared: f64,
    n_samples: i32,
}

/// Classification model (abstract)
struct ClassificationModel<const D: usize> {
    coefficients: EVector<D>,
    intercept: f64,
    calibration: CalibrationData,
}

/// Calibration data for probabilistic models
struct CalibrationData {
    bins: [f64],           // Predicted probability bins
    observed_freq: [f64],  // Observed frequency per bin
    n_per_bin: [i32],      // Samples per bin
}

// ============================================================================
// S-Learner Implementation
// ============================================================================

/// Fit S-Learner model
fn fit_s_learner<const D: usize>(
    X: &[EVector<D>],
    treatment: &[bool],
    outcome: &[f64]
) -> SLearner<D> with Alloc {
    let n = len(X) as i32

    // Augment features with treatment indicator
    var X_augmented: [EVector<D + 1>] = []
    for i in 0..n {
        var x_aug = evec_zeros::<D + 1>()
        for j in 0..D {
            x_aug.values[j] = X[i].values[j]
            x_aug.variances[j] = X[i].variances[j]
        }
        x_aug.values[D] = if treatment[i] { 1.0 } else { 0.0 }
        X_augmented = X_augmented ++ [x_aug]
    }

    // Fit regression model
    let model = fit_regression(&X_augmented, outcome)

    // Compute calibration
    let calibration = compute_calibration(&X_augmented, outcome, &model)

    SLearner {
        base_model: model,
        treatment_idx: D,
        calibration: calibration,
    }
}

/// Predict CATE with S-Learner
fn s_learner_predict<const D: usize>(
    learner: &SLearner<D>,
    x: &EVector<D>
) -> IndividualTreatmentEffect {
    // Predict with treatment = 1
    var x_treated = evec_zeros::<D + 1>()
    for j in 0..D {
        x_treated.values[j] = x.values[j]
        x_treated.variances[j] = x.variances[j]
    }
    x_treated.values[D] = 1.0

    let y_treated = predict_regression(&learner.base_model, &x_treated)

    // Predict with treatment = 0
    var x_control = x_treated
    x_control.values[D] = 0.0

    let y_control = predict_regression(&learner.base_model, &x_control)

    // CATE = E[Y|T=1,X] - E[Y|T=0,X]
    let cate_value = y_treated.value - y_control.value

    // Variance: sum of prediction variances (conservative)
    let cate_variance = y_treated.variance + y_control.variance

    // Treatment coefficient gives direct effect estimate
    let treatment_coef_var = learner.base_model.coefficients.variances[D]

    IndividualTreatmentEffect {
        cate: Knowledge {
            value: cate_value,
            variance: cate_variance + treatment_coef_var,
            confidence: Confidence::Frequentist {
                sample_size: learner.base_model.n_samples,
                confidence_level: 0.95,
            },
            provenance: Provenance::SLearner {
                n_samples: learner.base_model.n_samples,
                r_squared: learner.base_model.r_squared,
            },
        },
        ite_lower: cate_value - 1.96 * sqrt(cate_variance),
        ite_upper: cate_value + 1.96 * sqrt(cate_variance),
        treatment_prob: 0.5,  // Unknown without propensity model
        response_prob_treated: sigmoid(y_treated.value),
        response_prob_control: sigmoid(y_control.value),
        uncertainty_type: UncertaintyType::Epistemic,
    }
}

// ============================================================================
// T-Learner Implementation
// ============================================================================

/// Fit T-Learner model (separate models for treated/control)
fn fit_t_learner<const D: usize>(
    X: &[EVector<D>],
    treatment: &[bool],
    outcome: &[f64]
) -> TLearner<D> with Alloc {
    // Split data by treatment assignment
    var X_treated: [EVector<D>] = []
    var y_treated: [f64] = []
    var X_control: [EVector<D>] = []
    var y_control: [f64] = []

    for i in 0..len(X) {
        if treatment[i] {
            X_treated = X_treated ++ [X[i]]
            y_treated = y_treated ++ [outcome[i]]
        } else {
            X_control = X_control ++ [X[i]]
            y_control = y_control ++ [outcome[i]]
        }
    }

    // Fit separate models
    let model_treated = fit_regression(&X_treated, &y_treated)
    let model_control = fit_regression(&X_control, &y_control)

    TLearner {
        model_treated: model_treated,
        model_control: model_control,
        propensity_model: None,
        n_treated: len(X_treated) as i32,
        n_control: len(X_control) as i32,
    }
}

/// Predict CATE with T-Learner
fn t_learner_predict<const D: usize>(
    learner: &TLearner<D>,
    x: &EVector<D>
) -> IndividualTreatmentEffect {
    let y_treated = predict_regression(&learner.model_treated, x)
    let y_control = predict_regression(&learner.model_control, x)

    let cate_value = y_treated.value - y_control.value

    // Variance from both models plus between-model variance
    let within_var = y_treated.variance + y_control.variance

    // Adjust for sample size imbalance
    let n_effective = 2.0 * (learner.n_treated as f64 * learner.n_control as f64) /
                      (learner.n_treated + learner.n_control) as f64
    let sample_var = within_var / n_effective

    let total_var = within_var + sample_var

    IndividualTreatmentEffect {
        cate: Knowledge {
            value: cate_value,
            variance: total_var,
            confidence: Confidence::Frequentist {
                sample_size: learner.n_treated + learner.n_control,
                confidence_level: 0.95,
            },
            provenance: Provenance::TLearner {
                n_treated: learner.n_treated,
                n_control: learner.n_control,
            },
        },
        ite_lower: cate_value - 1.96 * sqrt(total_var),
        ite_upper: cate_value + 1.96 * sqrt(total_var),
        treatment_prob: learner.n_treated as f64 / (learner.n_treated + learner.n_control) as f64,
        response_prob_treated: sigmoid(y_treated.value),
        response_prob_control: sigmoid(y_control.value),
        uncertainty_type: UncertaintyType::Epistemic,
    }
}

// ============================================================================
// X-Learner Implementation
// ============================================================================

/// Fit X-Learner model (cross-fitting for improved CATE)
fn fit_x_learner<const D: usize>(
    X: &[EVector<D>],
    treatment: &[bool],
    outcome: &[f64]
) -> XLearner<D> with Alloc {
    // Split data
    var X_treated: [EVector<D>] = []
    var y_treated: [f64] = []
    var X_control: [EVector<D>] = []
    var y_control: [f64] = []

    for i in 0..len(X) {
        if treatment[i] {
            X_treated = X_treated ++ [X[i]]
            y_treated = y_treated ++ [outcome[i]]
        } else {
            X_control = X_control ++ [X[i]]
            y_control = y_control ++ [outcome[i]]
        }
    }

    // Step 1: Fit outcome models
    let model_treated = fit_regression(&X_treated, &y_treated)
    let model_control = fit_regression(&X_control, &y_control)

    // Step 2: Impute counterfactuals
    // For treated: impute control outcome
    var tau_treated: [f64] = []
    for i in 0..len(X_treated) {
        let y_hat_0 = predict_regression(&model_control, &X_treated[i])
        tau_treated = tau_treated ++ [y_treated[i] - y_hat_0.value]
    }

    // For control: impute treated outcome
    var tau_control: [f64] = []
    for i in 0..len(X_control) {
        let y_hat_1 = predict_regression(&model_treated, &X_control[i])
        tau_control = tau_control ++ [y_hat_1.value - y_control[i]]
    }

    // Step 3: Fit CATE models on imputed effects
    let cate_model_treated = fit_regression(&X_treated, &tau_treated)
    let cate_model_control = fit_regression(&X_control, &tau_control)

    // Step 4: Fit propensity model
    var treatment_labels: [f64] = []
    for t in treatment {
        treatment_labels = treatment_labels ++ [if t { 1.0 } else { 0.0 }]
    }
    let propensity = fit_classification(X, &treatment_labels)

    XLearner {
        model_treated: model_treated,
        model_control: model_control,
        cate_model_treated: cate_model_treated,
        cate_model_control: cate_model_control,
        propensity_model: propensity,
    }
}

/// Predict CATE with X-Learner
fn x_learner_predict<const D: usize>(
    learner: &XLearner<D>,
    x: &EVector<D>
) -> IndividualTreatmentEffect {
    // Get propensity score
    let e_x = predict_propensity(&learner.propensity_model, x)

    // Get CATE estimates from both directions
    let tau_t = predict_regression(&learner.cate_model_treated, x)
    let tau_c = predict_regression(&learner.cate_model_control, x)

    // Combine using propensity weights
    // CATE = (1 - e(x)) * tau_t + e(x) * tau_c
    let cate_value = (1.0 - e_x) * tau_t.value + e_x * tau_c.value

    // Variance: weighted sum plus uncertainty in weights
    let weight_var = e_x * (1.0 - e_x)  // Variance of propensity
    let component_var = (1.0 - e_x) * (1.0 - e_x) * tau_t.variance +
                        e_x * e_x * tau_c.variance
    let cross_var = weight_var * (tau_t.value - tau_c.value) * (tau_t.value - tau_c.value)

    let total_var = component_var + cross_var

    // Get outcome predictions for response probabilities
    let y_treated = predict_regression(&learner.model_treated, x)
    let y_control = predict_regression(&learner.model_control, x)

    IndividualTreatmentEffect {
        cate: Knowledge {
            value: cate_value,
            variance: total_var,
            confidence: Confidence::Bayesian {
                prior_weight: 0.1,
                data_weight: 0.9,
            },
            provenance: Provenance::XLearner {
                propensity: e_x,
            },
        },
        ite_lower: cate_value - 1.96 * sqrt(total_var),
        ite_upper: cate_value + 1.96 * sqrt(total_var),
        treatment_prob: e_x,
        response_prob_treated: sigmoid(y_treated.value),
        response_prob_control: sigmoid(y_control.value),
        uncertainty_type: UncertaintyType::Mixed,
    }
}

// ============================================================================
// R-Learner Implementation
// ============================================================================

/// Fit R-Learner (Robinson's transformation)
fn fit_r_learner<const D: usize>(
    X: &[EVector<D>],
    treatment: &[bool],
    outcome: &[f64]
) -> RLearner<D> with Alloc {
    let n = len(X)

    // Step 1: Fit outcome model m(X) = E[Y|X]
    let outcome_model = fit_regression(X, outcome)

    // Step 2: Fit propensity model e(X) = P(T=1|X)
    var treatment_labels: [f64] = []
    for t in treatment {
        treatment_labels = treatment_labels ++ [if t { 1.0 } else { 0.0 }]
    }
    let propensity_model = fit_classification(X, &treatment_labels)

    // Step 3: Compute residuals
    var y_residuals: [f64] = []
    var t_residuals: [f64] = []

    for i in 0..n {
        let m_x = predict_regression(&outcome_model, &X[i])
        let e_x = predict_propensity(&propensity_model, &X[i])
        let t = if treatment[i] { 1.0 } else { 0.0 }

        y_residuals = y_residuals ++ [outcome[i] - m_x.value]
        t_residuals = t_residuals ++ [t - e_x]
    }

    // Step 4: Fit CATE model on pseudo-outcomes
    // Pseudo-outcome: (Y - m(X)) / (T - e(X))
    // Use weighted regression instead for stability
    var pseudo_outcomes: [f64] = []
    var weights: [f64] = []

    for i in 0..n {
        let t_res = t_residuals[i]
        if abs(t_res) > 0.1 {  // Avoid division by near-zero
            pseudo_outcomes = pseudo_outcomes ++ [y_residuals[i] / t_res]
            weights = weights ++ [t_res * t_res]  // Weight by (T - e(X))^2
        } else {
            pseudo_outcomes = pseudo_outcomes ++ [0.0]
            weights = weights ++ [0.01]
        }
    }

    let cate_model = fit_weighted_regression(X, &pseudo_outcomes, &weights)

    // Compute residual variance
    var residual_sum = 0.0
    for i in 0..n {
        let tau = predict_regression(&cate_model, &X[i])
        let t = if treatment[i] { 1.0 } else { 0.0 }
        let e = predict_propensity(&propensity_model, &X[i])
        let m = predict_regression(&outcome_model, &X[i])
        let predicted = m.value + (t - e) * tau.value
        let residual = outcome[i] - predicted
        residual_sum = residual_sum + residual * residual
    }
    let residual_variance = residual_sum / (n as f64 - D as f64 - 1.0)

    RLearner {
        outcome_model: outcome_model,
        propensity_model: propensity_model,
        cate_model: cate_model,
        residual_variance: residual_variance,
    }
}

/// Predict CATE with R-Learner
fn r_learner_predict<const D: usize>(
    learner: &RLearner<D>,
    x: &EVector<D>
) -> IndividualTreatmentEffect {
    let tau = predict_regression(&learner.cate_model, x)
    let e_x = predict_propensity(&learner.propensity_model, x)
    let m_x = predict_regression(&learner.outcome_model, x)

    // Variance includes model uncertainty and residual variance
    let total_var = tau.variance + learner.residual_variance / 100.0  // Scale by typical sample size

    IndividualTreatmentEffect {
        cate: Knowledge {
            value: tau.value,
            variance: total_var,
            confidence: Confidence::Frequentist {
                sample_size: learner.cate_model.n_samples,
                confidence_level: 0.95,
            },
            provenance: Provenance::RLearner {
                residual_variance: learner.residual_variance,
            },
        },
        ite_lower: tau.value - 1.96 * sqrt(total_var),
        ite_upper: tau.value + 1.96 * sqrt(total_var),
        treatment_prob: e_x,
        response_prob_treated: sigmoid(m_x.value + (1.0 - e_x) * tau.value),
        response_prob_control: sigmoid(m_x.value - e_x * tau.value),
        uncertainty_type: UncertaintyType::Mixed,
    }
}

// ============================================================================
// Targeting and Optimization
// ============================================================================

/// Targeting recommendation for a population
struct TargetingRecommendation<const D: usize> {
    individuals: [Individual<D>],
    cate_estimates: [IndividualTreatmentEffect],
    treatment_assignment: [bool],
    expected_lift: Knowledge<f64>,
    budget_used: f64,
}

/// Recommend optimal targeting given budget constraint
fn recommend_targeting<const D: usize, E: CATEEstimator<D>>(
    estimator: &E,
    population: &[EVector<D>],
    budget_fraction: f64
) -> TargetingRecommendation<D> with Alloc {
    let n = len(population) as i32
    let budget = (n as f64 * budget_fraction) as i32

    // Predict CATE for everyone
    var predictions: [IndividualTreatmentEffect] = []
    for i in 0..n {
        predictions = predictions ++ [estimator.predict_cate(&population[i])]
    }

    // Rank by CATE (descending)
    let ranked_indices = argsort_by_cate(&predictions)

    // Assign treatment to top budget individuals
    var assignment: [bool] = []
    for i in 0..n {
        assignment = assignment ++ [false]
    }

    var total_lift = 0.0
    var total_var = 0.0

    for k in 0..budget {
        let idx = ranked_indices[k]
        assignment[idx] = true
        total_lift = total_lift + predictions[idx].cate.value
        total_var = total_var + predictions[idx].cate.variance
    }

    // Create individuals
    var individuals: [Individual<D>] = []
    for i in 0..n {
        individuals = individuals ++ [Individual {
            features: population[i],
            treatment: Some(assignment[i]),
            outcome: None,
        }]
    }

    TargetingRecommendation {
        individuals: individuals,
        cate_estimates: predictions,
        treatment_assignment: assignment,
        expected_lift: Knowledge {
            value: total_lift,
            variance: total_var,
            confidence: Confidence::Aggregate {
                n_estimates: budget,
            },
            provenance: Provenance::Targeting {
                budget_fraction: budget_fraction,
            },
        },
        budget_used: budget as f64 / n as f64,
    }
}

/// Thompson sampling for treatment assignment
fn thompson_sampling_assign<const D: usize>(
    cate: &IndividualTreatmentEffect,
    cost_treat: f64,
    cost_control: f64
) -> bool with Prob {
    // Sample from posterior of CATE
    let sampled_effect = cate.cate.value + sqrt(cate.cate.variance) * random_normal()

    // Assign treatment if expected benefit exceeds cost
    sampled_effect > cost_treat - cost_control
}

// ============================================================================
// Evaluation Metrics
// ============================================================================

/// Qini curve data
struct QiniCurve {
    fractions: [f64],          // Fraction of population targeted
    uplift: [Knowledge<f64>],  // Cumulative uplift at each fraction
    random_uplift: [f64],      // Random baseline uplift
    auuc: Knowledge<f64>,      // Area Under Uplift Curve
}

/// Compute Qini curve for CATE estimates
fn compute_qini_curve<const D: usize>(
    predictions: &[IndividualTreatmentEffect],
    actual_treatment: &[bool],
    actual_outcome: &[f64]
) -> QiniCurve with Alloc {
    let n = len(predictions) as i32

    // Rank by predicted CATE
    let ranked = argsort_by_cate(predictions)

    var fractions: [f64] = [0.0]
    var uplift: [Knowledge<f64>] = [Knowledge::exact(0.0)]
    var random_uplift: [f64] = [0.0]

    var cum_treated = 0
    var cum_control = 0
    var cum_y_treated = 0.0
    var cum_y_control = 0.0

    // Total treated/control outcomes
    var total_treated = 0
    var total_control = 0
    var total_y_treated = 0.0
    var total_y_control = 0.0

    for i in 0..n {
        if actual_treatment[i] {
            total_treated = total_treated + 1
            total_y_treated = total_y_treated + actual_outcome[i]
        } else {
            total_control = total_control + 1
            total_y_control = total_y_control + actual_outcome[i]
        }
    }

    let overall_treated_rate = total_y_treated / total_treated as f64
    let overall_control_rate = total_y_control / total_control as f64
    let overall_effect = overall_treated_rate - overall_control_rate

    for k in 0..n {
        let idx = ranked[k]

        if actual_treatment[idx] {
            cum_treated = cum_treated + 1
            cum_y_treated = cum_y_treated + actual_outcome[idx]
        } else {
            cum_control = cum_control + 1
            cum_y_control = cum_y_control + actual_outcome[idx]
        }

        let frac = (k + 1) as f64 / n as f64
        fractions = fractions ++ [frac]

        // Cumulative uplift
        let treated_rate = if cum_treated > 0 { cum_y_treated / cum_treated as f64 } else { 0.0 }
        let control_rate = if cum_control > 0 { cum_y_control / cum_control as f64 } else { 0.0 }
        let current_uplift = (treated_rate - control_rate) * (k + 1) as f64

        // Variance of uplift estimate
        let n_eff = min(cum_treated, cum_control)
        let var_est = if n_eff > 1 {
            (treated_rate * (1.0 - treated_rate) / cum_treated as f64 +
             control_rate * (1.0 - control_rate) / cum_control as f64) * (k + 1) as f64 * (k + 1) as f64
        } else {
            1.0
        }

        uplift = uplift ++ [Knowledge {
            value: current_uplift,
            variance: var_est,
            confidence: Confidence::Frequentist {
                sample_size: cum_treated + cum_control,
                confidence_level: 0.95,
            },
            provenance: Provenance::QiniCurve,
        }]

        // Random baseline
        random_uplift = random_uplift ++ [overall_effect * (k + 1) as f64]
    }

    // Compute AUUC (Area Under Uplift Curve)
    var auuc_value = 0.0
    var auuc_var = 0.0

    for k in 1..len(fractions) {
        let dx = fractions[k] - fractions[k - 1]
        let height = (uplift[k].value + uplift[k - 1].value) / 2.0 -
                     (random_uplift[k] + random_uplift[k - 1]) / 2.0
        auuc_value = auuc_value + dx * height
        auuc_var = auuc_var + dx * dx * (uplift[k].variance + uplift[k - 1].variance) / 4.0
    }

    QiniCurve {
        fractions: fractions,
        uplift: uplift,
        random_uplift: random_uplift,
        auuc: Knowledge {
            value: auuc_value,
            variance: auuc_var,
            confidence: Confidence::Frequentist {
                sample_size: n,
                confidence_level: 0.95,
            },
            provenance: Provenance::AUUC,
        },
    }
}

/// Calibration metrics for CATE estimates
struct CATECalibration {
    bins: [f64],                    // Predicted CATE bins
    observed_effect: [Knowledge<f64>],  // Observed effect per bin
    n_per_bin: [i32],
    calibration_error: Knowledge<f64>,
}

/// Compute CATE calibration
fn compute_cate_calibration<const D: usize>(
    predictions: &[IndividualTreatmentEffect],
    actual_treatment: &[bool],
    actual_outcome: &[f64],
    n_bins: i32
) -> CATECalibration with Alloc {
    // Find CATE range
    var min_cate = f64::MAX
    var max_cate = f64::MIN

    for p in predictions {
        if p.cate.value < min_cate { min_cate = p.cate.value }
        if p.cate.value > max_cate { max_cate = p.cate.value }
    }

    let bin_width = (max_cate - min_cate) / n_bins as f64

    // Initialize bins
    var bins: [f64] = []
    var y_treated_sum: [f64] = []
    var y_control_sum: [f64] = []
    var n_treated: [i32] = []
    var n_control: [i32] = []

    for b in 0..n_bins {
        bins = bins ++ [min_cate + (b as f64 + 0.5) * bin_width]
        y_treated_sum = y_treated_sum ++ [0.0]
        y_control_sum = y_control_sum ++ [0.0]
        n_treated = n_treated ++ [0]
        n_control = n_control ++ [0]
    }

    // Assign to bins
    for i in 0..len(predictions) {
        let bin_idx = min(((predictions[i].cate.value - min_cate) / bin_width) as i32, n_bins - 1)
        let bin = max(0, bin_idx)

        if actual_treatment[i] {
            y_treated_sum[bin] = y_treated_sum[bin] + actual_outcome[i]
            n_treated[bin] = n_treated[bin] + 1
        } else {
            y_control_sum[bin] = y_control_sum[bin] + actual_outcome[i]
            n_control[bin] = n_control[bin] + 1
        }
    }

    // Compute observed effects
    var observed: [Knowledge<f64>] = []
    var n_per_bin: [i32] = []
    var calibration_error = 0.0
    var n_valid_bins = 0

    for b in 0..n_bins {
        let n_t = n_treated[b]
        let n_c = n_control[b]
        n_per_bin = n_per_bin ++ [n_t + n_c]

        if n_t > 0 && n_c > 0 {
            let rate_t = y_treated_sum[b] / n_t as f64
            let rate_c = y_control_sum[b] / n_c as f64
            let effect = rate_t - rate_c

            let var = rate_t * (1.0 - rate_t) / n_t as f64 +
                      rate_c * (1.0 - rate_c) / n_c as f64

            observed = observed ++ [Knowledge {
                value: effect,
                variance: var,
                confidence: Confidence::Frequentist {
                    sample_size: n_t + n_c,
                    confidence_level: 0.95,
                },
                provenance: Provenance::CalibrationBin,
            }]

            // Calibration error: |predicted - observed|
            calibration_error = calibration_error + abs(bins[b] - effect)
            n_valid_bins = n_valid_bins + 1
        } else {
            observed = observed ++ [Knowledge {
                value: 0.0,
                variance: f64::MAX,
                confidence: Confidence::None,
                provenance: Provenance::InsufficientData,
            }]
        }
    }

    let avg_cal_error = if n_valid_bins > 0 {
        calibration_error / n_valid_bins as f64
    } else {
        f64::MAX
    }

    CATECalibration {
        bins: bins,
        observed_effect: observed,
        n_per_bin: n_per_bin,
        calibration_error: Knowledge {
            value: avg_cal_error,
            variance: avg_cal_error * avg_cal_error / n_valid_bins as f64,
            confidence: Confidence::Frequentist {
                sample_size: len(predictions) as i32,
                confidence_level: 0.95,
            },
            provenance: Provenance::Calibration,
        },
    }
}

// ============================================================================
// Helper Functions
// ============================================================================

/// Fit simple linear regression
fn fit_regression<const D: usize>(X: &[EVector<D>], y: &[f64]) -> RegressionModel<D> with Alloc {
    let n = len(X) as f64

    // Compute means
    var x_mean = evec_zeros::<D>()
    var y_mean = 0.0

    for i in 0..len(X) {
        for j in 0..D {
            x_mean.values[j] = x_mean.values[j] + X[i].values[j]
        }
        y_mean = y_mean + y[i]
    }

    for j in 0..D {
        x_mean.values[j] = x_mean.values[j] / n
    }
    y_mean = y_mean / n

    // Simple OLS (for demonstration - real implementation would use matrix inversion)
    var coef = evec_zeros::<D>()
    var residual_sum = 0.0
    var total_sum = 0.0

    // Simplified: use gradient descent
    let lr = 0.01
    for iter in 0..100 {
        for i in 0..len(X) {
            var pred = y_mean
            for j in 0..D {
                pred = pred + coef.values[j] * (X[i].values[j] - x_mean.values[j])
            }
            let error = y[i] - pred

            for j in 0..D {
                coef.values[j] = coef.values[j] + lr * error * (X[i].values[j] - x_mean.values[j])
            }
        }
    }

    // Compute R-squared
    for i in 0..len(X) {
        var pred = y_mean
        for j in 0..D {
            pred = pred + coef.values[j] * (X[i].values[j] - x_mean.values[j])
        }
        residual_sum = residual_sum + (y[i] - pred) * (y[i] - pred)
        total_sum = total_sum + (y[i] - y_mean) * (y[i] - y_mean)
    }

    let r_sq = 1.0 - residual_sum / (total_sum + 1e-10)
    let res_var = residual_sum / (n - D as f64 - 1.0)

    // Estimate coefficient variance (simplified)
    for j in 0..D {
        var x_var = 0.0
        for i in 0..len(X) {
            let diff = X[i].values[j] - x_mean.values[j]
            x_var = x_var + diff * diff
        }
        coef.variances[j] = res_var / (x_var + 1e-10)
    }

    RegressionModel {
        coefficients: coef,
        intercept: Knowledge {
            value: y_mean,
            variance: res_var / n,
            confidence: Confidence::Frequentist { sample_size: len(X) as i32, confidence_level: 0.95 },
            provenance: Provenance::OLS,
        },
        residual_variance: res_var,
        r_squared: r_sq,
        n_samples: len(X) as i32,
    }
}

/// Fit weighted regression
fn fit_weighted_regression<const D: usize>(X: &[EVector<D>], y: &[f64], w: &[f64]) -> RegressionModel<D> with Alloc {
    // Simplified: just use regular regression with weighted samples
    // In practice, would implement WLS
    fit_regression(X, y)
}

/// Predict with regression model
fn predict_regression<const D: usize>(model: &RegressionModel<D>, x: &EVector<D>) -> Knowledge<f64> {
    var pred = model.intercept.value

    for j in 0..D {
        pred = pred + model.coefficients.values[j] * x.values[j]
    }

    // Prediction variance
    var pred_var = model.intercept.variance + model.residual_variance

    for j in 0..D {
        pred_var = pred_var + model.coefficients.variances[j] * x.values[j] * x.values[j] +
                   model.coefficients.values[j] * model.coefficients.values[j] * x.variances[j]
    }

    Knowledge {
        value: pred,
        variance: pred_var,
        confidence: model.intercept.confidence,
        provenance: Provenance::Regression,
    }
}

/// Fit logistic regression for classification
fn fit_classification<const D: usize>(X: &[EVector<D>], y: &[f64]) -> ClassificationModel<D> with Alloc {
    // Simplified logistic regression
    var coef = evec_zeros::<D>()
    var intercept = 0.0
    let lr = 0.01

    for iter in 0..100 {
        for i in 0..len(X) {
            var z = intercept
            for j in 0..D {
                z = z + coef.values[j] * X[i].values[j]
            }
            let p = sigmoid(z)
            let error = y[i] - p

            intercept = intercept + lr * error
            for j in 0..D {
                coef.values[j] = coef.values[j] + lr * error * X[i].values[j]
            }
        }
    }

    ClassificationModel {
        coefficients: coef,
        intercept: intercept,
        calibration: CalibrationData {
            bins: [],
            observed_freq: [],
            n_per_bin: [],
        },
    }
}

/// Predict propensity score
fn predict_propensity<const D: usize>(model: &ClassificationModel<D>, x: &EVector<D>) -> f64 {
    var z = model.intercept
    for j in 0..D {
        z = z + model.coefficients.values[j] * x.values[j]
    }
    sigmoid(z)
}

/// Compute calibration data
fn compute_calibration<const D: usize>(X: &[EVector<D>], y: &[f64], model: &RegressionModel<D>) -> CalibrationData with Alloc {
    CalibrationData {
        bins: [],
        observed_freq: [],
        n_per_bin: [],
    }
}

/// Argsort by CATE (descending)
fn argsort_by_cate(predictions: &[IndividualTreatmentEffect]) -> [i32] with Alloc {
    let n = len(predictions)
    var indices: [i32] = []
    for i in 0..n {
        indices = indices ++ [i as i32]
    }

    // Simple bubble sort (for demonstration)
    for i in 0..n {
        for j in (i + 1)..n {
            if predictions[indices[j]].cate.value > predictions[indices[i]].cate.value {
                let tmp = indices[i]
                indices[i] = indices[j]
                indices[j] = tmp
            }
        }
    }

    indices
}

// Utility functions
fn sigmoid(x: f64) -> f64 {
    1.0 / (1.0 + exp(-x))
}

fn sqrt(x: f64) -> f64 { @extern("sqrt") }
fn exp(x: f64) -> f64 { @extern("exp") }
fn log(x: f64) -> f64 { @extern("log") }
fn abs(x: f64) -> f64 { if x < 0.0 { -x } else { x } }
fn min(a: f64, b: f64) -> f64 { if a < b { a } else { b } }
fn max(a: f64, b: f64) -> f64 { if a > b { a } else { b } }
fn min(a: i32, b: i32) -> i32 { if a < b { a } else { b } }
fn max(a: i32, b: i32) -> i32 { if a > b { a } else { b } }
fn len<T>(arr: [T]) -> usize { @extern("array_len") }
fn random_normal() -> f64 with Prob { @extern("random_normal") }

// ============================================================================
// Unit Tests
// ============================================================================

#[test]
fn test_sigmoid() {
    assert(abs(sigmoid(0.0) - 0.5) < 1e-10)
    assert(sigmoid(10.0) > 0.99)
    assert(sigmoid(-10.0) < 0.01)
}

#[test]
fn test_s_learner_basic() with Alloc {
    // Simple test data
    let X = [
        evec_new([1.0, 0.0]),
        evec_new([0.0, 1.0]),
        evec_new([1.0, 1.0]),
        evec_new([0.0, 0.0]),
    ]
    let treatment = [true, false, true, false]
    let outcome = [1.0, 0.0, 1.5, 0.5]

    let model = fit_s_learner(&X, &treatment, &outcome)

    // Should predict positive treatment effect
    let x_test = evec_new([0.5, 0.5])
    let effect = s_learner_predict(&model, &x_test)

    // Effect should be positive (treated outcomes > control outcomes)
    assert(effect.cate.value > 0.0)
}

#[test]
fn test_t_learner_basic() with Alloc {
    let X = [
        evec_new([1.0]),
        evec_new([2.0]),
        evec_new([3.0]),
        evec_new([4.0]),
    ]
    let treatment = [true, true, false, false]
    let outcome = [2.0, 4.0, 1.0, 2.0]

    let model = fit_t_learner(&X, &treatment, &outcome)

    assert(model.n_treated == 2)
    assert(model.n_control == 2)

    let x_test = evec_new([2.5])
    let effect = t_learner_predict(&model, &x_test)

    // Should have reasonable bounds
    assert(effect.ite_lower < effect.cate.value)
    assert(effect.cate.value < effect.ite_upper)
}

#[test]
fn test_individual_treatment_effect_uncertainty() with Alloc {
    let X = [
        evec_new([1.0, 2.0]),
        evec_new([2.0, 1.0]),
    ]
    let treatment = [true, false]
    let outcome = [1.0, 0.0]

    let model = fit_t_learner(&X, &treatment, &outcome)
    let effect = t_learner_predict(&model, &evec_new([1.5, 1.5]))

    // With only 2 samples, uncertainty should be high
    assert(effect.cate.variance > 0.0)
    assert(effect.ite_upper - effect.ite_lower > 0.5)  // Wide interval
}

#[test]
fn test_argsort() with Alloc {
    let predictions = [
        IndividualTreatmentEffect {
            cate: Knowledge::exact(0.3),
            ite_lower: 0.0,
            ite_upper: 0.6,
            treatment_prob: 0.5,
            response_prob_treated: 0.5,
            response_prob_control: 0.5,
            uncertainty_type: UncertaintyType::Epistemic,
        },
        IndividualTreatmentEffect {
            cate: Knowledge::exact(0.8),
            ite_lower: 0.5,
            ite_upper: 1.0,
            treatment_prob: 0.5,
            response_prob_treated: 0.5,
            response_prob_control: 0.5,
            uncertainty_type: UncertaintyType::Epistemic,
        },
        IndividualTreatmentEffect {
            cate: Knowledge::exact(0.1),
            ite_lower: -0.1,
            ite_upper: 0.3,
            treatment_prob: 0.5,
            response_prob_treated: 0.5,
            response_prob_control: 0.5,
            uncertainty_type: UncertaintyType::Epistemic,
        },
    ]

    let sorted = argsort_by_cate(&predictions)

    // Should be sorted descending: 0.8 > 0.3 > 0.1
    assert(sorted[0] == 1)  // Index of 0.8
    assert(sorted[1] == 0)  // Index of 0.3
    assert(sorted[2] == 2)  // Index of 0.1
}

#[test]
fn test_qini_curve_basic() with Alloc {
    let predictions = [
        IndividualTreatmentEffect {
            cate: Knowledge::exact(0.5),
            ite_lower: 0.0,
            ite_upper: 1.0,
            treatment_prob: 0.5,
            response_prob_treated: 0.7,
            response_prob_control: 0.2,
            uncertainty_type: UncertaintyType::Epistemic,
        },
        IndividualTreatmentEffect {
            cate: Knowledge::exact(-0.2),
            ite_lower: -0.5,
            ite_upper: 0.1,
            treatment_prob: 0.5,
            response_prob_treated: 0.3,
            response_prob_control: 0.5,
            uncertainty_type: UncertaintyType::Epistemic,
        },
    ]

    let treatment = [true, false]
    let outcome = [1.0, 0.0]

    let qini = compute_qini_curve(&predictions, &treatment, &outcome)

    // Should have n+1 points
    assert(len(qini.fractions) == 3)
    assert(abs(qini.fractions[0]) < 1e-10)
    assert(abs(qini.fractions[2] - 1.0) < 1e-10)
}

#[test]
fn test_calibration_bins() with Alloc {
    let predictions = [
        IndividualTreatmentEffect {
            cate: Knowledge::exact(0.1),
            ite_lower: 0.0,
            ite_upper: 0.2,
            treatment_prob: 0.5,
            response_prob_treated: 0.5,
            response_prob_control: 0.4,
            uncertainty_type: UncertaintyType::Epistemic,
        },
        IndividualTreatmentEffect {
            cate: Knowledge::exact(0.9),
            ite_lower: 0.7,
            ite_upper: 1.0,
            treatment_prob: 0.5,
            response_prob_treated: 0.9,
            response_prob_control: 0.1,
            uncertainty_type: UncertaintyType::Epistemic,
        },
    ]

    let treatment = [true, false]
    let outcome = [0.6, 0.4]

    let cal = compute_cate_calibration(&predictions, &treatment, &outcome, 2)

    assert(len(cal.bins) == 2)
    assert(len(cal.n_per_bin) == 2)
}

#[test]
fn test_targeting_recommendation() with Alloc {
    // Test that targeting works with budget constraint

    // Create simple mock model
    struct MockEstimator {}

    impl CATEEstimator<2> for MockEstimator {
        fn predict_cate(self: &Self, x: &EVector<2>) -> IndividualTreatmentEffect {
            // CATE = x[0] (first feature determines effect)
            IndividualTreatmentEffect {
                cate: Knowledge::exact(x.values[0]),
                ite_lower: x.values[0] - 0.1,
                ite_upper: x.values[0] + 0.1,
                treatment_prob: 0.5,
                response_prob_treated: 0.5 + x.values[0] / 2.0,
                response_prob_control: 0.5,
                uncertainty_type: UncertaintyType::Epistemic,
            }
        }

        fn predict_batch(self: &Self, X: &[EVector<2>]) -> [IndividualTreatmentEffect] {
            var result: [IndividualTreatmentEffect] = []
            for x in X {
                result = result ++ [self.predict_cate(x)]
            }
            result
        }

        fn feature_importance(self: &Self) -> EVector<2> {
            evec_new([1.0, 0.0])
        }
    }

    let estimator = MockEstimator {}
    let population = [
        evec_new([0.1, 0.5]),  // Low effect
        evec_new([0.9, 0.5]),  // High effect
        evec_new([0.5, 0.5]),  // Medium effect
        evec_new([0.3, 0.5]),  // Low-medium effect
    ]

    let rec = recommend_targeting(&estimator, &population, 0.5)

    // Should target 50% = 2 individuals
    var n_treated = 0
    for a in rec.treatment_assignment {
        if a { n_treated = n_treated + 1 }
    }
    assert(n_treated == 2)

    // Should target high-effect individuals (indices 1 and 2)
    assert(rec.treatment_assignment[1])  // 0.9 effect
    assert(rec.treatment_assignment[2])  // 0.5 effect
}
